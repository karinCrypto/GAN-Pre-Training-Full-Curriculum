{"cells":[{"cell_type":"markdown","id":"058d5474","metadata":{"id":"058d5474"},"source":["# ğŸ¨ Day 3: DCGAN (Deep Convolutional GAN) ì™„ì „ ì •ë³µ\n","\n","## ğŸ“š í•™ìŠµ ëª©í‘œ\n","\n","1. Vanilla GANì˜ í•œê³„ì  (Training Instability, Mode Collapse) ì´í•´\n","2. DCGANì˜ 5ê°€ì§€ ì„¤ê³„ ì›ì¹™ê³¼ ê°ê°ì˜ íš¨ê³¼ íŒŒì•…\n","3. Transposed Convolutionì˜ ë™ì‘ ì›ë¦¬ ì™„ë²½ ì´í•´\n","4. TensorFlowì™€ PyTorchë¡œ DCGAN êµ¬í˜„\n","5. CelebA ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ëŒ ì–¼êµ´ ì´ë¯¸ì§€ ìƒì„±\n","\n","---"]},{"cell_type":"markdown","id":"e5b61b38","metadata":{"id":"e5b61b38"},"source":["# ğŸ• 1êµì‹œ: Vanilla GAN ë³µìŠµ ë° í•œê³„ì \n","\n","---"]},{"cell_type":"markdown","id":"0a3dcfc7","metadata":{"id":"0a3dcfc7"},"source":["## 1.1 Vanilla GAN í•µì‹¬ ë³µìŠµ\n","\n","### GANì˜ ê¸°ë³¸ êµ¬ì¡°\n","\n","![](https://developers.google.com/static/machine-learning/gan/images/gan_diagram.svg?hl=ko)\n","\n","### Min-Max ê²Œì„\n","\n","$$\n","\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]\n","$$\n","\n","- **Generator**: $D(G(z))$ë¥¼ ìµœëŒ€í™” â†’ Discriminatorë¥¼ ì†ì´ê¸°\n","- **Discriminator**: ì§„ì§œëŠ” 1, ê°€ì§œëŠ” 0ìœ¼ë¡œ ì •í™•íˆ ë¶„ë¥˜\n","\n","### Vanilla GANì˜ êµ¬ì¡° (ë³µìŠµ)\n","\n","```python\n","# Generator: 100 â†’ 128 â†’ 256 â†’ 512 â†’ 784\n","# Discriminator: 784 â†’ 512 â†’ 256 â†’ 1\n","# ëª¨ë‘ Fully Connected (Dense) Layerë¡œ êµ¬ì„±\n","```"]},{"cell_type":"markdown","id":"c50fb540","metadata":{"id":"c50fb540"},"source":["## 1.2 Vanilla GANì˜ í•œê³„ì \n","\n","Vanilla GANì€ í˜ì‹ ì ì¸ ì•„ì´ë””ì–´ì˜€ì§€ë§Œ, ì‹¤ì œ í•™ìŠµì—ì„œ ì—¬ëŸ¬ ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.\n","\n","---"]},{"cell_type":"markdown","id":"e8384061","metadata":{"id":"e8384061"},"source":["### ğŸ”´ í•œê³„ì  1: Training Instability (í•™ìŠµ ë¶ˆì•ˆì •ì„±)\n","\n","#### ë¬¸ì œ í˜„ìƒ\n","\n","1. **Gradient Vanishing (ê¸°ìš¸ê¸° ì†Œì‹¤)**\n","   - Dê°€ ë„ˆë¬´ ê°•í•´ì§€ë©´ â†’ $D(G(z)) \\approx 0$\n","   - $\\log(1 - D(G(z))) \\approx \\log(1) = 0$\n","   - Gì˜ gradientê°€ ê±°ì˜ 0ì´ ë˜ì–´ í•™ìŠµ ë©ˆì¶¤\n","\n","2. **Gradient Exploding (ê¸°ìš¸ê¸° í­ë°œ)**\n","   - ë¶ˆê· í˜•í•œ í•™ìŠµë¥ ë¡œ gradientê°€ ë°œì‚°\n","   - Lossê°€ NaNì´ ë˜ëŠ” í˜„ìƒ\n","\n","3. **Loss ì§„ë™**\n","   - Dì™€ Gê°€ ë²ˆê°ˆì•„ê°€ë©° ìš°ì„¸í•´ì§\n","   - ìˆ˜ë ´í•˜ì§€ ì•Šê³  ì§„ë™\n","\n","#### ì‹œê°ì  ì˜ˆì‹œ\n","\n","![](https://wikidocs.net/images/page/229995/chal1.png)"]},{"cell_type":"markdown","id":"ef23e918","metadata":{"id":"ef23e918"},"source":["### ğŸ”´ í•œê³„ì  2: Mode Collapse (ëª¨ë“œ ë¶•ê´´)\n","\n","#### ë¬¸ì œ í˜„ìƒ\n","\n","Generatorê°€ **í•œ ê°€ì§€ íŒ¨í„´ë§Œ ë°˜ë³µ ìƒì„±**í•˜ëŠ” í˜„ìƒ\n","\n","![](https://wikidocs.net/images/page/229995/chal3.png)\n","\n","#### ë°œìƒ ì›ì¸\n","\n","1. **Dê°€ ë„ˆë¬´ ê°•í•¨** â†’ Gê°€ \"ì•ˆì „í•œ\" ì¶œë ¥ë§Œ í•™ìŠµ\n","2. **Gradient íë¦„ ë¬¸ì œ** â†’ íŠ¹ì • ëª¨ë“œë¡œë§Œ gradient ì§‘ì¤‘\n","3. **ë‹¤ì–‘ì„± ì†ì‹¤** â†’ Gê°€ íƒìƒ‰ì„ ë©ˆì¶”ê³  exploitationë§Œ ìˆ˜í–‰\n","\n","#### ë©”ì»¤ë‹ˆì¦˜\n","\n","```\n","Step 1: Gê°€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ìƒì„± ì‹œë„\n","Step 2: Dê°€ ì¼ë¶€ ì´ë¯¸ì§€ë§Œ \"ì§„ì§œ ê°™ë‹¤\"ê³  íŒë‹¨\n","Step 3: GëŠ” ê·¸ ì´ë¯¸ì§€ë“¤ë§Œ ë” ë§ì´ ìƒì„±\n","Step 4: ê²°êµ­ í•œ ê°€ì§€ íŒ¨í„´ì— ìˆ˜ë ´ (Mode Collapse!)\n","```"]},{"cell_type":"markdown","id":"2ca3dacb","metadata":{"id":"2ca3dacb"},"source":["### ğŸ”´ í•œê³„ì  3: Fully Connected Layerì˜ êµ¬ì¡°ì  í•œê³„\n","\n","#### ë¬¸ì œ 1: ê³µê°„ ì •ë³´ ì†ì‹¤\n","\n","```python\n","# Vanilla GANì˜ ì…ë ¥ ì²˜ë¦¬\n","image = (28, 28, 1)  # 2D ì´ë¯¸ì§€\n","flattened = (784,)   # 1D ë²¡í„°ë¡œ ë³€í™˜ â†’ ê³µê°„ êµ¬ì¡° ì™„ì „íˆ ì‚¬ë¼ì§!\n","\n","# ë¬¸ì œ: ì¸ì ‘í•œ í”½ì…€ ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ì—†ìŒ\n","# ì˜ˆ: (0,0) í”½ì…€ê³¼ (0,1) í”½ì…€ì´ ì´ì›ƒì´ë¼ëŠ” ì •ë³´ ì†ì‹¤\n","```\n","\n","#### ë¬¸ì œ 2: íŒŒë¼ë¯¸í„° í­ì¦\n","\n","```python\n","# 64Ã—64Ã—3 ì»¬ëŸ¬ ì´ë¯¸ì§€ì˜ ê²½ìš°\n","input_size = 64 * 64 * 3  # = 12,288\n","hidden_size = 1024\n","\n","# ì²« ë²ˆì§¸ FC Layerë§Œ í•´ë„:\n","parameters = 12288 * 1024 = 12,582,912ê°œ! (ì•½ 1200ë§Œ ê°œ)\n","\n","# CNNì˜ ê²½ìš°:\n","# 3Ã—3 kernel, 64 filters = 3*3*3*64 = 1,728ê°œ\n","# â†’ ì•½ 7000ë°° ì°¨ì´!\n","```\n","\n","#### ë¬¸ì œ 3: ì§€ì—­ì  íŠ¹ì§• í•™ìŠµ ë¶ˆê°€\n","\n","```\n","FC Layer:                    CNN:\n","ëª¨ë“  í”½ì…€ì´ ëª¨ë“  ë‰´ëŸ°ì— ì—°ê²°    ì§€ì—­ì  ì˜ì—­ë§Œ ì—°ê²° (receptive field)\n","\n","â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n","â”‚â— â— â— â— â— â— â— â—â”‚          â”‚â–  â–  â–¡ â–¡ â–¡ â–¡ â–¡ â–¡â”‚\n","â”‚â— â— â— â— â— â— â— â—â”‚          â”‚â–  â–  â–¡ â–¡ â–¡ â–¡ â–¡ â–¡â”‚\n","â”‚â— â— â— â— â— â— â— â—â”‚          â”‚â–¡ â–¡ â–¡ â–¡ â–¡ â–¡ â–¡ â–¡â”‚\n","â”‚ëª¨ë“  ì—°ê²° (ë¹„íš¨ìœ¨) â”‚          â”‚3Ã—3 ì˜ì—­ë§Œ ì—°ê²°   â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","```\n","\n","#### ë¬¸ì œ 4: ê³ í•´ìƒë„ í™•ì¥ ë¶ˆê°€ëŠ¥\n","\n","| í•´ìƒë„ | FC íŒŒë¼ë¯¸í„° ìˆ˜ | ë©”ëª¨ë¦¬ |\n","|--------|----------------|--------|\n","| 28Ã—28 | ~80ë§Œ ê°œ | ~3MB |\n","| 64Ã—64 | ~1200ë§Œ ê°œ | ~48MB |\n","| 256Ã—256 | ~2ì–µ ê°œ | ~800MB |\n","| 1024Ã—1024 | ~30ì–µ ê°œ | ë¶ˆê°€ëŠ¥ |"]},{"cell_type":"markdown","id":"0f1981ef","metadata":{"id":"0f1981ef"},"source":["## 1.3 DCGANì˜ ë“±ì¥ ë°°ê²½\n","\n","### CNNì´ í•´ê²°ì±…ì¸ ì´ìœ \n","\n","| ë¬¸ì œ | FC Layer | CNN |\n","|------|----------|-----|\n","| ê³µê°„ ì •ë³´ | ì™„ì „íˆ ì†ì‹¤ | ë³´ì¡´ë¨ |\n","| íŒŒë¼ë¯¸í„° ìˆ˜ | í­ë°œì  ì¦ê°€ | íŒŒë¼ë¯¸í„° ê³µìœ ë¡œ íš¨ìœ¨ì  |\n","| ì§€ì—­ íŠ¹ì§• | í•™ìŠµ ë¶ˆê°€ | Convolutionìœ¼ë¡œ í•™ìŠµ |\n","| ê³ í•´ìƒë„ | ë¶ˆê°€ëŠ¥ | ê°€ëŠ¥ |\n","\n","### 2015ë…„ Radford et al. ë…¼ë¬¸\n","\n","**\"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\"**\n","\n","- CNNì„ GANì— ì„±ê³µì ìœ¼ë¡œ ì ìš©í•œ ì²« ë²ˆì§¸ ë…¼ë¬¸\n","- **ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•œ 5ê°€ì§€ ê°€ì´ë“œë¼ì¸** ì œì‹œ\n","- ì´í›„ ê±°ì˜ ëª¨ë“  GANì˜ ê¸°ì´ˆê°€ ë¨\n","\n","---"]},{"cell_type":"markdown","id":"f322f181","metadata":{"id":"f322f181"},"source":["# ğŸ• 2êµì‹œ: DCGAN ì•„í‚¤í…ì²˜ ì´í•´\n","\n","---"]},{"cell_type":"markdown","id":"cb2edd57","metadata":{"id":"cb2edd57"},"source":["## 2.1 DCGANì˜ 5ê°€ì§€ ì„¤ê³„ ì›ì¹™ (ìƒì„¸)\n","\n","DCGAN ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ 5ê°€ì§€ í•µì‹¬ ê°€ì´ë“œë¼ì¸ì„ í•˜ë‚˜ì”© ìƒì„¸íˆ ì‚´í´ë´…ë‹ˆë‹¤.\n","\n","---"]},{"cell_type":"markdown","id":"832ccb42","metadata":{"id":"832ccb42"},"source":["### ğŸ“Œ ì›ì¹™ 1: Pooling Layer ì œê±° â†’ Strided Convolution\n","\n","#### âŒ ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì \n","\n","```python\n","# ì „í†µì ì¸ CNN (ë¶„ë¥˜ìš©)\n","x = Conv2D(64, 3, padding='same')(x)\n","x = MaxPooling2D(2)(x)  # ë¬¸ì œ!\n","```\n","\n","**MaxPoolingì˜ ë¬¸ì œ:**\n","1. **ì •ë³´ ì†ì‹¤**: 2Ã—2 ì˜ì—­ì—ì„œ ìµœëŒ“ê°’ë§Œ ì„ íƒ, ë‚˜ë¨¸ì§€ 3ê°œ ë²„ë¦¼\n","2. **Gradient íë¦„ ë°©í•´**: ì„ íƒë˜ì§€ ì•Šì€ ìœ„ì¹˜ëŠ” gradient = 0\n","3. **í•™ìŠµ ë¶ˆê°€ëŠ¥**: Pooling ìì²´ëŠ” í•™ìŠµ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŒ\n","\n","```\n","MaxPooling ì˜ˆì‹œ:\n","â”Œâ”€â”€â”€â”¬â”€â”€â”€â”\n","â”‚ 1 â”‚ 3 â”‚     ìµœëŒ“ê°’\n","â”œâ”€â”€â”€â”¼â”€â”€â”€â”¤  â†’  â”Œâ”€â”€â”€â”\n","â”‚ 2 â”‚ 4 â”‚     â”‚ 4 â”‚  â† 4ë§Œ ì„ íƒ, 1,2,3ì˜ ì •ë³´ ì†ì‹¤!\n","â””â”€â”€â”€â”´â”€â”€â”€â”˜     â””â”€â”€â”€â”˜\n","```\n","\n","#### âœ… DCGANì˜ í•´ê²°ì±…: Strided Convolution\n","\n","```python\n","# DCGAN ë°©ì‹ (Discriminator)\n","x = Conv2D(64, 4, strides=2, padding='same')(x)  # stride=2ë¡œ ë‹¤ìš´ìƒ˜í”Œë§\n","```\n","\n","**Strided Convolutionì˜ ì¥ì :**\n","1. **í•™ìŠµ ê°€ëŠ¥í•œ ë‹¤ìš´ìƒ˜í”Œë§**: ì»¤ë„ ê°€ì¤‘ì¹˜ê°€ í•™ìŠµë¨\n","2. **ì •ë³´ ë³´ì¡´**: ëª¨ë“  ìœ„ì¹˜ì˜ ì •ë³´ê°€ ì¶œë ¥ì— ë°˜ì˜\n","3. **ì•ˆì •ì  Gradient**: ëª¨ë“  ì…ë ¥ì— gradient ì „íŒŒ\n","\n","```\n","Strided Conv (stride=2) ì˜ˆì‹œ:\n","â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”     ì»¤ë„ì´ 2ì¹¸ì”© ì´ë™í•˜ë©°\n","â”‚ a â”‚ b â”‚ c â”‚ d â”‚     í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¡œ ë‹¤ìš´ìƒ˜í”Œë§\n","â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤  â†’  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”\n","â”‚ e â”‚ f â”‚ g â”‚ h â”‚     â”‚ * â”‚ * â”‚  â† ëª¨ë“  ì •ë³´ í™œìš©!\n","â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤     â””â”€â”€â”€â”´â”€â”€â”€â”˜\n","â”‚ i â”‚ j â”‚ k â”‚ l â”‚\n","â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n","```\n","\n","#### ğŸ”§ ì½”ë“œ ë¹„êµ"]},{"cell_type":"code","execution_count":null,"id":"2983f698","metadata":{"id":"2983f698"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","\n","# ì˜ˆì‹œ ì…ë ¥: 4Ã—4 ì´ë¯¸ì§€\n","example_input = tf.random.normal([1, 4, 4, 1])\n","print(\"ì…ë ¥ shape:\", example_input.shape)  # (1, 4, 4, 1)\n","\n","# âŒ MaxPooling ë°©ì‹\n","maxpool = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n","maxpool_output = maxpool(example_input)\n","print(\"MaxPooling ì¶œë ¥ shape:\", maxpool_output.shape)  # (1, 2, 2, 1)\n","\n","# âœ… Strided Conv ë°©ì‹ (DCGAN)\n","strided_conv = tf.keras.layers.Conv2D(1, kernel_size=4, strides=2, padding=\"same\")\n","strided_output = strided_conv(example_input)\n","print(\"Strided Conv ì¶œë ¥ shape:\", strided_output.shape)  # (1, 2, 2, 1)\n","\n","# í•µì‹¬ ì°¨ì´: Strided ConvëŠ” í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ìˆìŒ!\n","print(f\"\\nMaxPooling íŒŒë¼ë¯¸í„° ìˆ˜: {maxpool.count_params()}\")  # 0\n","print(\n","    f\"Strided Conv íŒŒë¼ë¯¸í„° ìˆ˜: {strided_conv.count_params()}\"\n",")  # ì»¤ë„ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¦„"]},{"cell_type":"markdown","id":"6ef0087e","metadata":{"id":"6ef0087e"},"source":["### ğŸ“Œ ì›ì¹™ 2: Fully Connected Layer ì œê±° (ì¤‘ê°„ì¸µ)\n","\n","#### âŒ ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì \n","\n","```python\n","# ì „í†µì ì¸ CNN ë¶„ë¥˜ê¸°\n","x = Conv2D(...)(x)\n","x = Flatten()(x)           # ê³µê°„ ì •ë³´ ì†ì‹¤!\n","x = Dense(1024)(x)         # íŒŒë¼ë¯¸í„° í­ë°œ!\n","x = Dense(num_classes)(x)\n","```\n","\n","**ë¬¸ì œ:**\n","- Flatten í›„ Denseë¥¼ ì“°ë©´ ì•ì„œ ì–¸ê¸‰í•œ FCì˜ ëª¨ë“  ë¬¸ì œ ë°œìƒ\n","- ê³µê°„ êµ¬ì¡°ê°€ ì™„ì „íˆ íŒŒê´´ë¨\n","\n","#### âœ… DCGANì˜ í•´ê²°ì±…\n","\n","**Generator:**\n","```python\n","# ë…¸ì´ì¦ˆë¥¼ ì§ì ‘ 4D í…ì„œë¡œ reshape\n","z = Dense(4 * 4 * 512)(noise)  # ìœ ì¼í•œ FC layer\n","z = Reshape((4, 4, 512))(z)    # ì¦‰ì‹œ ê³µê°„ êµ¬ì¡° ë¶€ì—¬\n","# ì´í›„ëŠ” ëª¨ë‘ Conv2DTranspose\n","```\n","\n","**Discriminator:**\n","```python\n","# Convë§Œìœ¼ë¡œ 1Ã—1Ã—1ê¹Œì§€ ì¶•ì†Œ\n","x = Conv2D(..., strides=2)(x)  # ê³„ì† ë‹¤ìš´ìƒ˜í”Œë§\n","# ë§ˆì§€ë§‰ì— Flatten â†’ Dense(1) (ì¶œë ¥ì¸µë§Œ FC)\n","```\n","\n","#### ğŸ”§ ì½”ë“œ ë¹„êµ"]},{"cell_type":"code","execution_count":null,"id":"621579c4","metadata":{"id":"621579c4"},"outputs":[],"source":["# ì˜ˆì‹œ: 64Ã—64Ã—3 ì´ë¯¸ì§€ ì²˜ë¦¬\n","\n","# âŒ FC ë°©ì‹ (Vanilla GAN)\n","flatten_size = 64 * 64 * 3  # = 12,288\n","fc_params = flatten_size * 1024  # ì•½ 1200ë§Œ ê°œ!\n","print(f\"FC ë°©ì‹ íŒŒë¼ë¯¸í„°: {fc_params:,}ê°œ\")\n","\n","# âœ… Conv ë°©ì‹ (DCGAN)\n","# 64Ã—64 â†’ 32Ã—32 â†’ 16Ã—16 â†’ 8Ã—8 â†’ 4Ã—4 â†’ 1Ã—1\n","# ê° Conv layer: kernel_size=4, filters ì¦ê°€\n","conv_params = (\n","    (4 * 4 * 3 * 64)\n","    + (4 * 4 * 64 * 128)\n","    + (4 * 4 * 128 * 256)\n","    + (4 * 4 * 256 * 512)\n","    + (4 * 4 * 512 * 1)\n",")\n","print(f\"Conv ë°©ì‹ íŒŒë¼ë¯¸í„°: {conv_params:,}ê°œ\")\n","print(f\"íŒŒë¼ë¯¸í„° ì ˆê°: {fc_params / conv_params:.1f}ë°°\")"]},{"cell_type":"markdown","id":"ec072580","metadata":{"id":"ec072580"},"source":["### ğŸ“Œ ì›ì¹™ 3: Batch Normalization ì ìš©\n","\n","#### âŒ ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì : Internal Covariate Shift\n","\n","```\n","ê° ì¸µì˜ ì…ë ¥ ë¶„í¬ê°€ í•™ìŠµ ì¤‘ ê³„ì† ë³€í™”\n","â†’ ë’¤ìª½ ì¸µì€ ê³„ì† ë³€í•˜ëŠ” ì…ë ¥ì— ì ì‘í•´ì•¼ í•¨\n","â†’ í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ê³  ëŠë¦¼\n","\n","Layer 1 ì¶œë ¥: N(0, 1) â†’ N(2, 3) â†’ N(-1, 5) â†’ ...\n","             â†“          â†“          â†“\n","Layer 2: ê³„ì† ë‹¤ë¥¸ ë¶„í¬ì— ì ì‘í•´ì•¼ í•¨ â†’ ë¶ˆì•ˆì •!\n","```\n","\n","#### âœ… DCGANì˜ í•´ê²°ì±…: Batch Normalization\n","\n","```python\n","# ê° ì¸µì˜ ì¶œë ¥ì„ ì •ê·œí™”\n","x = Conv2D(...)(x)\n","x = BatchNormalization()(x)  # í‰ê· =0, ë¶„ì‚°=1ë¡œ ì •ê·œí™”\n","x = ReLU()(x)\n","```\n","\n","**BatchNorm ìˆ˜ì‹:**\n","$$\n","\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n","$$\n","$$\n","y = \\gamma \\hat{x} + \\beta\n","$$\n","\n","- $\\mu_B$, $\\sigma_B$: ë°°ì¹˜ì˜ í‰ê· ê³¼ ë¶„ì‚°\n","- $\\gamma$, $\\beta$: í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼, ì‹œí”„íŠ¸ íŒŒë¼ë¯¸í„°\n","\n","#### âš ï¸ ì¤‘ìš”: BatchNormì„ ì ìš©í•˜ì§€ ì•ŠëŠ” ìœ„ì¹˜\n","\n","| ìœ„ì¹˜ | BatchNorm | ì´ìœ  |\n","|------|-----------|------|\n","| G ì¶œë ¥ì¸µ | âŒ ì‚¬ìš© ì•ˆ í•¨ | Tanh ì¶œë ¥ ë²”ìœ„ [-1,1] ìœ ì§€ í•„ìš” |\n","| D ì…ë ¥ì¸µ | âŒ ì‚¬ìš© ì•ˆ í•¨ | ì›ë³¸ ì´ë¯¸ì§€ ë¶„í¬ ë³´ì¡´ í•„ìš” |\n","| ê·¸ ì™¸ ëª¨ë“  ì¸µ | âœ… ì‚¬ìš© | í•™ìŠµ ì•ˆì •í™” |\n","\n","#### ğŸ”§ ì½”ë“œ ì˜ˆì‹œ"]},{"cell_type":"code","execution_count":null,"id":"845e3565","metadata":{"id":"845e3565"},"outputs":[],"source":["# BatchNorm íš¨ê³¼ ì‹œê°í™”\n","import matplotlib.pyplot as plt\n","\n","# ê°€ìƒì˜ ì¸µ ì¶œë ¥ (ì •ê·œí™” ì „)\n","np.random.seed(42)\n","before_bn = np.random.normal(loc=5, scale=3, size=1000)  # í‰ê·  5, í‘œì¤€í¸ì°¨ 3\n","\n","# BatchNorm ì ìš© í›„ (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)\n","after_bn = (before_bn - before_bn.mean()) / before_bn.std()\n","\n","fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","axes[0].hist(before_bn, bins=50, color=\"red\", alpha=0.7)\n","axes[0].axvline(\n","    x=before_bn.mean(),\n","    color=\"black\",\n","    linestyle=\"--\",\n","    label=f\"í‰ê· : {before_bn.mean():.2f}\",\n",")\n","axes[0].set_title(\"BatchNorm ì ìš© ì „\", fontsize=14)\n","axes[0].set_xlabel(\"ê°’\")\n","axes[0].legend()\n","\n","axes[1].hist(after_bn, bins=50, color=\"green\", alpha=0.7)\n","axes[1].axvline(\n","    x=after_bn.mean(),\n","    color=\"black\",\n","    linestyle=\"--\",\n","    label=f\"í‰ê· : {after_bn.mean():.2f}\",\n",")\n","axes[1].set_title(\"BatchNorm ì ìš© í›„\", fontsize=14)\n","axes[1].set_xlabel(\"ê°’\")\n","axes[1].legend()\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"ì ìš© ì „ - í‰ê· : {before_bn.mean():.2f}, í‘œì¤€í¸ì°¨: {before_bn.std():.2f}\")\n","print(f\"ì ìš© í›„ - í‰ê· : {after_bn.mean():.2f}, í‘œì¤€í¸ì°¨: {after_bn.std():.2f}\")"]},{"cell_type":"markdown","id":"0bc88a9c","metadata":{"id":"0bc88a9c"},"source":["### ğŸ“Œ ì›ì¹™ 4: í™œì„±í™” í•¨ìˆ˜ ì„ íƒ\n","\n","#### Generatorì˜ í™œì„±í™” í•¨ìˆ˜\n","\n","| ìœ„ì¹˜ | í™œì„±í™” í•¨ìˆ˜ | ì´ìœ  |\n","|------|-------------|------|\n","| ì€ë‹‰ì¸µ | **ReLU** | Gradient íë¦„ ì›í™œ, í¬ì†Œ í‘œí˜„ |\n","| ì¶œë ¥ì¸µ | **Tanh** | ì¶œë ¥ ë²”ìœ„ [-1, 1]ë¡œ ì´ë¯¸ì§€ ê°’ê³¼ ì¼ì¹˜ |\n","\n","```python\n","# Generator êµ¬ì¡°\n","x = Conv2DTranspose(...)(x)\n","x = BatchNormalization()(x)\n","x = ReLU()(x)  # ì€ë‹‰ì¸µ: ReLU\n","...\n","output = Conv2DTranspose(..., activation='tanh')(x)  # ì¶œë ¥ì¸µ: Tanh\n","```\n","\n","#### Discriminatorì˜ í™œì„±í™” í•¨ìˆ˜\n","\n","| ìœ„ì¹˜ | í™œì„±í™” í•¨ìˆ˜ | ì´ìœ  |\n","|------|-------------|------|\n","| ì€ë‹‰ì¸µ | **LeakyReLU(0.2)** | Dying ReLU ë¬¸ì œ ë°©ì§€ |\n","| ì¶œë ¥ì¸µ | **Sigmoid** ë˜ëŠ” ì—†ìŒ | í™•ë¥  ì¶œë ¥ ë˜ëŠ” BCEWithLogits ì‚¬ìš© |\n","\n","#### âŒ Dying ReLU ë¬¸ì œ\n","\n","```\n","ReLU: f(x) = max(0, x)\n","\n","ë¬¸ì œ: x < 0ì¸ ì˜ì—­ì—ì„œ gradient = 0\n","â†’ í•œë²ˆ ìŒìˆ˜ê°€ ë˜ë©´ ì˜ì›íˆ ì—…ë°ì´íŠ¸ ì•ˆ ë¨ (ë‰´ëŸ° \"ì‚¬ë§\")\n","\n","      â”‚      /\n","      â”‚     /\n","â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€/â”€â”€â”€â”€â†’\n","      â”‚   /\n","ìŒìˆ˜ ì˜ì—­: gradient = 0 (ë¬¸ì œ!)\n","```\n","\n","#### âœ… LeakyReLU í•´ê²°ì±…\n","\n","```\n","LeakyReLU: f(x) = max(Î±x, x), Î± = 0.2\n","\n","      â”‚      /\n","      â”‚     /\n","â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€/â”€â”€â”€â”€â†’\n","     /â”‚   /\n","    / â”‚  /\n","ìŒìˆ˜ ì˜ì—­ì—ì„œë„ ì‘ì€ gradient ìœ ì§€!\n","```\n","\n","#### ğŸ”§ ì½”ë“œ ë¹„êµ"]},{"cell_type":"code","execution_count":null,"id":"6ee148e8","metadata":{"id":"6ee148e8"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","x = np.linspace(-3, 3, 100)\n","\n","# ReLU\n","relu = np.maximum(0, x)\n","\n","# LeakyReLU (alpha=0.2)\n","alpha = 0.2\n","leaky_relu = np.where(x > 0, x, alpha * x)\n","\n","# Tanh\n","tanh = np.tanh(x)\n","\n","fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n","\n","# ReLU\n","axes[0].plot(x, relu, \"b-\", linewidth=2)\n","axes[0].axhline(y=0, color=\"k\", linestyle=\"-\", linewidth=0.5)\n","axes[0].axvline(x=0, color=\"k\", linestyle=\"-\", linewidth=0.5)\n","axes[0].fill_between(\n","    x[x < 0], relu[x < 0], alpha=0.3, color=\"red\", label=\"Gradient=0 (ë¬¸ì œ!)\"\n",")\n","axes[0].set_title(\"ReLU (Generator ì€ë‹‰ì¸µ)\", fontsize=12)\n","axes[0].set_xlabel(\"x\")\n","axes[0].set_ylabel(\"f(x)\")\n","axes[0].legend()\n","axes[0].grid(True, alpha=0.3)\n","\n","# LeakyReLU\n","axes[1].plot(x, leaky_relu, \"g-\", linewidth=2)\n","axes[1].axhline(y=0, color=\"k\", linestyle=\"-\", linewidth=0.5)\n","axes[1].axvline(x=0, color=\"k\", linestyle=\"-\", linewidth=0.5)\n","axes[1].fill_between(\n","    x[x < 0], leaky_relu[x < 0], alpha=0.3, color=\"green\", label=\"Gradient=0.2 (í•´ê²°!)\"\n",")\n","axes[1].set_title(\"LeakyReLU (Discriminator ì€ë‹‰ì¸µ)\", fontsize=12)\n","axes[1].set_xlabel(\"x\")\n","axes[1].legend()\n","axes[1].grid(True, alpha=0.3)\n","\n","# Tanh\n","axes[2].plot(x, tanh, \"r-\", linewidth=2)\n","axes[2].axhline(y=0, color=\"k\", linestyle=\"-\", linewidth=0.5)\n","axes[2].axvline(x=0, color=\"k\", linestyle=\"-\", linewidth=0.5)\n","axes[2].axhline(y=1, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n","axes[2].axhline(y=-1, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n","axes[2].set_title(\"Tanh (Generator ì¶œë ¥ì¸µ)\", fontsize=12)\n","axes[2].set_xlabel(\"x\")\n","axes[2].set_ylim(-1.5, 1.5)\n","axes[2].annotate(\"ì¶œë ¥ ë²”ìœ„: [-1, 1]\", xy=(1.5, 0.8), fontsize=10)\n","axes[2].grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"e7a16549","metadata":{"id":"e7a16549"},"source":["### ğŸ“Œ ì›ì¹™ 5: Optimizer ì„¤ì •\n","\n","#### DCGAN ê¶Œì¥ ì„¤ì •\n","\n","```python\n","optimizer = Adam(\n","    learning_rate=0.0002,  # ë‚®ì€ í•™ìŠµë¥ \n","    beta_1=0.5,            # ëª¨ë©˜í…€ ê°ì†Œ (ê¸°ë³¸ê°’ 0.9ì—ì„œ ë³€ê²½)\n","    beta_2=0.999           # ê¸°ë³¸ê°’ ìœ ì§€\n",")\n","```\n","\n","#### ì™œ Î²1=0.5ì¸ê°€?\n","\n","| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | DCGAN | ì´ìœ  |\n","|----------|--------|-------|------|\n","| Î²1 | 0.9 | **0.5** | ê³¼ê±° gradient ì˜ì¡´ë„ ê°ì†Œ â†’ ë¶ˆì•ˆì •í•œ GANì— ì í•© |\n","| Î²2 | 0.999 | 0.999 | ìœ ì§€ |\n","| lr | 0.001 | **0.0002** | ë‚®ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì  í•™ìŠµ |\n","\n","**Î²1ì˜ ì—­í• :**\n","- Î²1ì´ í¬ë©´ (0.9): ê³¼ê±° gradientë¥¼ ë§ì´ ì°¸ê³  â†’ ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦¼\n","- Î²1ì´ ì‘ìœ¼ë©´ (0.5): í˜„ì¬ gradientì— ë” ì§‘ì¤‘ â†’ ë¹ ë¥´ì§€ë§Œ ë¶ˆì•ˆì •\n","- GANì€ ë³¸ì§ˆì ìœ¼ë¡œ ë¶ˆì•ˆì •í•˜ë¯€ë¡œ, ì•½ê°„ ë‚®ì€ Î²1ì´ ë” ì˜ ì‘ë™\n","\n","#### ğŸ”§ ì½”ë“œ ì˜ˆì‹œ"]},{"cell_type":"code","execution_count":null,"id":"132269fc","metadata":{"id":"132269fc"},"outputs":[],"source":["# DCGAN í‘œì¤€ Optimizer ì„¤ì •\n","optimizer_g = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n","optimizer_d = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n","\n","print(\"âœ… DCGAN Optimizer ì„¤ì •:\")\n","print(f\"   - Learning Rate: 0.0002\")\n","print(f\"   - Beta 1: 0.5 (ê¸°ë³¸ê°’ 0.9ì—ì„œ ë³€ê²½)\")\n","print(f\"   - Beta 2: 0.999 (ê¸°ë³¸ê°’ ìœ ì§€)\")"]},{"cell_type":"markdown","id":"566d0697","metadata":{"id":"566d0697"},"source":["### ğŸ“Š 5ê°€ì§€ ì›ì¹™ ìš”ì•½í‘œ\n","\n","| ì›ì¹™ | ê¸°ì¡´ ë°©ì‹ | DCGAN | íš¨ê³¼ |\n","|------|-----------|-------|------|\n","| 1. ë‹¤ìš´ìƒ˜í”Œë§ | MaxPooling | **Strided Conv** | í•™ìŠµ ê°€ëŠ¥í•œ ë‹¤ìš´ìƒ˜í”Œë§ |\n","| 2. ì¤‘ê°„ì¸µ | FC Layer | **Convë§Œ ì‚¬ìš©** | ê³µê°„ êµ¬ì¡° ë³´ì¡´, íŒŒë¼ë¯¸í„° íš¨ìœ¨ |\n","| 3. ì •ê·œí™” | ì—†ìŒ | **BatchNorm** | í•™ìŠµ ì•ˆì •í™” |\n","| 4. í™œì„±í™” (D) | ReLU | **LeakyReLU** | Dying ReLU ë°©ì§€ |\n","| 5. Optimizer | Adam ê¸°ë³¸ê°’ | **Adam (Î²1=0.5)** | GANì— ìµœì í™”ëœ ì„¤ì • |"]},{"cell_type":"markdown","id":"a86d19c7","metadata":{"id":"a86d19c7"},"source":["## 2.2 í•µì‹¬ ì—°ì‚°: Transposed Convolution\n","\n","DCGAN Generatorì˜ í•µì‹¬ì¸ **Transposed Convolution**ì„ ìƒì„¸íˆ ì´í•´í•©ë‹ˆë‹¤.\n","\n","---"]},{"cell_type":"markdown","id":"d290db90","metadata":{"id":"d290db90"},"source":["### Transposed Convolutionì´ë€?\n","\n","**ë‹¤ë¥¸ ì´ë¦„ë“¤:**\n","- Deconvolution (ì—„ë°€íˆëŠ” í‹€ë¦° í‘œí˜„)\n","- Fractionally-Strided Convolution\n","- Upconvolution\n","\n","**ëª©ì :** ì‘ì€ feature mapì„ í° feature mapìœ¼ë¡œ **ì—…ìƒ˜í”Œë§**\n","\n","```\n","ì¼ë°˜ Conv (ë‹¤ìš´ìƒ˜í”Œë§):     Transposed Conv (ì—…ìƒ˜í”Œë§):\n","\n","â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”          â”Œâ”€â”€â”€â”¬â”€â”€â”€â”\n","â”‚   â”‚   â”‚   â”‚   â”‚          â”‚   â”‚   â”‚\n","â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤   â†’      â”œâ”€â”€â”€â”¼â”€â”€â”€â”¤\n","â”‚   â”‚   â”‚   â”‚   â”‚  Conv    â”‚   â”‚   â”‚\n","â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤          â””â”€â”€â”€â”´â”€â”€â”€â”˜\n","â”‚   â”‚   â”‚   â”‚   â”‚            2Ã—2\n","â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n","â”‚   â”‚   â”‚   â”‚   â”‚          â”Œâ”€â”€â”€â”¬â”€â”€â”€â”          â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”\n","â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜          â”‚   â”‚   â”‚   â†’      â”‚   â”‚   â”‚   â”‚   â”‚\n","      4Ã—4                  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¤ TransConvâ”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n","                           â”‚   â”‚   â”‚          â”‚   â”‚   â”‚   â”‚   â”‚\n","                           â””â”€â”€â”€â”´â”€â”€â”€â”˜          â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤\n","                             2Ã—2              â”‚   â”‚   â”‚   â”‚   â”‚\n","                                              â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\n","                                                    4Ã—4\n","```"]},{"cell_type":"markdown","id":"79b4ef11","metadata":{"id":"79b4ef11"},"source":["### ë™ì‘ ì›ë¦¬ ì‹œê°í™”\n","\n","**í•µì‹¬ ì•„ì´ë””ì–´:** ì…ë ¥ì˜ ê° í”½ì…€ì´ ì»¤ë„ í¬ê¸°ë§Œí¼ì˜ ì¶œë ¥ ì˜ì—­ì— ê¸°ì—¬\n","\n","![](https://media.geeksforgeeks.org/wp-content/uploads/20230110162532/Transposed-Convolutional-2.png)"]},{"cell_type":"markdown","id":"270aa0a6","metadata":{"id":"270aa0a6"},"source":["### ì¶œë ¥ í¬ê¸° ê³„ì‚° ê³µì‹\n","\n","**Transposed Convolution ì¶œë ¥ í¬ê¸°:**\n","\n","$$\n","H_{out} = (H_{in} - 1) \\times stride - 2 \\times padding + kernel\\_size + output\\_padding\n","$$\n","\n","**DCGANì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì„¤ì • (2ë°° ì—…ìƒ˜í”Œë§):**\n","\n","```python\n","Conv2DTranspose(filters, kernel_size=4, strides=2, padding='same')\n","# padding='same'ì¼ ë•Œ: ì¶œë ¥ = ì…ë ¥ Ã— stride\n","# ì˜ˆ: 4Ã—4 â†’ 8Ã—8, 8Ã—8 â†’ 16Ã—16, ...\n","```\n","\n","| ì…ë ¥ í¬ê¸° | kernel | stride | padding | ì¶œë ¥ í¬ê¸° |\n","|-----------|--------|--------|---------|-----------|\n","| 4Ã—4 | 4 | 2 | same | **8Ã—8** |\n","| 8Ã—8 | 4 | 2 | same | **16Ã—16** |\n","| 16Ã—16 | 4 | 2 | same | **32Ã—32** |\n","| 32Ã—32 | 4 | 2 | same | **64Ã—64** |"]},{"cell_type":"code","execution_count":null,"id":"7edc3329","metadata":{"id":"7edc3329"},"outputs":[],"source":["# Transposed Convolution ì‹¤ì œ ë™ì‘ í™•ì¸\n","import tensorflow as tf\n","\n","# ì˜ˆì‹œ ì…ë ¥: 4Ã—4Ã—256 feature map\n","example_input = tf.random.normal([1, 4, 4, 256])\n","print(f\"ì…ë ¥ shape: {example_input.shape}\")  # (1, 4, 4, 256)\n","\n","# Transposed Convë¡œ ì—…ìƒ˜í”Œë§\n","# kernel_size=4, strides=2, padding='same' â†’ í¬ê¸° 2ë°°\n","trans_conv = tf.keras.layers.Conv2DTranspose(\n","    filters=128,  # ì¶œë ¥ ì±„ë„ ìˆ˜\n","    kernel_size=4,  # ì»¤ë„ í¬ê¸°\n","    strides=2,  # ìŠ¤íŠ¸ë¼ì´ë“œ (2ë°° ì—…ìƒ˜í”Œë§)\n","    padding=\"same\",  # ì¶œë ¥ í¬ê¸° = ì…ë ¥ í¬ê¸° Ã— stride\n","    use_bias=False,  # BatchNorm ì‚¬ìš© ì‹œ bias ë¶ˆí•„ìš”\n",")\n","\n","output = trans_conv(example_input)\n","print(f\"ì¶œë ¥ shape: {output.shape}\")  # (1, 8, 8, 128)\n","print(\n","    f\"í¬ê¸° ë³€í™”: {example_input.shape[1]}Ã—{example_input.shape[2]} â†’ {output.shape[1]}Ã—{output.shape[2]}\"\n",")"]},{"cell_type":"code","execution_count":null,"id":"7a0beec8","metadata":{"id":"7a0beec8"},"outputs":[],"source":["# DCGAN Generatorì˜ ì „ì²´ shape ë³€í™” ì¶”ì \n","print(\"=\" * 60)\n","print(\"DCGAN Generator Shape ë³€í™” (CelebA 64Ã—64)\")\n","print(\"=\" * 60)\n","\n","# ê° ì¸µë³„ shape ë³€í™” ì‹œë®¬ë ˆì´ì…˜\n","shapes = [\n","    (\"ì…ë ¥ ë…¸ì´ì¦ˆ z\", \"(batch, 100)\"),\n","    (\"Dense + Reshape\", \"(batch, 4, 4, 512)\"),\n","    (\"TransConv 1 (4â†’8)\", \"(batch, 8, 8, 256)\"),\n","    (\"TransConv 2 (8â†’16)\", \"(batch, 16, 16, 128)\"),\n","    (\"TransConv 3 (16â†’32)\", \"(batch, 32, 32, 64)\"),\n","    (\"TransConv 4 (32â†’64)\", \"(batch, 64, 64, 3)\"),\n","]\n","\n","for layer_name, shape in shapes:\n","    print(f\"{layer_name:25} â†’ {shape}\")\n","\n","print(\"\\nâ€» ê° TransConv: kernel=4, stride=2, padding='same'\")"]},{"cell_type":"markdown","id":"d451bcf3","metadata":{"id":"d451bcf3"},"source":["### Transposed Conv vs Upsampling + Conv\n","\n","ì—…ìƒ˜í”Œë§ì„ í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì„ ë¹„êµí•©ë‹ˆë‹¤.\n","\n","#### ë°©ë²• 1: Transposed Convolution (DCGAN ì‚¬ìš©)\n","\n","```python\n","x = Conv2DTranspose(64, 4, strides=2, padding='same')(x)\n","```\n","\n","#### ë°©ë²• 2: Upsampling + Convolution\n","\n","```python\n","x = UpSampling2D(size=2)(x)  # Nearest neighbor ë˜ëŠ” Bilinear\n","x = Conv2D(64, 3, padding='same')(x)\n","```\n","\n","#### ë¹„êµ\n","\n","| í•­ëª© | Transposed Conv | Upsampling + Conv |\n","|------|-----------------|-------------------|\n","| íŒŒë¼ë¯¸í„° ìˆ˜ | ì ìŒ (í•œ ì¸µ) | ë§ìŒ (ë‘ ì¸µ) |\n","| ê³„ì‚°ëŸ‰ | ì ìŒ | ë§ìŒ |\n","| Checkerboard Artifact | **ë°œìƒ ê°€ëŠ¥** | ì ìŒ |\n","| ìœ ì—°ì„± | ë‚®ìŒ | ë†’ìŒ |\n","\n","#### âš ï¸ Checkerboard Artifact\n","\n","Transposed Convolutionì—ì„œ strideì™€ kernel í¬ê¸°ê°€ ë§ì§€ ì•Šìœ¼ë©´ ë°œìƒí•˜ëŠ” ê²©ì íŒ¨í„´\n","\n","![](https://gaussian37.github.io/assets/img/dl/concept/checkboard_artifact/2.gif)\n","\n","```\n","ì •ìƒ ì´ë¯¸ì§€:              Checkerboard Artifact:\n","â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n","â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚           â”‚ â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆ â”‚\n","â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚           â”‚ â–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘ â”‚\n","â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚           â”‚ â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆâ–‘â–ˆ â”‚\n","â”‚ ë¶€ë“œëŸ¬ìš´ ê·¸ë¼ë°ì´ì…˜ â”‚           â”‚ ê²©ì ë¬´ëŠ¬ ë…¸ì´ì¦ˆâ”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","```\n","\n","**í•´ê²°ì±…:** kernel_sizeê°€ strideë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ê²Œ ì„¤ì • (ì˜ˆ: kernel=4, stride=2)"]},{"cell_type":"code","execution_count":null,"id":"6e2e368f","metadata":{"id":"6e2e368f"},"outputs":[],"source":["# Checkerboard Artifact ì‹œë®¬ë ˆì´ì…˜\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n","\n","# ì •ìƒì ì¸ ì—…ìƒ˜í”Œë§ (kernel=4, stride=2)\n","normal = np.random.rand(8, 8)\n","normal = np.kron(normal, np.ones((8, 8)))  # ë¶€ë“œëŸ¬ìš´ í™•ëŒ€\n","\n","# Checkerboard artifact (kernel=3, stride=2 - ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ì§€ ì•ŠìŒ)\n","checker = np.zeros((64, 64))\n","checker[::2, ::2] = 1\n","checker[1::2, 1::2] = 1\n","artifact = normal * (0.8 + 0.2 * checker)  # ê²©ì íŒ¨í„´ ì¶”ê°€\n","\n","axes[0].imshow(normal, cmap=\"gray\")\n","axes[0].set_title(\"ì •ìƒ (kernel=4, stride=2)\", fontsize=12)\n","axes[0].axis(\"off\")\n","\n","axes[1].imshow(artifact, cmap=\"gray\")\n","axes[1].set_title(\"Checkerboard Artifact\", fontsize=12)\n","axes[1].axis(\"off\")\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"ğŸ’¡ í•´ê²°ì±…: kernel_sizeê°€ strideë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ë„ë¡ ì„¤ì •\")\n","print(\"   ì˜ˆ: kernel=4, stride=2 (4Ã·2=2) âœ…\")\n","print(\"   ì˜ˆ: kernel=3, stride=2 (3Ã·2=1.5) âŒ\")"]},{"cell_type":"markdown","id":"e50540ae","metadata":{"id":"e50540ae"},"source":["## 2.3 Generator ì•„í‚¤í…ì²˜ (64Ã—64 CelebA)\n","\n","```\n","ì…ë ¥: z (100ì°¨ì› ë…¸ì´ì¦ˆ)\n","  â”‚\n","  â–¼\n","Dense(4Ã—4Ã—512) + Reshape\n","  â”‚ shape: (4, 4, 512)\n","  â–¼\n","ConvTranspose(256) + BN + ReLU\n","  â”‚ shape: (8, 8, 256)\n","  â–¼\n","ConvTranspose(128) + BN + ReLU\n","  â”‚ shape: (16, 16, 128)\n","  â–¼\n","ConvTranspose(64) + BN + ReLU\n","  â”‚ shape: (32, 32, 64)\n","  â–¼\n","ConvTranspose(3) + Tanh\n","  â”‚ shape: (64, 64, 3)\n","  â–¼\n","ì¶œë ¥: ê°€ì§œ ì´ë¯¸ì§€ (64Ã—64Ã—3)\n","```"]},{"cell_type":"markdown","id":"1a500987","metadata":{"id":"1a500987"},"source":["## 2.4 Discriminator ì•„í‚¤í…ì²˜ (64Ã—64 CelebA)\n","\n","```\n","ì…ë ¥: ì´ë¯¸ì§€ (64Ã—64Ã—3)\n","  â”‚\n","  â–¼\n","Conv(64, stride=2) + LeakyReLU\n","  â”‚ shape: (32, 32, 64)\n","  â–¼\n","Conv(128, stride=2) + BN + LeakyReLU\n","  â”‚ shape: (16, 16, 128)\n","  â–¼\n","Conv(256, stride=2) + BN + LeakyReLU\n","  â”‚ shape: (8, 8, 256)\n","  â–¼\n","Conv(512, stride=2) + BN + LeakyReLU\n","  â”‚ shape: (4, 4, 512)\n","  â–¼\n","Conv(1, stride=1) + Sigmoid (ë˜ëŠ” Flatten + Dense)\n","  â”‚ shape: (1,)\n","  â–¼\n","ì¶œë ¥: ì§„ì§œì¼ í™•ë¥  (0~1)\n","```"]},{"cell_type":"markdown","id":"3d2f802b","metadata":{"id":"3d2f802b"},"source":["## 3.1 í™˜ê²½ ì„¤ì • ë° CelebA ë°ì´í„° ì¤€ë¹„"]},{"cell_type":"markdown","metadata":{"id":"911ab6c2"},"source":["---\n","# ğŸ• 3êµì‹œ: DCGAN êµ¬í˜„ (TensorFlow + MNIST)\n","\n","---"],"id":"911ab6c2"},{"cell_type":"markdown","metadata":{"id":"4fe17fec"},"source":["## 3.1 í™˜ê²½ ì„¤ì • ë° MNIST ë°ì´í„° ì¤€ë¹„"],"id":"4fe17fec"},{"cell_type":"code","execution_count":null,"metadata":{"id":"063e1084"},"outputs":[],"source":["# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","# GPU í™•ì¸\n","print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n","print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {tf.config.list_physical_devices('GPU')}\")\n","\n","# ëœë¤ ì‹œë“œ ì„¤ì •\n","tf.random.set_seed(42)\n","np.random.seed(42)"],"id":"063e1084"},{"cell_type":"markdown","metadata":{"id":"bfbc7b81"},"source":["### MNIST ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬\n","\n","**MNIST (Modified National Institute of Standards and Technology):**\n","- 60,000ì¥ì˜ ì†ê¸€ì”¨ ìˆ«ì ì´ë¯¸ì§€ (í•™ìŠµìš©)\n","- 10,000ì¥ì˜ ì†ê¸€ì”¨ ìˆ«ì ì´ë¯¸ì§€ (í…ŒìŠ¤íŠ¸ìš©)\n","- 28Ã—28 í”½ì…€, í‘ë°±(grayscale)\n","- GAN í•™ìŠµì— ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ê¸°ë³¸ ë°ì´í„°ì…‹"],"id":"bfbc7b81"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5dc4033f"},"outputs":[],"source":["# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n","IMG_SIZE = 28  # ì´ë¯¸ì§€ í¬ê¸° (28Ã—28)\n","CHANNELS = 1  # Grayscale\n","BATCH_SIZE = 128  # ë°°ì¹˜ í¬ê¸°\n","NOISE_DIM = 100  # ë…¸ì´ì¦ˆ ë²¡í„° ì°¨ì›\n","EPOCHS = 50  # í•™ìŠµ ì—í­ ìˆ˜\n","\n","print(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\")\n","print(f\"   - ì´ë¯¸ì§€ í¬ê¸°: {IMG_SIZE}Ã—{IMG_SIZE}Ã—{CHANNELS}\")\n","print(f\"   - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n","print(f\"   - ë…¸ì´ì¦ˆ ì°¨ì›: {NOISE_DIM}\")\n","print(f\"   - í•™ìŠµ ì—í­: {EPOCHS}\")"],"id":"5dc4033f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4f0c59f2"},"outputs":[],"source":["# MNIST ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬\n","def load_mnist_dataset(batch_size=128):\n","    \"\"\"\n","    MNIST ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n","\n","    Args:\n","        batch_size: ë°°ì¹˜ í¬ê¸°\n","\n","    Returns:\n","        tf.data.Dataset ê°ì²´\n","    \"\"\"\n","    # MNIST ë°ì´í„°ì…‹ ë¡œë“œ (ìë™ ë‹¤ìš´ë¡œë“œ)\n","    (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n","\n","    # ë°ì´í„° ì „ì²˜ë¦¬\n","    # 1. ì°¨ì› ì¶”ê°€: (60000, 28, 28) â†’ (60000, 28, 28, 1)\n","    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(\n","        \"float32\"\n","    )\n","\n","    # 2. [0, 255] â†’ [-1, 1] ì •ê·œí™” (Tanh ì¶œë ¥ê³¼ ì¼ì¹˜)\n","    train_images = (train_images - 127.5) / 127.5\n","\n","    # 3. tf.data.Datasetìœ¼ë¡œ ë³€í™˜\n","    dataset = tf.data.Dataset.from_tensor_slices(train_images)\n","    dataset = dataset.shuffle(60000).batch(batch_size, drop_remainder=True)\n","    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n","\n","    return dataset\n","\n","\n","# ë°ì´í„°ì…‹ ìƒì„±\n","dataset = load_mnist_dataset(batch_size=BATCH_SIZE)\n","\n","print(f\"\\nâœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n","print(f\"   - ì´ ìƒ˜í”Œ ìˆ˜: 60,000\")\n","print(f\"   - ë°°ì¹˜ ìˆ˜: {len(list(dataset))}\")"],"id":"4f0c59f2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dd9e7e19"},"outputs":[],"source":["# ë°ì´í„° ìƒ˜í”Œ ì‹œê°í™”\n","sample_batch = next(iter(dataset))\n","print(f\"ë°°ì¹˜ shape: {sample_batch.shape}\")  # (128, 28, 28, 1)\n","print(f\"ê°’ ë²”ìœ„: [{sample_batch.numpy().min():.2f}, {sample_batch.numpy().max():.2f}]\")\n","\n","fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n","for i, ax in enumerate(axes.flat):\n","    # [-1, 1] â†’ [0, 1]ë¡œ ë³€í™˜í•˜ì—¬ ì‹œê°í™”\n","    img = (sample_batch[i].numpy() + 1) / 2\n","    ax.imshow(img.squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"MNIST ë°ì´í„° ìƒ˜í”Œ\", fontsize=14)\n","plt.tight_layout()\n","plt.show()"],"id":"dd9e7e19"},{"cell_type":"markdown","metadata":{"lines_to_next_cell":2,"id":"7569ab9d"},"source":["## 3.2 Generator êµ¬í˜„\n","\n","DCGAN GeneratorëŠ” 100ì°¨ì› ë…¸ì´ì¦ˆë¥¼ 28Ã—28Ã—1 ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n","MNISTëŠ” 28Ã—28ì´ë¯€ë¡œ 7Ã—7 â†’ 14Ã—14 â†’ 28Ã—28 êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."],"id":"7569ab9d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"855cda14"},"outputs":[],"source":["def build_generator(noise_dim=100):\n","    \"\"\"\n","    DCGAN Generator ìƒì„± (MNISTìš©)\n","\n","    êµ¬ì¡°:\n","        z (100) â†’ Dense(7Ã—7Ã—256) â†’ Reshape\n","               â†’ ConvTranspose(128) â†’ 14Ã—14\n","               â†’ ConvTranspose(1)   â†’ 28Ã—28\n","\n","    Args:\n","        noise_dim: ì…ë ¥ ë…¸ì´ì¦ˆ ë²¡í„°ì˜ ì°¨ì› (ê¸°ë³¸ê°’: 100)\n","\n","    Returns:\n","        tf.keras.Model: Generator ëª¨ë¸\n","    \"\"\"\n","\n","    model = tf.keras.Sequential(name=\"Generator\")\n","\n","    # ========================================\n","    # ì²« ë²ˆì§¸ ì¸µ: Dense + Reshape\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 100)\n","    # ì¶œë ¥: (batch_size, 7, 7, 256)\n","\n","    # Dense: 100 â†’ 7Ã—7Ã—256 = 12544\n","    model.add(\n","        layers.Dense(\n","            7 * 7 * 256,  # ì¶œë ¥ ìœ ë‹› ìˆ˜: 12544\n","            use_bias=False,  # BatchNorm ì‚¬ìš© ì‹œ bias ë¶ˆí•„ìš”\n","            input_shape=(noise_dim,),  # ì…ë ¥ shape: (100,)\n","        )\n","    )\n","    # ì˜ˆì‹œ: (batch=128, 100) â†’ (128, 12544)\n","\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.ReLU())\n","\n","    # Reshape: 1D â†’ 4D\n","    model.add(layers.Reshape((7, 7, 256)))\n","    # ì˜ˆì‹œ: (128, 12544) â†’ (128, 7, 7, 256)\n","\n","    # ========================================\n","    # ConvTranspose 1: 7Ã—7 â†’ 14Ã—14\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 7, 7, 256)\n","    # ì¶œë ¥: (batch_size, 14, 14, 128)\n","\n","    model.add(\n","        layers.Conv2DTranspose(\n","            filters=128,  # ì¶œë ¥ ì±„ë„ ìˆ˜\n","            kernel_size=5,  # ì»¤ë„ í¬ê¸° (5Ã—5)\n","            strides=2,  # ìŠ¤íŠ¸ë¼ì´ë“œ 2 â†’ í¬ê¸° 2ë°°\n","            padding=\"same\",  # ì¶œë ¥ í¬ê¸° = ì…ë ¥ Ã— stride\n","            use_bias=False,  # BatchNorm ì‚¬ìš© ì‹œ bias ë¶ˆí•„ìš”\n","        )\n","    )\n","    # ê³„ì‚°: 7 Ã— 2 = 14 (padding='same'ì´ë¯€ë¡œ)\n","    # ì˜ˆì‹œ: (128, 7, 7, 256) â†’ (128, 14, 14, 128)\n","\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.ReLU())\n","\n","    # ========================================\n","    # ConvTranspose 2 (ì¶œë ¥ì¸µ): 14Ã—14 â†’ 28Ã—28\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 14, 14, 128)\n","    # ì¶œë ¥: (batch_size, 28, 28, 1)\n","\n","    model.add(\n","        layers.Conv2DTranspose(\n","            filters=1,  # Grayscale 1ì±„ë„\n","            kernel_size=5,\n","            strides=2,\n","            padding=\"same\",\n","            use_bias=False,\n","            activation=\"tanh\",  # ì¶œë ¥ ë²”ìœ„: [-1, 1]\n","        )\n","    )\n","    # ì˜ˆì‹œ: (128, 14, 14, 128) â†’ (128, 28, 28, 1)\n","    # ì£¼ì˜: ì¶œë ¥ì¸µì—ëŠ” BatchNorm ì—†ìŒ! (Tanh ë²”ìœ„ ìœ ì§€)\n","\n","    return model\n","\n","\n","# Generator ìƒì„±\n","generator = build_generator(NOISE_DIM)\n","generator.summary()"],"id":"855cda14"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aaba415"},"outputs":[],"source":["# Generator í…ŒìŠ¤íŠ¸\n","print(\"\\nğŸ§ª Generator í…ŒìŠ¤íŠ¸\")\n","print(\"=\" * 50)\n","\n","# ëœë¤ ë…¸ì´ì¦ˆ ìƒì„±\n","test_noise = tf.random.normal([4, NOISE_DIM])\n","print(f\"ì…ë ¥ ë…¸ì´ì¦ˆ shape: {test_noise.shape}\")  # (4, 100)\n","\n","# Generator í†µê³¼\n","test_output = generator(test_noise, training=False)\n","print(f\"ì¶œë ¥ ì´ë¯¸ì§€ shape: {test_output.shape}\")  # (4, 28, 28, 1)\n","print(\n","    f\"ì¶œë ¥ ê°’ ë²”ìœ„: [{test_output.numpy().min():.3f}, {test_output.numpy().max():.3f}]\"\n",")\n","\n","# ìƒì„±ëœ ì´ë¯¸ì§€ ì‹œê°í™” (í•™ìŠµ ì „)\n","fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n","for i, ax in enumerate(axes):\n","    img = (test_output[i].numpy() + 1) / 2  # [-1,1] â†’ [0,1]\n","    ax.imshow(img.squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"Generator ì´ˆê¸° ì¶œë ¥ (í•™ìŠµ ì „ - ë…¸ì´ì¦ˆ)\", fontsize=14)\n","plt.tight_layout()\n","plt.show()"],"id":"6aaba415"},{"cell_type":"markdown","metadata":{"lines_to_next_cell":2,"id":"6f62e004"},"source":["## 3.3 Discriminator êµ¬í˜„\n","\n","DCGAN DiscriminatorëŠ” 28Ã—28Ã—1 ì´ë¯¸ì§€ë¥¼ ë°›ì•„ ì§„ì§œ/ê°€ì§œë¥¼ íŒë³„í•©ë‹ˆë‹¤."],"id":"6f62e004"},{"cell_type":"code","execution_count":null,"metadata":{"id":"335c74c3"},"outputs":[],"source":["def build_discriminator():\n","    \"\"\"\n","    DCGAN Discriminator ìƒì„± (MNISTìš©)\n","\n","    êµ¬ì¡°:\n","        ì´ë¯¸ì§€ (28Ã—28Ã—1) â†’ Conv(64)  â†’ 14Ã—14\n","                        â†’ Conv(128) â†’ 7Ã—7\n","                        â†’ Flatten â†’ Dense(1)\n","\n","    Returns:\n","        tf.keras.Model: Discriminator ëª¨ë¸\n","    \"\"\"\n","\n","    model = tf.keras.Sequential(name=\"Discriminator\")\n","\n","    # ========================================\n","    # Conv 1 (ì…ë ¥ì¸µ): 28Ã—28 â†’ 14Ã—14\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 28, 28, 1)\n","    # ì¶œë ¥: (batch_size, 14, 14, 64)\n","\n","    model.add(\n","        layers.Conv2D(\n","            filters=64,  # ì¶œë ¥ ì±„ë„ ìˆ˜\n","            kernel_size=5,  # ì»¤ë„ í¬ê¸° (5Ã—5)\n","            strides=2,  # ìŠ¤íŠ¸ë¼ì´ë“œ 2 â†’ í¬ê¸° 1/2\n","            padding=\"same\",  # ì¶œë ¥ í¬ê¸° = ì…ë ¥ / stride\n","            input_shape=(28, 28, 1),  # ì…ë ¥ shape\n","            use_bias=False,\n","        )\n","    )\n","    # ê³„ì‚°: 28 / 2 = 14 (padding='same'ì´ë¯€ë¡œ)\n","    # ì˜ˆì‹œ: (128, 28, 28, 1) â†’ (128, 14, 14, 64)\n","    # ì£¼ì˜: ì…ë ¥ì¸µì—ëŠ” BatchNorm ì—†ìŒ! (ì›ë³¸ ë¶„í¬ ìœ ì§€)\n","\n","    model.add(layers.LeakyReLU(0.2))  # ìŒìˆ˜ ì˜ì—­ì—ì„œë„ gradient ìœ ì§€\n","\n","    # ========================================\n","    # Conv 2: 14Ã—14 â†’ 7Ã—7\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 14, 14, 64)\n","    # ì¶œë ¥: (batch_size, 7, 7, 128)\n","\n","    model.add(\n","        layers.Conv2D(\n","            filters=128,\n","            kernel_size=5,\n","            strides=2,\n","            padding=\"same\",\n","            use_bias=False,\n","        )\n","    )\n","    # ì˜ˆì‹œ: (128, 14, 14, 64) â†’ (128, 7, 7, 128)\n","\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(0.2))\n","\n","    # ========================================\n","    # ì¶œë ¥ì¸µ: Flatten + Dense(1)\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 7, 7, 128)\n","    # ì¶œë ¥: (batch_size, 1)\n","\n","    model.add(layers.Flatten())\n","    # ì˜ˆì‹œ: (128, 7, 7, 128) â†’ (128, 6272)\n","\n","    model.add(layers.Dense(1))  # Sigmoid ì—†ìŒ â†’ BCEWithLogitsLoss ì‚¬ìš©\n","    # ì˜ˆì‹œ: (128, 6272) â†’ (128, 1)\n","\n","    return model\n","\n","\n","# Discriminator ìƒì„±\n","discriminator = build_discriminator()\n","discriminator.summary()"],"id":"335c74c3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5d8e9861"},"outputs":[],"source":["# Discriminator í…ŒìŠ¤íŠ¸\n","print(\"\\nğŸ§ª Discriminator í…ŒìŠ¤íŠ¸\")\n","print(\"=\" * 50)\n","\n","# ì§„ì§œ ì´ë¯¸ì§€ (ë°°ì¹˜ì—ì„œ ìƒ˜í”Œ)\n","real_batch = next(iter(dataset))[:4]\n","print(f\"ì§„ì§œ ì´ë¯¸ì§€ shape: {real_batch.shape}\")  # (4, 28, 28, 1)\n","\n","# ê°€ì§œ ì´ë¯¸ì§€ (Generator ì¶œë ¥)\n","fake_batch = generator(tf.random.normal([4, NOISE_DIM]), training=False)\n","print(f\"ê°€ì§œ ì´ë¯¸ì§€ shape: {fake_batch.shape}\")  # (4, 28, 28, 1)\n","\n","# Discriminator í†µê³¼\n","real_output = discriminator(real_batch, training=False)\n","fake_output = discriminator(fake_batch, training=False)\n","\n","print(f\"\\nì§„ì§œ ì´ë¯¸ì§€ íŒë³„ ê²°ê³¼: {real_output.numpy().flatten()}\")\n","print(f\"ê°€ì§œ ì´ë¯¸ì§€ íŒë³„ ê²°ê³¼: {fake_output.numpy().flatten()}\")\n","print(\"\\n(í•™ìŠµ ì „ì´ë¼ ëœë¤í•œ ê°’)\")"],"id":"5d8e9861"},{"cell_type":"markdown","metadata":{"id":"578a334a"},"source":["## 3.4 ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •"],"id":"578a334a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cc1e26e3"},"outputs":[],"source":["# Binary Cross Entropy with Logits\n","# Discriminator ì¶œë ¥ì— Sigmoidê°€ ì—†ìœ¼ë¯€ë¡œ from_logits=True\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","\n","def discriminator_loss(real_output, fake_output):\n","    \"\"\"\n","    Discriminator ì†ì‹¤ í•¨ìˆ˜\n","\n","    ëª©í‘œ: ì§„ì§œëŠ” 1, ê°€ì§œëŠ” 0ìœ¼ë¡œ ì •í™•íˆ ë¶„ë¥˜\n","\n","    Args:\n","        real_output: D(real_images) - ì§„ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ ê²°ê³¼\n","        fake_output: D(fake_images) - ê°€ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ ê²°ê³¼\n","\n","    Returns:\n","        total_loss: ì´ Discriminator ì†ì‹¤\n","    \"\"\"\n","    # ì§„ì§œ ì´ë¯¸ì§€ â†’ 1ë¡œ íŒë³„í•´ì•¼ í•¨\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","\n","    # ê°€ì§œ ì´ë¯¸ì§€ â†’ 0ìœ¼ë¡œ íŒë³„í•´ì•¼ í•¨\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","\n","def generator_loss(fake_output):\n","    \"\"\"\n","    Generator ì†ì‹¤ í•¨ìˆ˜\n","\n","    ëª©í‘œ: Discriminatorë¥¼ ì†ì—¬ì„œ ê°€ì§œë¥¼ ì§„ì§œ(1)ë¡œ íŒë³„í•˜ê²Œ ë§Œë“¤ê¸°\n","\n","    Args:\n","        fake_output: D(G(z)) - ê°€ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ ê²°ê³¼\n","\n","    Returns:\n","        loss: Generator ì†ì‹¤\n","    \"\"\"\n","    # ê°€ì§œ ì´ë¯¸ì§€ê°€ 1ë¡œ íŒë³„ë˜ê¸°ë¥¼ ì›í•¨ (Dë¥¼ ì†ì´ê¸°)\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","\n","# DCGAN í‘œì¤€ Optimizer ì„¤ì •\n","generator_optimizer = tf.keras.optimizers.Adam(\n","    learning_rate=0.0002,\n","    beta_1=0.5,  # ê¸°ë³¸ê°’ 0.9ì—ì„œ ë³€ê²½ (DCGAN ê¶Œì¥)\n","    beta_2=0.999,\n",")\n","\n","discriminator_optimizer = tf.keras.optimizers.Adam(\n","    learning_rate=0.0002, beta_1=0.5, beta_2=0.999\n",")\n","\n","print(\"âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ\")\n","print(f\"   - Loss: BCE with Logits\")\n","print(f\"   - Optimizer: Adam (lr=0.0002, Î²1=0.5)\")"],"id":"cc1e26e3"},{"cell_type":"markdown","metadata":{"lines_to_next_cell":2,"id":"ce6e952c"},"source":["## 3.5 í•™ìŠµ Step í•¨ìˆ˜ êµ¬í˜„"],"id":"ce6e952c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f52626b6"},"outputs":[],"source":["@tf.function\n","def train_step(real_images):\n","    \"\"\"\n","    DCGAN í•œ ìŠ¤í… í•™ìŠµ\n","\n","    Args:\n","        real_images: ì§„ì§œ ì´ë¯¸ì§€ ë°°ì¹˜ (batch_size, 28, 28, 1)\n","\n","    Returns:\n","        d_loss: Discriminator ì†ì‹¤\n","        g_loss: Generator ì†ì‹¤\n","    \"\"\"\n","    # í˜„ì¬ ë°°ì¹˜ í¬ê¸°\n","    batch_size = tf.shape(real_images)[0]\n","\n","    # ë…¸ì´ì¦ˆ ìƒì„±\n","    noise = tf.random.normal([batch_size, NOISE_DIM])\n","\n","    # ========================================\n","    # Discriminator í•™ìŠµ\n","    # ========================================\n","    with tf.GradientTape() as disc_tape:\n","        # Generatorë¡œ ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±\n","        fake_images = generator(noise, training=True)\n","\n","        # Discriminatorë¡œ ì§„ì§œ/ê°€ì§œ íŒë³„\n","        real_output = discriminator(real_images, training=True)\n","        fake_output = discriminator(fake_images, training=True)\n","\n","        # Discriminator ì†ì‹¤ ê³„ì‚°\n","        d_loss = discriminator_loss(real_output, fake_output)\n","\n","    # Discriminator gradient ê³„ì‚° ë° ì ìš©\n","    gradients_of_discriminator = disc_tape.gradient(\n","        d_loss, discriminator.trainable_variables\n","    )\n","    discriminator_optimizer.apply_gradients(\n","        zip(gradients_of_discriminator, discriminator.trainable_variables)\n","    )\n","\n","    # ========================================\n","    # Generator í•™ìŠµ\n","    # ========================================\n","    # ìƒˆë¡œìš´ ë…¸ì´ì¦ˆ ìƒì„±\n","    noise = tf.random.normal([batch_size, NOISE_DIM])\n","\n","    with tf.GradientTape() as gen_tape:\n","        # Generatorë¡œ ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±\n","        fake_images = generator(noise, training=True)\n","\n","        # Discriminatorë¡œ íŒë³„\n","        fake_output = discriminator(fake_images, training=True)\n","\n","        # Generator ì†ì‹¤ ê³„ì‚°\n","        g_loss = generator_loss(fake_output)\n","\n","    # Generator gradient ê³„ì‚° ë° ì ìš©\n","    gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n","    generator_optimizer.apply_gradients(\n","        zip(gradients_of_generator, generator.trainable_variables)\n","    )\n","\n","    return d_loss, g_loss\n","\n","\n","print(\"âœ… train_step í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"],"id":"f52626b6"},{"cell_type":"markdown","metadata":{"id":"5556f704"},"source":["## 3.6 ì´ë¯¸ì§€ ìƒì„± ë° ì‹œê°í™” í•¨ìˆ˜"],"id":"5556f704"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cf4386c"},"outputs":[],"source":["# ê³ ì • ë…¸ì´ì¦ˆ ì‹œë“œ (í•™ìŠµ ê³¼ì • ë¹„êµìš©)\n","fixed_seed = tf.random.normal([16, NOISE_DIM])\n","\n","\n","def generate_and_save_images(epoch, generator, seed):\n","    \"\"\"\n","    Generatorë¡œ ì´ë¯¸ì§€ ìƒì„± ë° ì‹œê°í™”\n","\n","    Args:\n","        epoch: í˜„ì¬ ì—í­ ë²ˆí˜¸\n","        generator: Generator ëª¨ë¸\n","        seed: ê³ ì •ëœ ë…¸ì´ì¦ˆ ì‹œë“œ\n","    \"\"\"\n","    # Generatorë¡œ ì´ë¯¸ì§€ ìƒì„± (training=False)\n","    predictions = generator(seed, training=False)\n","\n","    # ì‹œê°í™”\n","    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n","\n","    for i, ax in enumerate(axes.flat):\n","        # [-1, 1] â†’ [0, 1]\n","        img = (predictions[i].numpy() + 1) / 2\n","        img = np.clip(img, 0, 1)  # ë²”ìœ„ ë³´ì •\n","        ax.imshow(img.squeeze(), cmap=\"gray\")\n","        ax.axis(\"off\")\n","\n","    plt.suptitle(f\"Epoch {epoch}\", fontsize=16)\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","# ì´ˆê¸° ì´ë¯¸ì§€ ìƒì„± (í•™ìŠµ ì „)\n","print(\"ğŸ¨ í•™ìŠµ ì „ Generator ì¶œë ¥\")\n","generate_and_save_images(0, generator, fixed_seed)"],"id":"6cf4386c"},{"cell_type":"markdown","metadata":{"id":"19dbcd7d"},"source":["## 3.7 ì „ì²´ í•™ìŠµ ì‹¤í–‰"],"id":"19dbcd7d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a10722c9"},"outputs":[],"source":["# í•™ìŠµ ì‹¤í–‰\n","print(\"=\" * 60)\n","print(\"ğŸ”¥ DCGAN í•™ìŠµ ì‹œì‘ (MNIST)\")\n","print(\"=\" * 60)\n","print(f\"ì´ ì—í­: {EPOCHS}\")\n","print(f\"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n","print(f\"ì´ë¯¸ì§€ í¬ê¸°: {IMG_SIZE}Ã—{IMG_SIZE}\")\n","print(\"=\" * 60)\n","\n","# Loss ê¸°ë¡\n","history = {\"d_loss\": [], \"g_loss\": []}\n","\n","for epoch in range(1, EPOCHS + 1):\n","    print(f\"\\nğŸ”¥ Epoch {epoch}/{EPOCHS}\")\n","\n","    epoch_d_loss = []\n","    epoch_g_loss = []\n","\n","    for batch_idx, real_batch in enumerate(dataset):\n","        d_loss, g_loss = train_step(real_batch)\n","\n","        epoch_d_loss.append(d_loss.numpy())\n","        epoch_g_loss.append(g_loss.numpy())\n","\n","        # ì§„í–‰ ìƒí™© ì¶œë ¥ (50 ë°°ì¹˜ë§ˆë‹¤)\n","        if (batch_idx + 1) % 50 == 0:\n","            print(\n","                f\"  Batch {batch_idx + 1} - D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}\"\n","            )\n","\n","    # ì—í­ í‰ê·  Loss ê¸°ë¡\n","    avg_d_loss = np.mean(epoch_d_loss)\n","    avg_g_loss = np.mean(epoch_g_loss)\n","    history[\"d_loss\"].append(avg_d_loss)\n","    history[\"g_loss\"].append(avg_g_loss)\n","\n","    print(f\"ğŸ“Š Epoch {epoch} í‰ê·  - D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}\")\n","\n","    # 5 ì—í­ë§ˆë‹¤ ì´ë¯¸ì§€ ìƒì„±\n","    if epoch % 5 == 0:\n","        generate_and_save_images(epoch, generator, fixed_seed)\n","\n","print(\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")"],"id":"a10722c9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"654becdd"},"outputs":[],"source":["# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history[\"d_loss\"], label=\"Discriminator Loss\", color=\"blue\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Discriminator Loss\")\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history[\"g_loss\"], label=\"Generator Loss\", color=\"red\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Generator Loss\")\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"],"id":"654becdd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a14cf5bd"},"outputs":[],"source":["# ìµœì¢… ìƒì„± ì´ë¯¸ì§€ (ë” ë§ì€ ìƒ˜í”Œ)\n","print(\"ğŸ¨ ìµœì¢… ìƒì„± ì´ë¯¸ì§€\")\n","\n","final_noise = tf.random.normal([25, NOISE_DIM])\n","final_images = generator(final_noise, training=False)\n","\n","plt.figure(figsize=(10, 10))\n","for i in range(25):\n","    plt.subplot(5, 5, i + 1)\n","    img = (final_images[i].numpy() + 1) / 2\n","    img = np.clip(img, 0, 1)\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","    plt.axis(\"off\")\n","plt.suptitle(\"DCGAN ìµœì¢… ìƒì„± ì´ë¯¸ì§€ (TensorFlow + MNIST)\", fontsize=16)\n","plt.tight_layout()\n","plt.show()"],"id":"a14cf5bd"},{"cell_type":"markdown","metadata":{"id":"7ae17489"},"source":["---\n","# ğŸ• 4êµì‹œ: DCGAN êµ¬í˜„ (PyTorch + MNIST)\n","\n","---"],"id":"7ae17489"},{"cell_type":"markdown","metadata":{"id":"b7e4fa89"},"source":["## 4.1 í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"],"id":"b7e4fa89"},{"cell_type":"code","execution_count":null,"metadata":{"id":"72b16460"},"outputs":[],"source":["# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","# GPU í™•ì¸\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n","print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n","\n","# ëœë¤ ì‹œë“œ ì„¤ì •\n","torch.manual_seed(42)\n","if device.type == \"cuda\":\n","    torch.cuda.manual_seed(42)\n","np.random.seed(42)"],"id":"72b16460"},{"cell_type":"markdown","metadata":{"id":"c7a4205a"},"source":["## 4.2 í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •"],"id":"c7a4205a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1deffdae"},"outputs":[],"source":["# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n","IMG_SIZE = 28  # ì´ë¯¸ì§€ í¬ê¸° (28Ã—28)\n","CHANNELS = 1  # Grayscale\n","BATCH_SIZE = 128  # ë°°ì¹˜ í¬ê¸°\n","NOISE_DIM = 100  # ë…¸ì´ì¦ˆ ë²¡í„° ì°¨ì›\n","EPOCHS = 50  # í•™ìŠµ ì—í­ ìˆ˜\n","LR = 0.0002  # í•™ìŠµë¥ \n","BETA1 = 0.5  # Adam optimizer beta1\n","\n","print(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\")\n","print(f\"   - ì´ë¯¸ì§€ í¬ê¸°: {IMG_SIZE}Ã—{IMG_SIZE}Ã—{CHANNELS}\")\n","print(f\"   - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n","print(f\"   - ë…¸ì´ì¦ˆ ì°¨ì›: {NOISE_DIM}\")\n","print(f\"   - í•™ìŠµ ì—í­: {EPOCHS}\")\n","print(f\"   - í•™ìŠµë¥ : {LR}\")\n","print(f\"   - Beta1: {BETA1}\")"],"id":"1deffdae"},{"cell_type":"markdown","metadata":{"id":"209297fd"},"source":["## 4.3 ë°ì´í„°ì…‹ ì¤€ë¹„\n","\n","PyTorchì—ì„œëŠ” `torchvision.transforms`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."],"id":"209297fd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9b36d8c3"},"outputs":[],"source":["# ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n","# MNISTëŠ” 28Ã—28 grayscale â†’ [-1, 1] ì •ê·œí™”\n","transform = transforms.Compose(\n","    [\n","        transforms.ToTensor(),  # [0, 1] ë²”ìœ„ë¡œ ë³€í™˜\n","        transforms.Normalize(\n","            (0.5,), (0.5,)\n","        ),  # [-1, 1] ì •ê·œí™” (grayscaleì´ë¯€ë¡œ ë‹¨ì¼ ê°’)\n","    ]\n",")\n","\n","# MNIST ë°ì´í„°ì…‹ ë¡œë“œ\n","dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n","\n","# DataLoader ìƒì„±\n","dataloader = DataLoader(\n","    dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2\n",")\n","\n","print(f\"\\nâœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n","print(f\"   - ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}\")\n","print(f\"   - ë°°ì¹˜ ìˆ˜: {len(dataloader)}\")"],"id":"9b36d8c3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"016d7086"},"outputs":[],"source":["# ë°ì´í„° ìƒ˜í”Œ ì‹œê°í™”\n","sample_batch, sample_labels = next(iter(dataloader))\n","print(f\"ë°°ì¹˜ shape: {sample_batch.shape}\")  # (128, 1, 28, 28)\n","print(f\"ê°’ ë²”ìœ„: [{sample_batch.min():.2f}, {sample_batch.max():.2f}]\")\n","\n","fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n","for i, ax in enumerate(axes.flat):\n","    # [-1, 1] â†’ [0, 1]ë¡œ ë³€í™˜í•˜ì—¬ ì‹œê°í™”\n","    # PyTorch: (C, H, W) â†’ (H, W, C)ë¡œ ë³€í™˜ í•„ìš”\n","    img = sample_batch[i].numpy()\n","    img = (img + 1) / 2  # [-1, 1] â†’ [0, 1]\n","    ax.imshow(img.squeeze(), cmap=\"gray\")\n","    ax.set_title(f\"Label: {sample_labels[i].item()}\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"MNIST ë°ì´í„° ìƒ˜í”Œ\", fontsize=14)\n","plt.tight_layout()\n","plt.show()"],"id":"016d7086"},{"cell_type":"markdown","metadata":{"lines_to_next_cell":2,"id":"293888f0"},"source":["## 4.4 ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” í•¨ìˆ˜\n","\n","DCGAN ë…¼ë¬¸ì—ì„œëŠ” **í‰ê·  0, í‘œì¤€í¸ì°¨ 0.02ì˜ ì •ê·œë¶„í¬**ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."],"id":"293888f0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"66ccd838"},"outputs":[],"source":["def weights_init(m):\n","    \"\"\"\n","    DCGAN í‘œì¤€ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n","\n","    - Conv, ConvTranspose, BatchNorm ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n","    - í‰ê·  0.0, í‘œì¤€í¸ì°¨ 0.02ì˜ ì •ê·œë¶„í¬ë¡œ ì´ˆê¸°í™”\n","\n","    Args:\n","        m: nn.Module (ë ˆì´ì–´)\n","    \"\"\"\n","    classname = m.__class__.__name__\n","\n","    if classname.find(\"Conv\") != -1:\n","        # Conv2d, ConvTranspose2dì˜ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","\n","    elif classname.find(\"BatchNorm\") != -1:\n","        # BatchNormì˜ ê°€ì¤‘ì¹˜ì™€ bias ì´ˆê¸°í™”\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)  # gamma\n","        nn.init.constant_(m.bias.data, 0)  # beta\n","\n","\n","print(\"âœ… ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"],"id":"66ccd838"},{"cell_type":"markdown","metadata":{"lines_to_next_cell":2,"id":"2503fb46"},"source":["## 4.5 Generator êµ¬í˜„\n","\n","PyTorchì—ì„œëŠ” `nn.ConvTranspose2d`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—…ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."],"id":"2503fb46"},{"cell_type":"code","execution_count":null,"metadata":{"id":"edf66b19"},"outputs":[],"source":["class Generator(nn.Module):\n","    \"\"\"\n","    DCGAN Generator (PyTorch + MNIST)\n","\n","    êµ¬ì¡°:\n","        z (100) â†’ ConvTranspose2d(256) â†’ 7Ã—7\n","               â†’ ConvTranspose2d(128) â†’ 14Ã—14\n","               â†’ ConvTranspose2d(1)   â†’ 28Ã—28\n","\n","    Args:\n","        noise_dim: ì…ë ¥ ë…¸ì´ì¦ˆ ë²¡í„°ì˜ ì°¨ì› (ê¸°ë³¸ê°’: 100)\n","    \"\"\"\n","\n","    def __init__(self, noise_dim=100):\n","        super(Generator, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            # ========================================\n","            # ConvTranspose 1: 1Ã—1 â†’ 7Ã—7\n","            # ========================================\n","            # ì…ë ¥: (N, 100, 1, 1)\n","            # ì¶œë ¥: (N, 256, 7, 7)\n","            nn.ConvTranspose2d(\n","                in_channels=noise_dim,  # ì…ë ¥ ì±„ë„: 100\n","                out_channels=256,  # ì¶œë ¥ ì±„ë„: 256\n","                kernel_size=7,  # ì»¤ë„ í¬ê¸°: 7Ã—7\n","                stride=1,  # ìŠ¤íŠ¸ë¼ì´ë“œ: 1\n","                padding=0,  # íŒ¨ë”©: 0\n","                bias=False,  # BatchNorm ì‚¬ìš© ì‹œ bias ë¶ˆí•„ìš”\n","            ),\n","            # ê³„ì‚°: (1-1)*1 - 2*0 + 7 = 7 â†’ 7Ã—7\n","            # ì˜ˆì‹œ: (128, 100, 1, 1) â†’ (128, 256, 7, 7)\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            # ========================================\n","            # ConvTranspose 2: 7Ã—7 â†’ 14Ã—14\n","            # ========================================\n","            # ì…ë ¥: (N, 256, 7, 7)\n","            # ì¶œë ¥: (N, 128, 14, 14)\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n","            # ê³„ì‚°: (7-1)*2 - 2*1 + 4 = 14 â†’ 14Ã—14\n","            # ì˜ˆì‹œ: (128, 256, 7, 7) â†’ (128, 128, 14, 14)\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            # ========================================\n","            # ConvTranspose 3 (ì¶œë ¥ì¸µ): 14Ã—14 â†’ 28Ã—28\n","            # ========================================\n","            # ì…ë ¥: (N, 128, 14, 14)\n","            # ì¶œë ¥: (N, 1, 28, 28)\n","            nn.ConvTranspose2d(128, 1, 4, 2, 1, bias=False),\n","            # ì˜ˆì‹œ: (128, 128, 14, 14) â†’ (128, 1, 28, 28)\n","            # ì£¼ì˜: ì¶œë ¥ì¸µì—ëŠ” BatchNorm ì—†ìŒ!\n","            nn.Tanh(),  # ì¶œë ¥ ë²”ìœ„: [-1, 1]\n","        )\n","\n","    def forward(self, z):\n","        \"\"\"\n","        ìˆœì „íŒŒ\n","\n","        Args:\n","            z: ë…¸ì´ì¦ˆ ë²¡í„° (batch_size, noise_dim, 1, 1)\n","\n","        Returns:\n","            ìƒì„±ëœ ì´ë¯¸ì§€ (batch_size, 1, 28, 28)\n","        \"\"\"\n","        return self.main(z)\n","\n","\n","# Generator ìƒì„±\n","generator = Generator(NOISE_DIM).to(device)\n","generator.apply(weights_init)  # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n","\n","print(\"\\nğŸ“Š Generator êµ¬ì¡°:\")\n","print(generator)"],"id":"edf66b19"},{"cell_type":"markdown","metadata":{"id":"788f4d34"},"source":["### Generator Shape ë³€í™” ì¶”ì "],"id":"788f4d34"},{"cell_type":"code","execution_count":null,"metadata":{"id":"84b3f235"},"outputs":[],"source":["print(\"\\nğŸ§ª Generator í…ŒìŠ¤íŠ¸\")\n","print(\"=\" * 50)\n","\n","# ëœë¤ ë…¸ì´ì¦ˆ ìƒì„±\n","# PyTorch: (batch_size, channels, height, width) í˜•ì‹\n","test_noise = torch.randn(4, NOISE_DIM, 1, 1).to(device)\n","print(f\"ì…ë ¥ ë…¸ì´ì¦ˆ shape: {test_noise.shape}\")  # (4, 100, 1, 1)\n","\n","# Generator í†µê³¼\n","test_output = generator(test_noise)\n","print(f\"ì¶œë ¥ ì´ë¯¸ì§€ shape: {test_output.shape}\")  # (4, 1, 28, 28)\n","print(f\"ì¶œë ¥ ê°’ ë²”ìœ„: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n","\n","# ìƒì„±ëœ ì´ë¯¸ì§€ ì‹œê°í™” (í•™ìŠµ ì „)\n","test_output_cpu = test_output.detach().cpu()\n","fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n","for i, ax in enumerate(axes):\n","    img = test_output_cpu[i].numpy()\n","    img = (img + 1) / 2  # [-1,1] â†’ [0,1]\n","    ax.imshow(img.squeeze(), cmap=\"gray\")\n","    ax.axis(\"off\")\n","plt.suptitle(\"Generator ì´ˆê¸° ì¶œë ¥ (í•™ìŠµ ì „ - ë…¸ì´ì¦ˆ)\", fontsize=14)\n","plt.tight_layout()\n","plt.show()"],"id":"84b3f235"},{"cell_type":"markdown","metadata":{"lines_to_next_cell":2,"id":"2040e5b8"},"source":["## 4.6 Discriminator êµ¬í˜„"],"id":"2040e5b8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dac8d749"},"outputs":[],"source":["class Discriminator(nn.Module):\n","    \"\"\"\n","    DCGAN Discriminator (PyTorch + MNIST)\n","\n","    êµ¬ì¡°:\n","        ì´ë¯¸ì§€ (28Ã—28Ã—1) â†’ Conv2d(64)  â†’ 14Ã—14\n","                        â†’ Conv2d(128) â†’ 7Ã—7\n","                        â†’ Conv2d(1)   â†’ 1Ã—1\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            # ========================================\n","            # Conv 1 (ì…ë ¥ì¸µ): 28Ã—28 â†’ 14Ã—14\n","            # ========================================\n","            # ì…ë ¥: (N, 1, 28, 28)\n","            # ì¶œë ¥: (N, 64, 14, 14)\n","            nn.Conv2d(\n","                in_channels=1,  # ì…ë ¥ ì±„ë„: Grayscale 1\n","                out_channels=64,  # ì¶œë ¥ ì±„ë„: 64\n","                kernel_size=4,  # ì»¤ë„ í¬ê¸°: 4Ã—4\n","                stride=2,  # ìŠ¤íŠ¸ë¼ì´ë“œ: 2 (ë‹¤ìš´ìƒ˜í”Œë§)\n","                padding=1,  # íŒ¨ë”©: 1\n","                bias=False,\n","            ),\n","            # ê³„ì‚°: (28 - 4 + 2*1) / 2 + 1 = 14 â†’ 14Ã—14\n","            # ì˜ˆì‹œ: (128, 1, 28, 28) â†’ (128, 64, 14, 14)\n","            # ì£¼ì˜: ì…ë ¥ì¸µì—ëŠ” BatchNorm ì—†ìŒ!\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # ========================================\n","            # Conv 2: 14Ã—14 â†’ 7Ã—7\n","            # ========================================\n","            # ì…ë ¥: (N, 64, 14, 14)\n","            # ì¶œë ¥: (N, 128, 7, 7)\n","            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n","            # ì˜ˆì‹œ: (128, 64, 14, 14) â†’ (128, 128, 7, 7)\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            # ========================================\n","            # Conv 3 (ì¶œë ¥ì¸µ): 7Ã—7 â†’ 1Ã—1\n","            # ========================================\n","            # ì…ë ¥: (N, 128, 7, 7)\n","            # ì¶œë ¥: (N, 1, 1, 1)\n","            nn.Conv2d(128, 1, 7, 1, 0, bias=False),\n","            # ê³„ì‚°: (7 - 7 + 2*0) / 1 + 1 = 1 â†’ 1Ã—1\n","            # ì˜ˆì‹œ: (128, 128, 7, 7) â†’ (128, 1, 1, 1)\n","            # SigmoidëŠ” BCEWithLogitsLoss ì‚¬ìš© ì‹œ ë¶ˆí•„ìš”\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        ìˆœì „íŒŒ\n","\n","        Args:\n","            x: ì…ë ¥ ì´ë¯¸ì§€ (batch_size, 1, 28, 28)\n","\n","        Returns:\n","            íŒë³„ ê²°ê³¼ (batch_size, 1, 1, 1) â†’ viewë¡œ (batch_size,)ë¡œ ë³€í™˜\n","        \"\"\"\n","        output = self.main(x)\n","        return output.view(-1)  # (N, 1, 1, 1) â†’ (N,)\n","\n","\n","# Discriminator ìƒì„±\n","discriminator = Discriminator().to(device)\n","discriminator.apply(weights_init)  # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n","\n","print(\"\\nğŸ“Š Discriminator êµ¬ì¡°:\")\n","print(discriminator)"],"id":"dac8d749"},{"cell_type":"code","execution_count":null,"metadata":{"id":"27231f75"},"outputs":[],"source":["# Discriminator í…ŒìŠ¤íŠ¸\n","print(\"\\nğŸ§ª Discriminator í…ŒìŠ¤íŠ¸\")\n","print(\"=\" * 50)\n","\n","# ì§„ì§œ ì´ë¯¸ì§€ (ë°°ì¹˜ì—ì„œ ìƒ˜í”Œ)\n","real_batch, _ = next(iter(dataloader))\n","real_batch = real_batch[:4].to(device)\n","print(f\"ì§„ì§œ ì´ë¯¸ì§€ shape: {real_batch.shape}\")  # (4, 1, 28, 28)\n","\n","# ê°€ì§œ ì´ë¯¸ì§€ (Generator ì¶œë ¥)\n","fake_batch = generator(torch.randn(4, NOISE_DIM, 1, 1).to(device))\n","print(f\"ê°€ì§œ ì´ë¯¸ì§€ shape: {fake_batch.shape}\")  # (4, 1, 28, 28)\n","\n","# Discriminator í†µê³¼\n","real_output = discriminator(real_batch)\n","fake_output = discriminator(fake_batch.detach())\n","\n","print(f\"\\nì§„ì§œ ì´ë¯¸ì§€ íŒë³„ ê²°ê³¼: {real_output.detach().cpu().numpy()}\")\n","print(f\"ê°€ì§œ ì´ë¯¸ì§€ íŒë³„ ê²°ê³¼: {fake_output.detach().cpu().numpy()}\")\n","print(\"\\n(í•™ìŠµ ì „ì´ë¼ ëœë¤í•œ ê°’)\")"],"id":"27231f75"},{"cell_type":"markdown","metadata":{"id":"230ce5dd"},"source":["## 4.7 ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •"],"id":"230ce5dd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ea23692"},"outputs":[],"source":["# BCEWithLogitsLoss (Sigmoid + BCELoss í†µí•©)\n","# Discriminator ì¶œë ¥ì— Sigmoidê°€ ì—†ìœ¼ë¯€ë¡œ ì´ loss ì‚¬ìš©\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# DCGAN í‘œì¤€ Optimizer ì„¤ì •\n","optimizer_g = optim.Adam(\n","    generator.parameters(),\n","    lr=LR,\n","    betas=(BETA1, 0.999),  # (beta1, beta2)\n",")\n","\n","optimizer_d = optim.Adam(discriminator.parameters(), lr=LR, betas=(BETA1, 0.999))\n","\n","print(\"âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ\")\n","print(f\"   - Loss: BCEWithLogitsLoss\")\n","print(f\"   - Optimizer: Adam (lr={LR}, Î²1={BETA1})\")"],"id":"3ea23692"},{"cell_type":"markdown","metadata":{"lines_to_next_cell":2,"id":"67aa8eed"},"source":["## 4.8 í•™ìŠµ Step í•¨ìˆ˜ êµ¬í˜„"],"id":"67aa8eed"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5f8c7507"},"outputs":[],"source":["def train_step(real_images):\n","    \"\"\"\n","    DCGAN í•œ ìŠ¤í… í•™ìŠµ (PyTorch)\n","\n","    Args:\n","        real_images: ì§„ì§œ ì´ë¯¸ì§€ ë°°ì¹˜ (batch_size, 1, 28, 28)\n","\n","    Returns:\n","        d_loss: Discriminator ì†ì‹¤\n","        g_loss: Generator ì†ì‹¤\n","    \"\"\"\n","    # í˜„ì¬ ë°°ì¹˜ í¬ê¸°\n","    batch_size = real_images.size(0)\n","\n","    # ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n","    real_images = real_images.to(device)\n","\n","    # ë¼ë²¨ ìƒì„±\n","    # BCEWithLogitsLossëŠ” logitì„ ì…ë ¥ë°›ìœ¼ë¯€ë¡œ, ë¼ë²¨ì€ 0 ë˜ëŠ” 1\n","    real_labels = torch.ones(batch_size, device=device)  # ì§„ì§œ: 1\n","    fake_labels = torch.zeros(batch_size, device=device)  # ê°€ì§œ: 0\n","\n","    # ========================================\n","    # 1. Discriminator í•™ìŠµ\n","    # ========================================\n","    discriminator.zero_grad()\n","\n","    # (1) ì§„ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ ì†ì‹¤\n","    # real_output: (batch_size,)\n","    real_output = discriminator(real_images)\n","    d_loss_real = criterion(real_output, real_labels)\n","\n","    # (2) ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±\n","    # noise: (batch_size, 100, 1, 1)\n","    noise = torch.randn(batch_size, NOISE_DIM, 1, 1, device=device)\n","    fake_images = generator(noise)\n","\n","    # ê°€ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ ì†ì‹¤\n","    # detach()ë¡œ Generatorì˜ gradient ì „íŒŒ ë°©ì§€\n","    fake_output = discriminator(fake_images.detach())\n","    d_loss_fake = criterion(fake_output, fake_labels)\n","\n","    # (3) Discriminator ì´ ì†ì‹¤\n","    d_loss = d_loss_real + d_loss_fake\n","\n","    # (4) ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸\n","    d_loss.backward()\n","    optimizer_d.step()\n","\n","    # ========================================\n","    # 2. Generator í•™ìŠµ\n","    # ========================================\n","    generator.zero_grad()\n","\n","    # ìƒˆë¡œìš´ ë…¸ì´ì¦ˆë¡œ ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±\n","    noise = torch.randn(batch_size, NOISE_DIM, 1, 1, device=device)\n","    fake_images = generator(noise)\n","\n","    # Discriminatorë¡œ íŒë³„\n","    # ì´ë²ˆì—ëŠ” detach() ì—†ì´ â†’ Generatorê¹Œì§€ gradient ì „íŒŒ\n","    fake_output = discriminator(fake_images)\n","\n","    # GeneratorëŠ” ê°€ì§œë¥¼ ì§„ì§œ(1)ë¡œ ì¸ì‹ì‹œí‚¤ê³  ì‹¶ìŒ\n","    g_loss = criterion(fake_output, real_labels)\n","\n","    # ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸\n","    g_loss.backward()\n","    optimizer_g.step()\n","\n","    return d_loss.item(), g_loss.item()\n","\n","\n","print(\"âœ… train_step í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"],"id":"5f8c7507"},{"cell_type":"markdown","metadata":{"id":"c1849fcb"},"source":["## 4.9 ì´ë¯¸ì§€ ìƒì„± ë° ì‹œê°í™” í•¨ìˆ˜"],"id":"c1849fcb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dac6b3b9"},"outputs":[],"source":["# ê³ ì • ë…¸ì´ì¦ˆ ì‹œë“œ (í•™ìŠµ ê³¼ì • ë¹„êµìš©)\n","fixed_noise = torch.randn(16, NOISE_DIM, 1, 1, device=device)\n","\n","\n","def generate_and_save_images(epoch, generator, noise):\n","    \"\"\"\n","    Generatorë¡œ ì´ë¯¸ì§€ ìƒì„± ë° ì‹œê°í™” (PyTorch)\n","\n","    Args:\n","        epoch: í˜„ì¬ ì—í­ ë²ˆí˜¸\n","        generator: Generator ëª¨ë¸\n","        noise: ê³ ì •ëœ ë…¸ì´ì¦ˆ ì‹œë“œ (16, 100, 1, 1)\n","    \"\"\"\n","    generator.eval()  # í‰ê°€ ëª¨ë“œ\n","\n","    with torch.no_grad():\n","        # Generatorë¡œ ì´ë¯¸ì§€ ìƒì„±\n","        fake_images = generator(noise).detach().cpu()\n","\n","    # ì‹œê°í™”\n","    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n","\n","    for i, ax in enumerate(axes.flat):\n","        # [-1, 1] â†’ [0, 1]\n","        img = fake_images[i].numpy()\n","        img = (img + 1) / 2\n","        img = np.clip(img, 0, 1)  # ë²”ìœ„ ë³´ì •\n","        ax.imshow(img.squeeze(), cmap=\"gray\")\n","        ax.axis(\"off\")\n","\n","    plt.suptitle(f\"Epoch {epoch}\", fontsize=16)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    generator.train()  # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œ\n","\n","\n","# ì´ˆê¸° ì´ë¯¸ì§€ ìƒì„± (í•™ìŠµ ì „)\n","print(\"ğŸ¨ í•™ìŠµ ì „ Generator ì¶œë ¥\")\n","generate_and_save_images(0, generator, fixed_noise)"],"id":"dac6b3b9"},{"cell_type":"markdown","metadata":{"id":"cf26a60b"},"source":["## 4.10 ì „ì²´ í•™ìŠµ ì‹¤í–‰"],"id":"cf26a60b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8920cb2d"},"outputs":[],"source":["# í•™ìŠµ ì‹¤í–‰\n","print(\"=\" * 60)\n","print(\"ğŸ”¥ DCGAN í•™ìŠµ ì‹œì‘ (PyTorch + MNIST)\")\n","print(\"=\" * 60)\n","print(f\"ì´ ì—í­: {EPOCHS}\")\n","print(f\"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n","print(f\"ì´ë¯¸ì§€ í¬ê¸°: {IMG_SIZE}Ã—{IMG_SIZE}\")\n","print(\"=\" * 60)\n","\n","# Loss ê¸°ë¡\n","history = {\"d_loss\": [], \"g_loss\": []}\n","\n","for epoch in range(1, EPOCHS + 1):\n","    print(f\"\\nğŸ”¥ Epoch {epoch}/{EPOCHS}\")\n","\n","    epoch_d_loss = []\n","    epoch_g_loss = []\n","\n","    for batch_idx, (real_batch, _) in enumerate(dataloader):\n","        d_loss, g_loss = train_step(real_batch)\n","\n","        epoch_d_loss.append(d_loss)\n","        epoch_g_loss.append(g_loss)\n","\n","        # ì§„í–‰ ìƒí™© ì¶œë ¥ (50 ë°°ì¹˜ë§ˆë‹¤)\n","        if (batch_idx + 1) % 50 == 0:\n","            print(\n","                f\"  Batch {batch_idx + 1} - D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}\"\n","            )\n","\n","    # ì—í­ í‰ê·  Loss ê¸°ë¡\n","    avg_d_loss = np.mean(epoch_d_loss)\n","    avg_g_loss = np.mean(epoch_g_loss)\n","    history[\"d_loss\"].append(avg_d_loss)\n","    history[\"g_loss\"].append(avg_g_loss)\n","\n","    print(f\"ğŸ“Š Epoch {epoch} í‰ê·  - D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}\")\n","\n","    # 5 ì—í­ë§ˆë‹¤ ì´ë¯¸ì§€ ìƒì„±\n","    if epoch % 5 == 0:\n","        generate_and_save_images(epoch, generator, fixed_noise)\n","\n","print(\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")"],"id":"8920cb2d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"812bd430"},"outputs":[],"source":["# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history[\"d_loss\"], label=\"Discriminator Loss\", color=\"blue\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Discriminator Loss\")\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history[\"g_loss\"], label=\"Generator Loss\", color=\"red\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Generator Loss\")\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"],"id":"812bd430"},{"cell_type":"code","execution_count":null,"metadata":{"id":"808f8f33"},"outputs":[],"source":["# ìµœì¢… ìƒì„± ì´ë¯¸ì§€ (ë” ë§ì€ ìƒ˜í”Œ)\n","print(\"ğŸ¨ ìµœì¢… ìƒì„± ì´ë¯¸ì§€\")\n","\n","generator.eval()\n","with torch.no_grad():\n","    final_noise = torch.randn(25, NOISE_DIM, 1, 1, device=device)\n","    final_images = generator(final_noise).detach().cpu()\n","\n","plt.figure(figsize=(10, 10))\n","for i in range(25):\n","    plt.subplot(5, 5, i + 1)\n","    img = final_images[i].numpy()\n","    img = (img + 1) / 2\n","    img = np.clip(img, 0, 1)\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","    plt.axis(\"off\")\n","plt.suptitle(\"DCGAN ìµœì¢… ìƒì„± ì´ë¯¸ì§€ (PyTorch + MNIST)\", fontsize=16)\n","plt.tight_layout()\n","plt.show()"],"id":"808f8f33"},{"cell_type":"code","execution_count":null,"id":"a2cd0575","metadata":{"id":"a2cd0575"},"outputs":[],"source":["# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import urllib.request\n","import zipfile\n","from PIL import Image\n","import glob\n","\n","# GPU í™•ì¸\n","print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n","print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {tf.config.list_physical_devices('GPU')}\")\n","\n","# ëœë¤ ì‹œë“œ ì„¤ì •\n","tf.random.set_seed(42)\n","np.random.seed(42)"]},{"cell_type":"markdown","id":"5a85ca4f","metadata":{"id":"5a85ca4f"},"source":["### CelebA ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬\n","\n","**CelebA (CelebFaces Attributes Dataset):**\n","- 202,599ì¥ì˜ ìœ ëª…ì¸ ì–¼êµ´ ì´ë¯¸ì§€\n","- ë‹¤ì–‘í•œ í¬ì¦ˆ, í‘œì •, ë°°ê²½\n","- GAN í•™ìŠµì— ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹\n","\n","> âš ï¸ **Colabì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì‹œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**\n","> ì‚¬ì „ì— ë‹¤ìš´ë¡œë“œëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ìƒ˜í”Œ ë°ì´í„°ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"id":"76908a7a","metadata":{"id":"76908a7a"},"outputs":[],"source":["# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n","IMG_SIZE = 64  # ì´ë¯¸ì§€ í¬ê¸° (64Ã—64)\n","CHANNELS = 3  # RGB ì»¬ëŸ¬\n","BATCH_SIZE = 128  # ë°°ì¹˜ í¬ê¸° (L4 GPUì—ì„œ ì•ˆì •ì )\n","NOISE_DIM = 100  # ë…¸ì´ì¦ˆ ë²¡í„° ì°¨ì›\n","EPOCHS = 50  # í•™ìŠµ ì—í­ ìˆ˜\n","\n","print(\"âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ\")\n","print(f\"   - ì´ë¯¸ì§€ í¬ê¸°: {IMG_SIZE}Ã—{IMG_SIZE}Ã—{CHANNELS}\")\n","print(f\"   - ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n","print(f\"   - ë…¸ì´ì¦ˆ ì°¨ì›: {NOISE_DIM}\")\n","print(f\"   - í•™ìŠµ ì—í­: {EPOCHS}\")"]},{"cell_type":"code","execution_count":null,"id":"f9457a07","metadata":{"lines_to_next_cell":2,"id":"f9457a07"},"outputs":[],"source":["# CelebA ë°ì´í„°ì…‹ ë¡œë“œ í•¨ìˆ˜\n","def load_celeba_dataset(data_dir, img_size=64, batch_size=128):\n","    \"\"\"\n","    CelebA ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n","\n","    Args:\n","        data_dir: ì´ë¯¸ì§€ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n","        img_size: ë¦¬ì‚¬ì´ì¦ˆí•  ì´ë¯¸ì§€ í¬ê¸°\n","        batch_size: ë°°ì¹˜ í¬ê¸°\n","\n","    Returns:\n","        tf.data.Dataset ê°ì²´\n","    \"\"\"\n","\n","    def preprocess_image(file_path):\n","        \"\"\"ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n","        # ì´ë¯¸ì§€ ì½ê¸°\n","        image = tf.io.read_file(file_path)\n","        image = tf.image.decode_jpeg(image, channels=3)\n","\n","        # CelebA ì´ë¯¸ì§€ëŠ” 178Ã—218 í¬ê¸°\n","        # ì–¼êµ´ ì¤‘ì‹¬ ë¶€ë¶„ì„ crop (140Ã—140)\n","        image = tf.image.crop_to_bounding_box(image, 40, 20, 140, 140)\n","\n","        # 64Ã—64ë¡œ ë¦¬ì‚¬ì´ì¦ˆ\n","        image = tf.image.resize(image, [img_size, img_size])\n","\n","        # [0, 255] â†’ [-1, 1] ì •ê·œí™” (Tanh ì¶œë ¥ê³¼ ì¼ì¹˜)\n","        image = (image - 127.5) / 127.5\n","\n","        return image\n","\n","    # ì´ë¯¸ì§€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°\n","    image_paths = tf.data.Dataset.list_files(data_dir + \"/*.jpg\", shuffle=True)\n","\n","    # ì „ì²˜ë¦¬ ë° ë°°ì¹˜ ì²˜ë¦¬\n","    dataset = image_paths.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n","    dataset = dataset.batch(batch_size, drop_remainder=True)\n","    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"id":"ef6cccaf","metadata":{"id":"ef6cccaf"},"outputs":[],"source":["# ë°ëª¨ìš©: CelebA ëŒ€ì‹  ëœë¤ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ìƒì„±\n","# (ì‹¤ì œ í•™ìŠµ ì‹œì—ëŠ” CelebA ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì„¸ìš”)\n","\n","\n","def create_demo_dataset(num_samples=10000, img_size=64, batch_size=128):\n","    \"\"\"\n","    ë°ëª¨ìš© ëœë¤ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ìƒì„±\n","    ì‹¤ì œ í•™ìŠµì—ì„œëŠ” CelebAë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤!\n","    \"\"\"\n","    # ëœë¤ ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œë¡œëŠ” ì˜ë¯¸ì—†ëŠ” ë…¸ì´ì¦ˆ)\n","    # ì‹¤ì œ CelebA ë°ì´í„°ë¡œ ëŒ€ì²´ í•„ìš”\n","    images = np.random.uniform(-1, 1, (num_samples, img_size, img_size, 3)).astype(\n","        np.float32\n","    )\n","\n","    dataset = tf.data.Dataset.from_tensor_slices(images)\n","    dataset = dataset.shuffle(num_samples).batch(batch_size, drop_remainder=True)\n","    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n","\n","    return dataset\n","\n","\n","# ë°ì´í„°ì…‹ ìƒì„±\n","# ì‹¤ì œ ì‚¬ìš© ì‹œ: dataset = load_celeba_dataset('/path/to/celeba/img_align_celeba')\n","dataset = create_demo_dataset(\n","    num_samples=10000, img_size=IMG_SIZE, batch_size=BATCH_SIZE\n",")\n","\n","print(f\"\\nâœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")\n","print(f\"   - ë°°ì¹˜ ìˆ˜: {len(list(dataset))}\")"]},{"cell_type":"code","execution_count":null,"id":"9ab912c4","metadata":{"id":"9ab912c4"},"outputs":[],"source":["# ë°ì´í„° ìƒ˜í”Œ ì‹œê°í™”\n","sample_batch = next(iter(dataset))\n","print(f\"ë°°ì¹˜ shape: {sample_batch.shape}\")  # (128, 64, 64, 3)\n","print(f\"ê°’ ë²”ìœ„: [{sample_batch.numpy().min():.2f}, {sample_batch.numpy().max():.2f}]\")\n","\n","fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n","for i, ax in enumerate(axes.flat):\n","    # [-1, 1] â†’ [0, 1]ë¡œ ë³€í™˜í•˜ì—¬ ì‹œê°í™”\n","    img = (sample_batch[i].numpy() + 1) / 2\n","    ax.imshow(img)\n","    ax.axis(\"off\")\n","plt.suptitle(\"ë°ì´í„° ìƒ˜í”Œ (ë°ëª¨ìš© ëœë¤ ì´ë¯¸ì§€)\", fontsize=14)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"ae052c7c","metadata":{"lines_to_next_cell":2,"id":"ae052c7c"},"source":["## 3.2 Generator êµ¬í˜„\n","\n","DCGAN GeneratorëŠ” 100ì°¨ì› ë…¸ì´ì¦ˆë¥¼ 64Ã—64Ã—3 ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"id":"5dd8908a","metadata":{"id":"5dd8908a"},"outputs":[],"source":["def build_generator(noise_dim=100):\n","    \"\"\"\n","    DCGAN Generator ìƒì„±\n","\n","    êµ¬ì¡°:\n","        z (100) â†’ Dense(4Ã—4Ã—512) â†’ Reshape\n","               â†’ ConvTranspose(256) â†’ 8Ã—8\n","               â†’ ConvTranspose(128) â†’ 16Ã—16\n","               â†’ ConvTranspose(64)  â†’ 32Ã—32\n","               â†’ ConvTranspose(3)   â†’ 64Ã—64\n","\n","    Args:\n","        noise_dim: ì…ë ¥ ë…¸ì´ì¦ˆ ë²¡í„°ì˜ ì°¨ì› (ê¸°ë³¸ê°’: 100)\n","\n","    Returns:\n","        tf.keras.Model: Generator ëª¨ë¸\n","    \"\"\"\n","\n","    model = tf.keras.Sequential(name=\"Generator\")\n","\n","    # ========================================\n","    # ì²« ë²ˆì§¸ ì¸µ: Dense + Reshape\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 100)\n","    # ì¶œë ¥: (batch_size, 4, 4, 512)\n","\n","    # Dense: 100 â†’ 4Ã—4Ã—512 = 8192\n","    model.add(\n","        layers.Dense(\n","            4 * 4 * 512,  # ì¶œë ¥ ìœ ë‹› ìˆ˜: 8192\n","            use_bias=False,  # BatchNorm ì‚¬ìš© ì‹œ bias ë¶ˆí•„ìš”\n","            input_shape=(noise_dim,),  # ì…ë ¥ shape: (100,)\n","        )\n","    )\n","    # ì˜ˆì‹œ: (batch=128, 100) â†’ (128, 8192)\n","\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.ReLU())\n","\n","    # Reshape: 1D â†’ 4D\n","    model.add(layers.Reshape((4, 4, 512)))\n","    # ì˜ˆì‹œ: (128, 8192) â†’ (128, 4, 4, 512)\n","\n","    # ========================================\n","    # ConvTranspose 1: 4Ã—4 â†’ 8Ã—8\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 4, 4, 512)\n","    # ì¶œë ¥: (batch_size, 8, 8, 256)\n","\n","    model.add(\n","        layers.Conv2DTranspose(\n","            filters=256,  # ì¶œë ¥ ì±„ë„ ìˆ˜\n","            kernel_size=4,  # ì»¤ë„ í¬ê¸° (4Ã—4)\n","            strides=2,  # ìŠ¤íŠ¸ë¼ì´ë“œ 2 â†’ í¬ê¸° 2ë°°\n","            padding=\"same\",  # ì¶œë ¥ í¬ê¸° = ì…ë ¥ Ã— stride\n","            use_bias=False,  # BatchNorm ì‚¬ìš© ì‹œ bias ë¶ˆí•„ìš”\n","        )\n","    )\n","    # ê³„ì‚°: 4 Ã— 2 = 8 (padding='same'ì´ë¯€ë¡œ)\n","    # ì˜ˆì‹œ: (128, 4, 4, 512) â†’ (128, 8, 8, 256)\n","\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.ReLU())\n","\n","    # ========================================\n","    # ConvTranspose 2: 8Ã—8 â†’ 16Ã—16\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 8, 8, 256)\n","    # ì¶œë ¥: (batch_size, 16, 16, 128)\n","\n","    model.add(\n","        layers.Conv2DTranspose(\n","            filters=128, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n","        )\n","    )\n","    # ì˜ˆì‹œ: (128, 8, 8, 256) â†’ (128, 16, 16, 128)\n","\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.ReLU())\n","\n","    # ========================================\n","    # ConvTranspose 3: 16Ã—16 â†’ 32Ã—32\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 16, 16, 128)\n","    # ì¶œë ¥: (batch_size, 32, 32, 64)\n","\n","    model.add(\n","        layers.Conv2DTranspose(\n","            filters=64, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n","        )\n","    )\n","    # ì˜ˆì‹œ: (128, 16, 16, 128) â†’ (128, 32, 32, 64)\n","\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.ReLU())\n","\n","    # ========================================\n","    # ConvTranspose 4 (ì¶œë ¥ì¸µ): 32Ã—32 â†’ 64Ã—64\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 32, 32, 64)\n","    # ì¶œë ¥: (batch_size, 64, 64, 3)\n","\n","    model.add(\n","        layers.Conv2DTranspose(\n","            filters=3,  # RGB 3ì±„ë„\n","            kernel_size=4,\n","            strides=2,\n","            padding=\"same\",\n","            use_bias=False,\n","            activation=\"tanh\",  # ì¶œë ¥ ë²”ìœ„: [-1, 1]\n","        )\n","    )\n","    # ì˜ˆì‹œ: (128, 32, 32, 64) â†’ (128, 64, 64, 3)\n","    # ì£¼ì˜: ì¶œë ¥ì¸µì—ëŠ” BatchNorm ì—†ìŒ! (Tanh ë²”ìœ„ ìœ ì§€)\n","\n","    return model\n","\n","\n","# Generator ìƒì„±\n","generator = build_generator(NOISE_DIM)\n","generator.summary()"]},{"cell_type":"code","execution_count":null,"id":"df76b96d","metadata":{"id":"df76b96d"},"outputs":[],"source":["# Generator í…ŒìŠ¤íŠ¸\n","print(\"\\nğŸ§ª Generator í…ŒìŠ¤íŠ¸\")\n","print(\"=\" * 50)\n","\n","# ëœë¤ ë…¸ì´ì¦ˆ ìƒì„±\n","test_noise = tf.random.normal([4, NOISE_DIM])\n","print(f\"ì…ë ¥ ë…¸ì´ì¦ˆ shape: {test_noise.shape}\")  # (4, 100)\n","\n","# Generator í†µê³¼\n","test_output = generator(test_noise, training=False)\n","print(f\"ì¶œë ¥ ì´ë¯¸ì§€ shape: {test_output.shape}\")  # (4, 64, 64, 3)\n","print(\n","    f\"ì¶œë ¥ ê°’ ë²”ìœ„: [{test_output.numpy().min():.3f}, {test_output.numpy().max():.3f}]\"\n",")\n","\n","# ìƒì„±ëœ ì´ë¯¸ì§€ ì‹œê°í™” (í•™ìŠµ ì „)\n","fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n","for i, ax in enumerate(axes):\n","    img = (test_output[i].numpy() + 1) / 2  # [-1,1] â†’ [0,1]\n","    ax.imshow(img)\n","    ax.axis(\"off\")\n","plt.suptitle(\"Generator ì´ˆê¸° ì¶œë ¥ (í•™ìŠµ ì „ - ë…¸ì´ì¦ˆ)\", fontsize=14)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"7fc5af93","metadata":{"lines_to_next_cell":2,"id":"7fc5af93"},"source":["## 3.3 Discriminator êµ¬í˜„\n","\n","DCGAN DiscriminatorëŠ” 64Ã—64Ã—3 ì´ë¯¸ì§€ë¥¼ ë°›ì•„ ì§„ì§œ/ê°€ì§œë¥¼ íŒë³„í•©ë‹ˆë‹¤."]},{"cell_type":"code","execution_count":null,"id":"0cd18843","metadata":{"id":"0cd18843"},"outputs":[],"source":["def build_discriminator():\n","    \"\"\"\n","    DCGAN Discriminator ìƒì„±\n","\n","    êµ¬ì¡°:\n","        ì´ë¯¸ì§€ (64Ã—64Ã—3) â†’ Conv(64)  â†’ 32Ã—32\n","                        â†’ Conv(128) â†’ 16Ã—16\n","                        â†’ Conv(256) â†’ 8Ã—8\n","                        â†’ Conv(512) â†’ 4Ã—4\n","                        â†’ Flatten â†’ Dense(1)\n","\n","    Returns:\n","        tf.keras.Model: Discriminator ëª¨ë¸\n","    \"\"\"\n","\n","    model = tf.keras.Sequential(name=\"Discriminator\")\n","\n","    # ========================================\n","    # Conv 1 (ì…ë ¥ì¸µ): 64Ã—64 â†’ 32Ã—32\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 64, 64, 3)\n","    # ì¶œë ¥: (batch_size, 32, 32, 64)\n","\n","    model.add(\n","        layers.Conv2D(\n","            filters=64,  # ì¶œë ¥ ì±„ë„ ìˆ˜\n","            kernel_size=4,  # ì»¤ë„ í¬ê¸° (4Ã—4)\n","            strides=2,  # ìŠ¤íŠ¸ë¼ì´ë“œ 2 â†’ í¬ê¸° 1/2\n","            padding=\"same\",  # ì¶œë ¥ í¬ê¸° = ì…ë ¥ / stride\n","            input_shape=(64, 64, 3),  # ì…ë ¥ shape\n","            use_bias=False,\n","        )\n","    )\n","    # ê³„ì‚°: 64 / 2 = 32 (padding='same'ì´ë¯€ë¡œ)\n","    # ì˜ˆì‹œ: (128, 64, 64, 3) â†’ (128, 32, 32, 64)\n","    # ì£¼ì˜: ì…ë ¥ì¸µì—ëŠ” BatchNorm ì—†ìŒ! (ì›ë³¸ ë¶„í¬ ìœ ì§€)\n","\n","    model.add(layers.LeakyReLU(0.2))  # ìŒìˆ˜ ì˜ì—­ì—ì„œë„ gradient ìœ ì§€\n","\n","    # ========================================\n","    # Conv 2: 32Ã—32 â†’ 16Ã—16\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 32, 32, 64)\n","    # ì¶œë ¥: (batch_size, 16, 16, 128)\n","\n","    model.add(\n","        layers.Conv2D(\n","            filters=128, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n","        )\n","    )\n","    # ì˜ˆì‹œ: (128, 32, 32, 64) â†’ (128, 16, 16, 128)\n","\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(0.2))\n","\n","    # ========================================\n","    # Conv 3: 16Ã—16 â†’ 8Ã—8\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 16, 16, 128)\n","    # ì¶œë ¥: (batch_size, 8, 8, 256)\n","\n","    model.add(\n","        layers.Conv2D(\n","            filters=256, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n","        )\n","    )\n","    # ì˜ˆì‹œ: (128, 16, 16, 128) â†’ (128, 8, 8, 256)\n","\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(0.2))\n","\n","    # ========================================\n","    # Conv 4: 8Ã—8 â†’ 4Ã—4\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 8, 8, 256)\n","    # ì¶œë ¥: (batch_size, 4, 4, 512)\n","\n","    model.add(\n","        layers.Conv2D(\n","            filters=512, kernel_size=4, strides=2, padding=\"same\", use_bias=False\n","        )\n","    )\n","    # ì˜ˆì‹œ: (128, 8, 8, 256) â†’ (128, 4, 4, 512)\n","\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU(0.2))\n","\n","    # ========================================\n","    # ì¶œë ¥ì¸µ: Flatten + Dense(1)\n","    # ========================================\n","    # ì…ë ¥: (batch_size, 4, 4, 512)\n","    # ì¶œë ¥: (batch_size, 1)\n","\n","    model.add(layers.Flatten())\n","    # ì˜ˆì‹œ: (128, 4, 4, 512) â†’ (128, 8192)\n","\n","    model.add(layers.Dense(1))  # Sigmoid ì—†ìŒ â†’ BCEWithLogitsLoss ì‚¬ìš©\n","    # ì˜ˆì‹œ: (128, 8192) â†’ (128, 1)\n","\n","    return model\n","\n","\n","# Discriminator ìƒì„±\n","discriminator = build_discriminator()\n","discriminator.summary()"]},{"cell_type":"code","execution_count":null,"id":"9a9fd619","metadata":{"id":"9a9fd619"},"outputs":[],"source":["# Discriminator í…ŒìŠ¤íŠ¸\n","print(\"\\nğŸ§ª Discriminator í…ŒìŠ¤íŠ¸\")\n","print(\"=\" * 50)\n","\n","# ì§„ì§œ ì´ë¯¸ì§€ (ë°°ì¹˜ì—ì„œ ìƒ˜í”Œ)\n","real_batch = next(iter(dataset))[:4]\n","print(f\"ì§„ì§œ ì´ë¯¸ì§€ shape: {real_batch.shape}\")  # (4, 64, 64, 3)\n","\n","# ê°€ì§œ ì´ë¯¸ì§€ (Generator ì¶œë ¥)\n","fake_batch = generator(tf.random.normal([4, NOISE_DIM]), training=False)\n","print(f\"ê°€ì§œ ì´ë¯¸ì§€ shape: {fake_batch.shape}\")  # (4, 64, 64, 3)\n","\n","# Discriminator í†µê³¼\n","real_output = discriminator(real_batch, training=False)\n","fake_output = discriminator(fake_batch, training=False)\n","\n","print(f\"\\nì§„ì§œ ì´ë¯¸ì§€ íŒë³„ ê²°ê³¼: {real_output.numpy().flatten()}\")\n","print(f\"ê°€ì§œ ì´ë¯¸ì§€ íŒë³„ ê²°ê³¼: {fake_output.numpy().flatten()}\")\n","print(\"\\n(í•™ìŠµ ì „ì´ë¼ ëœë¤í•œ ê°’)\")"]},{"cell_type":"markdown","id":"21f46e61","metadata":{"id":"21f46e61"},"source":["## 3.4 ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì„¤ì •"]},{"cell_type":"code","execution_count":null,"id":"4f3c11ab","metadata":{"id":"4f3c11ab"},"outputs":[],"source":["# Binary Cross Entropy with Logits\n","# Discriminator ì¶œë ¥ì— Sigmoidê°€ ì—†ìœ¼ë¯€ë¡œ from_logits=True\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","\n","def discriminator_loss(real_output, fake_output):\n","    \"\"\"\n","    Discriminator ì†ì‹¤ í•¨ìˆ˜\n","\n","    ëª©í‘œ: ì§„ì§œëŠ” 1, ê°€ì§œëŠ” 0ìœ¼ë¡œ ì •í™•íˆ ë¶„ë¥˜\n","\n","    Args:\n","        real_output: D(real_images) - ì§„ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ ê²°ê³¼\n","        fake_output: D(fake_images) - ê°€ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ ê²°ê³¼\n","\n","    Returns:\n","        total_loss: ì´ Discriminator ì†ì‹¤\n","    \"\"\"\n","    # ì§„ì§œ ì´ë¯¸ì§€ â†’ 1ë¡œ íŒë³„í•´ì•¼ í•¨\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","\n","    # ê°€ì§œ ì´ë¯¸ì§€ â†’ 0ìœ¼ë¡œ íŒë³„í•´ì•¼ í•¨\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","\n","def generator_loss(fake_output):\n","    \"\"\"\n","    Generator ì†ì‹¤ í•¨ìˆ˜\n","\n","    ëª©í‘œ: Discriminatorë¥¼ ì†ì—¬ì„œ ê°€ì§œë¥¼ ì§„ì§œ(1)ë¡œ íŒë³„í•˜ê²Œ ë§Œë“¤ê¸°\n","\n","    Args:\n","        fake_output: D(G(z)) - ê°€ì§œ ì´ë¯¸ì§€ì— ëŒ€í•œ íŒë³„ ê²°ê³¼\n","\n","    Returns:\n","        loss: Generator ì†ì‹¤\n","    \"\"\"\n","    # ê°€ì§œ ì´ë¯¸ì§€ê°€ 1ë¡œ íŒë³„ë˜ê¸°ë¥¼ ì›í•¨ (Dë¥¼ ì†ì´ê¸°)\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","\n","# DCGAN í‘œì¤€ Optimizer ì„¤ì •\n","generator_optimizer = tf.keras.optimizers.Adam(\n","    learning_rate=0.0002,\n","    beta_1=0.5,  # ê¸°ë³¸ê°’ 0.9ì—ì„œ ë³€ê²½ (DCGAN ê¶Œì¥)\n","    beta_2=0.999,\n",")\n","\n","discriminator_optimizer = tf.keras.optimizers.Adam(\n","    learning_rate=0.0002, beta_1=0.5, beta_2=0.999\n",")\n","\n","print(\"âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ\")\n","print(f\"   - Loss: BCE with Logits\")\n","print(f\"   - Optimizer: Adam (lr=0.0002, Î²1=0.5)\")"]},{"cell_type":"markdown","id":"cd0f3390","metadata":{"lines_to_next_cell":2,"id":"cd0f3390"},"source":["## 3.5 í•™ìŠµ Step í•¨ìˆ˜ êµ¬í˜„"]},{"cell_type":"code","execution_count":null,"id":"8ddcabde","metadata":{"id":"8ddcabde"},"outputs":[],"source":["@tf.function\n","def train_step(real_images):\n","    \"\"\"\n","    DCGAN í•œ ìŠ¤í… í•™ìŠµ\n","\n","    Args:\n","        real_images: ì§„ì§œ ì´ë¯¸ì§€ ë°°ì¹˜ (batch_size, 64, 64, 3)\n","\n","    Returns:\n","        d_loss: Discriminator ì†ì‹¤\n","        g_loss: Generator ì†ì‹¤\n","    \"\"\"\n","    # í˜„ì¬ ë°°ì¹˜ í¬ê¸°\n","    batch_size = tf.shape(real_images)[0]\n","\n","    # ë…¸ì´ì¦ˆ ìƒì„±\n","    noise = tf.random.normal([batch_size, NOISE_DIM])\n","\n","    # ========================================\n","    # Discriminator í•™ìŠµ\n","    # ========================================\n","    with tf.GradientTape() as disc_tape:\n","        # Generatorë¡œ ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±\n","        fake_images = generator(noise, training=True)\n","\n","        # Discriminatorë¡œ ì§„ì§œ/ê°€ì§œ íŒë³„\n","        real_output = discriminator(real_images, training=True)\n","        fake_output = discriminator(fake_images, training=True)\n","\n","        # Discriminator ì†ì‹¤ ê³„ì‚°\n","        d_loss = discriminator_loss(real_output, fake_output)\n","\n","    # Discriminator gradient ê³„ì‚° ë° ì ìš©\n","    gradients_of_discriminator = disc_tape.gradient(\n","        d_loss, discriminator.trainable_variables\n","    )\n","    discriminator_optimizer.apply_gradients(\n","        zip(gradients_of_discriminator, discriminator.trainable_variables)\n","    )\n","\n","    # ========================================\n","    # Generator í•™ìŠµ\n","    # ========================================\n","    # ìƒˆë¡œìš´ ë…¸ì´ì¦ˆ ìƒì„±\n","    noise = tf.random.normal([batch_size, NOISE_DIM])\n","\n","    with tf.GradientTape() as gen_tape:\n","        # Generatorë¡œ ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±\n","        fake_images = generator(noise, training=True)\n","\n","        # Discriminatorë¡œ íŒë³„\n","        fake_output = discriminator(fake_images, training=True)\n","\n","        # Generator ì†ì‹¤ ê³„ì‚°\n","        g_loss = generator_loss(fake_output)\n","\n","    # Generator gradient ê³„ì‚° ë° ì ìš©\n","    gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n","    generator_optimizer.apply_gradients(\n","        zip(gradients_of_generator, generator.trainable_variables)\n","    )\n","\n","    return d_loss, g_loss\n","\n","\n","print(\"âœ… train_step í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"]},{"cell_type":"markdown","id":"aa944391","metadata":{"id":"aa944391"},"source":["## 3.6 ì´ë¯¸ì§€ ìƒì„± ë° ì‹œê°í™” í•¨ìˆ˜"]},{"cell_type":"code","execution_count":null,"id":"3a0ab2dc","metadata":{"id":"3a0ab2dc"},"outputs":[],"source":["# ê³ ì • ë…¸ì´ì¦ˆ ì‹œë“œ (í•™ìŠµ ê³¼ì • ë¹„êµìš©)\n","fixed_seed = tf.random.normal([16, NOISE_DIM])\n","\n","\n","def generate_and_save_images(epoch, generator, seed):\n","    \"\"\"\n","    Generatorë¡œ ì´ë¯¸ì§€ ìƒì„± ë° ì‹œê°í™”\n","\n","    Args:\n","        epoch: í˜„ì¬ ì—í­ ë²ˆí˜¸\n","        generator: Generator ëª¨ë¸\n","        seed: ê³ ì •ëœ ë…¸ì´ì¦ˆ ì‹œë“œ\n","    \"\"\"\n","    # Generatorë¡œ ì´ë¯¸ì§€ ìƒì„± (training=False)\n","    predictions = generator(seed, training=False)\n","\n","    # ì‹œê°í™”\n","    fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n","\n","    for i, ax in enumerate(axes.flat):\n","        # [-1, 1] â†’ [0, 1]\n","        img = (predictions[i].numpy() + 1) / 2\n","        img = np.clip(img, 0, 1)  # ë²”ìœ„ ë³´ì •\n","        ax.imshow(img)\n","        ax.axis(\"off\")\n","\n","    plt.suptitle(f\"Epoch {epoch}\", fontsize=16)\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","# ì´ˆê¸° ì´ë¯¸ì§€ ìƒì„± (í•™ìŠµ ì „)\n","print(\"ğŸ¨ í•™ìŠµ ì „ Generator ì¶œë ¥\")\n","generate_and_save_images(0, generator, fixed_seed)"]},{"cell_type":"markdown","id":"14c9156c","metadata":{"id":"14c9156c"},"source":["## 3.7 ì „ì²´ í•™ìŠµ ì‹¤í–‰"]},{"cell_type":"code","execution_count":null,"id":"454ef480","metadata":{"id":"454ef480"},"outputs":[],"source":["# í•™ìŠµ ì‹¤í–‰\n","print(\"=\" * 60)\n","print(\"ğŸ”¥ DCGAN í•™ìŠµ ì‹œì‘\")\n","print(\"=\" * 60)\n","print(f\"ì´ ì—í­: {EPOCHS}\")\n","print(f\"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n","print(f\"ì´ë¯¸ì§€ í¬ê¸°: {IMG_SIZE}Ã—{IMG_SIZE}\")\n","print(\"=\" * 60)\n","\n","# Loss ê¸°ë¡\n","history = {\"d_loss\": [], \"g_loss\": []}\n","\n","for epoch in range(1, EPOCHS + 1):\n","    print(f\"\\nğŸ”¥ Epoch {epoch}/{EPOCHS}\")\n","\n","    epoch_d_loss = []\n","    epoch_g_loss = []\n","\n","    for batch_idx, real_batch in enumerate(dataset):\n","        d_loss, g_loss = train_step(real_batch)\n","\n","        epoch_d_loss.append(d_loss.numpy())\n","        epoch_g_loss.append(g_loss.numpy())\n","\n","        # ì§„í–‰ ìƒí™© ì¶œë ¥ (20 ë°°ì¹˜ë§ˆë‹¤)\n","        if (batch_idx + 1) % 20 == 0:\n","            print(\n","                f\"  Batch {batch_idx + 1} - D Loss: {d_loss:.4f}, G Loss: {g_loss:.4f}\"\n","            )\n","\n","    # ì—í­ í‰ê·  Loss ê¸°ë¡\n","    avg_d_loss = np.mean(epoch_d_loss)\n","    avg_g_loss = np.mean(epoch_g_loss)\n","    history[\"d_loss\"].append(avg_d_loss)\n","    history[\"g_loss\"].append(avg_g_loss)\n","\n","    print(f\"ğŸ“Š Epoch {epoch} í‰ê·  - D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}\")\n","\n","    # 10 ì—í­ë§ˆë‹¤ ì´ë¯¸ì§€ ìƒì„±\n","    if epoch % 10 == 0:\n","        generate_and_save_images(epoch, generator, fixed_seed)\n","\n","print(\"\\nâœ… í•™ìŠµ ì™„ë£Œ!\")"]},{"cell_type":"code","execution_count":null,"id":"86c84c25","metadata":{"id":"86c84c25"},"outputs":[],"source":["# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history[\"d_loss\"], label=\"Discriminator Loss\", color=\"blue\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Discriminator Loss\")\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history[\"g_loss\"], label=\"Generator Loss\", color=\"red\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Generator Loss\")\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"2ae0bfaf","metadata":{"lines_to_next_cell":0,"id":"2ae0bfaf"},"outputs":[],"source":["# ìµœì¢… ìƒì„± ì´ë¯¸ì§€ (ë” ë§ì€ ìƒ˜í”Œ)\n","print(\"ğŸ¨ ìµœì¢… ìƒì„± ì´ë¯¸ì§€\")\n","\n","final_noise = tf.random.normal([25, NOISE_DIM])\n","final_images = generator(final_noise, training=False)\n","\n","plt.figure(figsize=(10, 10))\n","for i in range(25):\n","    plt.subplot(5, 5, i + 1)\n","    img = (final_images[i].numpy() + 1) / 2\n","    img = np.clip(img, 0, 1)\n","    plt.imshow(img)\n","    plt.axis(\"off\")\n","plt.suptitle(\"DCGAN ìµœì¢… ìƒì„± ì´ë¯¸ì§€ (TensorFlow)\", fontsize=16)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"d2c0574e","metadata":{"id":"d2c0574e"},"source":["---\n","# ğŸ“ Day 2 ë³µìŠµ ë° ì •ë¦¬\n","\n","---"]},{"cell_type":"markdown","id":"4195ce7c","metadata":{"id":"4195ce7c"},"source":["## ğŸ¯ ì˜¤ëŠ˜ ë°°ìš´ í•µì‹¬ ë‚´ìš©\n","\n","### 1. Vanilla GANì˜ í•œê³„ì \n","\n","| ë¬¸ì œ | ì„¤ëª… | DCGAN í•´ê²°ì±… |\n","|------|------|-------------|\n","| **Training Instability** | Gradient vanishing/exploding | BatchNorm, ì ì ˆí•œ í™œì„±í™” í•¨ìˆ˜ |\n","| **Mode Collapse** | í•œ ê°€ì§€ íŒ¨í„´ë§Œ ë°˜ë³µ ìƒì„± | ì•ˆì •ì ì¸ í•™ìŠµ êµ¬ì¡° |\n","| **ê³µê°„ ì •ë³´ ì†ì‹¤** | FC Layerë¡œ ì¸í•œ êµ¬ì¡° íŒŒê´´ | CNN ì‚¬ìš© |\n","| **íŒŒë¼ë¯¸í„° í­ì¦** | ê³ í•´ìƒë„ í™•ì¥ ë¶ˆê°€ëŠ¥ | Convolution íŒŒë¼ë¯¸í„° ê³µìœ  |\n","\n","---"]},{"cell_type":"markdown","id":"541999fa","metadata":{"id":"541999fa"},"source":["### 2. DCGANì˜ 5ê°€ì§€ ì„¤ê³„ ì›ì¹™\n","\n","#### âœ… ì›ì¹™ 1: Pooling â†’ Strided Convolution\n","- MaxPooling ëŒ€ì‹  **Strided Convolution** ì‚¬ìš©\n","- í•™ìŠµ ê°€ëŠ¥í•œ ë‹¤ìš´ìƒ˜í”Œë§\n","- ëª¨ë“  ìœ„ì¹˜ì˜ ì •ë³´ ë³´ì¡´\n","\n","#### âœ… ì›ì¹™ 2: FC Layer ì œê±° (ì¤‘ê°„ì¸µ)\n","- Generator: ì²« ì¸µë§Œ Dense â†’ ì¦‰ì‹œ Reshape\n","- Discriminator: ë§ˆì§€ë§‰ ì¸µë§Œ Dense\n","- ê³µê°„ êµ¬ì¡° ë³´ì¡´\n","\n","#### âœ… ì›ì¹™ 3: Batch Normalization\n","- ëª¨ë“  ì¸µì— ì ìš© (ì…ë ¥ì¸µ, ì¶œë ¥ì¸µ ì œì™¸)\n","- Internal Covariate Shift ë°©ì§€\n","- í•™ìŠµ ì•ˆì •í™”\n","\n","#### âœ… ì›ì¹™ 4: ì ì ˆí•œ í™œì„±í™” í•¨ìˆ˜\n","- Generator ì€ë‹‰ì¸µ: **ReLU**\n","- Generator ì¶œë ¥ì¸µ: **Tanh** ([-1, 1])\n","- Discriminator ì€ë‹‰ì¸µ: **LeakyReLU(0.2)**\n","\n","#### âœ… ì›ì¹™ 5: Optimizer ì„¤ì •\n","- Adam optimizer\n","- Learning rate: 0.0002\n","- Beta1: 0.5 (ê¸°ë³¸ê°’ 0.9ì—ì„œ ë³€ê²½)\n","\n","---"]},{"cell_type":"markdown","id":"517680c7","metadata":{"id":"517680c7"},"source":["### 3. Transposed Convolution\n","\n","**ëª©ì :** ì‘ì€ feature map â†’ í° feature map (ì—…ìƒ˜í”Œë§)\n","\n","**ì¶œë ¥ í¬ê¸° ê³„ì‚° (padding='same', stride=2):**\n","```\n","ì¶œë ¥ í¬ê¸° = ì…ë ¥ í¬ê¸° Ã— stride\n","ì˜ˆ: 4Ã—4 â†’ 8Ã—8 â†’ 16Ã—16 â†’ 32Ã—32 â†’ 64Ã—64\n","```\n","\n","**ì£¼ì˜ì‚¬í•­:**\n","- Checkerboard Artifact ë°©ì§€\n","- kernel_sizeê°€ strideë¡œ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ë„ë¡ ì„¤ì •\n","- ì˜ˆ: kernel=4, stride=2 âœ…\n","\n","---"]},{"cell_type":"markdown","id":"cd5cb888","metadata":{"id":"cd5cb888"},"source":["### 4. TensorFlow vs PyTorch êµ¬í˜„ ì°¨ì´\n","\n","| í•­ëª© | TensorFlow | PyTorch |\n","|------|-----------|---------|\n","| **ì´ë¯¸ì§€ í˜•ì‹** | (N, H, W, C) | (N, C, H, W) |\n","| **ì •ê·œí™”** | `(x - 127.5) / 127.5` | `Normalize((0.5,), (0.5,))` |\n","| **Transposed Conv** | `Conv2DTranspose` | `ConvTranspose2d` |\n","| **Loss** | `BinaryCrossentropy(from_logits=True)` | `BCEWithLogitsLoss()` |\n","| **í•™ìŠµ ëª¨ë“œ** | `training=True/False` | `model.train()` / `model.eval()` |\n","| **Gradient** | `GradientTape()` | `loss.backward()` |\n","\n","---"]},{"cell_type":"markdown","id":"9e5bd2ba","metadata":{"id":"9e5bd2ba"},"source":["## ğŸ“Š DCGAN vs Vanilla GAN ë¹„êµ ìš”ì•½\n","\n","| í•­ëª© | Vanilla GAN | DCGAN |\n","|------|-------------|-------|\n","| **êµ¬ì¡°** | Fully Connected | Convolutional |\n","| **ë‹¤ìš´ìƒ˜í”Œë§** | - | Strided Conv |\n","| **ì—…ìƒ˜í”Œë§** | - | Transposed Conv |\n","| **ì •ê·œí™”** | ì—†ìŒ | Batch Normalization |\n","| **í™œì„±í™”** | ReLU, Sigmoid | ReLU/LeakyReLU, Tanh |\n","| **í•™ìŠµ ì•ˆì •ì„±** | ë§¤ìš° ë¶ˆì•ˆì • | ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì • |\n","| **Mode Collapse** | ìì£¼ ë°œìƒ | ëœ ë°œìƒ |\n","| **ì´ë¯¸ì§€ í’ˆì§ˆ** | ë‚®ìŒ (íë¦¿í•¨) | ë†’ìŒ (ì„ ëª…í•¨) |\n","| **í•´ìƒë„** | 28Ã—28 ì •ë„ | 64Ã—64 ì´ìƒ |\n","\n","---"]},{"cell_type":"markdown","id":"92b7b8bf","metadata":{"id":"92b7b8bf"},"source":["## ğŸ” ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)\n","\n","### Q1. ì™œ Generator ì¶œë ¥ì¸µì—ëŠ” BatchNormì„ ì“°ì§€ ì•Šë‚˜ìš”?\n","\n","**A:** Generator ì¶œë ¥ì¸µì€ Tanh í™œì„±í™”ë¡œ [-1, 1] ë²”ìœ„ë¥¼ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤. BatchNormì„ ì ìš©í•˜ë©´ ì´ ë²”ìœ„ê°€ ë³€ê²½ë˜ì–´ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§‘ë‹ˆë‹¤.\n","\n","```python\n","# âŒ ì˜ëª»ëœ ì˜ˆ\n","output = Conv2DTranspose(...)(x)\n","output = BatchNormalization()(output)  # ë²”ìœ„ ë³€ê²½!\n","output = Tanh()(output)\n","\n","# âœ… ì˜¬ë°”ë¥¸ ì˜ˆ\n","output = Conv2DTranspose(..., activation='tanh')(x)  # BatchNorm ì—†ìŒ\n","```\n","\n","---\n","\n","### Q2. Discriminator ì…ë ¥ì¸µì—ë„ BatchNormì´ ì—†ëŠ” ì´ìœ ëŠ”?\n","\n","**A:** ì…ë ¥ì¸µì— BatchNormì„ ì ìš©í•˜ë©´ ì›ë³¸ ì´ë¯¸ì§€ì˜ ë¶„í¬ê°€ ë³€ê²½ë©ë‹ˆë‹¤. DiscriminatorëŠ” \"ì‹¤ì œ ë°ì´í„° ë¶„í¬\"ë¥¼ í•™ìŠµí•´ì•¼ í•˜ë¯€ë¡œ, ì…ë ¥ì€ ì •ê·œí™”í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n","\n","```python\n","# Discriminator ì²« ë²ˆì§¸ ì¸µ\n","model.add(Conv2D(...))  # BatchNorm ì—†ìŒ!\n","model.add(LeakyReLU(0.2))\n","```\n","\n","---\n","\n","### Q3. LeakyReLUì˜ alpha=0.2ëŠ” ì–´ë–»ê²Œ ì •í•´ì¡Œë‚˜ìš”?\n","\n","**A:** DCGAN ë…¼ë¬¸ì—ì„œ ì‹¤í—˜ì ìœ¼ë¡œ 0.2ê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ 0.1~0.3 ì‚¬ì´ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n","\n","```python\n","# Î± ê°’ì— ë”°ë¥¸ íŠ¹ì§•\n","Î± = 0.0  â†’ ReLU (Dying ReLU ë¬¸ì œ)\n","Î± = 0.2  â†’ DCGAN ê¶Œì¥ê°’ (ê· í˜•)\n","Î± = 1.0  â†’ ì„ í˜• í•¨ìˆ˜ (ì˜ë¯¸ ì—†ìŒ)\n","```\n","\n","---\n","\n","### Q4. ì™œ Generatorì™€ Discriminatorì˜ í•™ìŠµë¥ ì´ ê°™ë‚˜ìš”?\n","\n","**A:** DCGAN ë…¼ë¬¸ì—ì„œëŠ” ë‘˜ ë‹¤ 0.0002ë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, ì‹¤ì œë¡œëŠ” ìƒí™©ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n","\n","```python\n","# DCGAN í‘œì¤€\n","lr_g = 0.0002\n","lr_d = 0.0002\n","\n","# ê²½ìš°ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥\n","lr_g = 0.0001  # Gë¥¼ ë” ì²œì²œíˆ\n","lr_d = 0.0004  # Dë¥¼ ë” ë¹ ë¥´ê²Œ\n","```\n","\n","---\n","\n","### Q5. Mode Collapseê°€ ë°œìƒí•˜ë©´ ì–´ë–»ê²Œ í•´ê²°í•˜ë‚˜ìš”?\n","\n","**A:** ì—¬ëŸ¬ í•´ê²° ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤:\n","\n","1. **í•™ìŠµë¥  ì¡°ì •:** Dì˜ í•™ìŠµë¥ ì„ ë‚®ì¶”ê±°ë‚˜ Gì˜ í•™ìŠµë¥ ì„ ë†’ì„\n","2. **Label Smoothing:** ì§„ì§œ ë¼ë²¨ì„ 1.0 â†’ 0.9ë¡œ ë³€ê²½\n","3. **Noisy Labels:** ë¼ë²¨ì— ë…¸ì´ì¦ˆ ì¶”ê°€ (í™•ë¥ ì ìœ¼ë¡œ ë’¤ì§‘ê¸°)\n","4. **ë” ê¸´ í•™ìŠµ:** ì¶©ë¶„í•œ ì—í­ ìˆ˜ í™•ë³´\n","5. **ë‹¤ë¥¸ GAN ê¸°ë²•:** Wasserstein GAN, Spectral Normalization ë“±\n","\n","---\n","\n","### Q6. BatchNormì˜ momentum íŒŒë¼ë¯¸í„°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n","\n","**A:** BatchNormì€ ì´ë™ í‰ê· (moving average)ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ì‹œ í†µê³„ëŸ‰ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n","\n","```python\n","# TensorFlow\n","BatchNormalization(momentum=0.9)  # ê¸°ë³¸ê°’\n","\n","# PyTorch\n","nn.BatchNorm2d(channels, momentum=0.1)  # ì£¼ì˜: TFì™€ ì •ì˜ê°€ ë°˜ëŒ€!\n","```\n","\n","- **TensorFlow:** `new = momentum * old + (1-momentum) * new`\n","- **PyTorch:** `new = (1-momentum) * old + momentum * new`\n","\n","---\n","\n","### Q7. Transposed Convì™€ UpSampling+Conv ì¤‘ ì–´ëŠ ê²ƒì´ ë” ì¢‹ë‚˜ìš”?\n","\n","**A:** ê°ê° ì¥ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤:\n","\n","| í•­ëª© | Transposed Conv | UpSampling + Conv |\n","|------|----------------|-------------------|\n","| **ì†ë„** | ë¹ ë¦„ | ëŠë¦¼ |\n","| **íŒŒë¼ë¯¸í„°** | ì ìŒ | ë§ìŒ |\n","| **Artifact** | ë°œìƒ ê°€ëŠ¥ | ì ìŒ |\n","| **ì‚¬ìš©ì²˜** | DCGAN, Pix2Pix | StyleGAN, ProGAN |\n","\n","---\n","\n","### Q8. í•™ìŠµ ì¤‘ D Lossê°€ 0ì— ê°€ê¹Œì›Œì§€ë©´ ë¬¸ì œì¸ê°€ìš”?\n","\n","**A:** ë„¤, ì‹¬ê°í•œ ë¬¸ì œì…ë‹ˆë‹¤!\n","\n","```\n","ì •ìƒì ì¸ í•™ìŠµ:          ë¬¸ì œ ìˆëŠ” í•™ìŠµ:\n","D Loss: 0.6-0.8        D Loss: 0.01-0.05  â† Dê°€ ë„ˆë¬´ ê°•í•¨!\n","G Loss: 0.8-1.2        G Loss: 5.0-10.0   â† Gê°€ í•™ìŠµ ëª»í•¨\n","```\n","\n","**í•´ê²°ì±…:**\n","1. Dì˜ í•™ìŠµë¥  ë‚®ì¶”ê¸°\n","2. Gë¥¼ ë” ìì£¼ í•™ìŠµ (ì˜ˆ: D 1íšŒ â†’ G 2íšŒ)\n","3. Label Smoothing ì ìš©\n","\n","---\n","\n","### Q9. CelebA ë°ì´í„°ì…‹ì€ ì–´ë””ì„œ ë‹¤ìš´ë¡œë“œí•˜ë‚˜ìš”?\n","\n","**A:** ì—¬ëŸ¬ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤:\n","\n","```python\n","# 1. Kaggle API\n","!kaggle datasets download -d jessicali9530/celeba-dataset\n","\n","# 2. TensorFlow Datasets\n","import tensorflow_datasets as tfds\n","ds = tfds.load('celeb_a', split='train')\n","\n","# 3. PyTorch Torchvision\n","from torchvision.datasets import CelebA\n","dataset = CelebA(root='./data', split='train', download=True)\n","\n","# 4. ì§ì ‘ ë‹¤ìš´ë¡œë“œ\n","# https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n","```\n","\n","---\n","\n","### Q10. DCGANìœ¼ë¡œ ê³ í•´ìƒë„ ì´ë¯¸ì§€(256Ã—256 ì´ìƒ)ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‚˜ìš”?\n","\n","**A:** ê°€ëŠ¥í•˜ì§€ë§Œ ì œí•œì ì…ë‹ˆë‹¤:\n","\n","- **64Ã—64:** DCGANì˜ sweet spot âœ…\n","- **128Ã—128:** ê°€ëŠ¥í•˜ë‚˜ í’ˆì§ˆ ì €í•˜\n","- **256Ã—256:** ì–´ë ¤ì›€, Mode Collapse ì‰½ê²Œ ë°œìƒ\n","- **512Ã—512 ì´ìƒ:** Progressive GAN, StyleGAN ë“± ì‚¬ìš© ê¶Œì¥\n","\n","```python\n","# 128Ã—128ë¥¼ ìœ„í•œ ìˆ˜ì •\n","# Generatorì— ConvTranspose ì¸µ 1ê°œ ì¶”ê°€\n","# Discriminatorì— Conv ì¸µ 1ê°œ ì¶”ê°€\n","```\n","\n","---"]},{"cell_type":"markdown","id":"075a8859","metadata":{"id":"075a8859"},"source":["## ğŸ“ ê°ê´€ì‹ í€´ì¦ˆ\n","\n","### í€´ì¦ˆ 1\n","\n","**DCGANì—ì„œ Generatorì˜ ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ë¡œ Tanhë¥¼ ì‚¬ìš©í•˜ëŠ” ì£¼ëœ ì´ìœ ëŠ”?**\n","\n","â‘  ReLUë³´ë‹¤ í•™ìŠµì´ ë¹ ë¥´ê¸° ë•Œë¬¸\n","â‘¡ ì¶œë ¥ ë²”ìœ„ë¥¼ [-1, 1]ë¡œ ì œí•œí•˜ê¸° ìœ„í•´\n","â‘¢ Gradient vanishingì„ ë°©ì§€í•˜ê¸° ìœ„í•´\n","â‘£ BatchNormê³¼ í•¨ê»˜ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸"]},{"cell_type":"markdown","id":"5bcd0a13","metadata":{"id":"5bcd0a13"},"source":["<details>\n","<summary><b>ì •ë‹µ ë° í•´ì„¤ ë³´ê¸°</b></summary>\n","\n","**ì •ë‹µ: â‘¡**\n","\n","**í•´ì„¤:**\n","- TanhëŠ” ì¶œë ¥ ë²”ìœ„ë¥¼ **[-1, 1]**ë¡œ ì œí•œí•©ë‹ˆë‹¤.\n","- ì…ë ¥ ì´ë¯¸ì§€ë„ ë™ì¼í•˜ê²Œ [-1, 1]ë¡œ ì •ê·œí™”í•˜ë¯€ë¡œ, Generator ì¶œë ¥ê³¼ ì‹¤ì œ ì´ë¯¸ì§€ì˜ ë²”ìœ„ê°€ ì¼ì¹˜í•©ë‹ˆë‹¤.\n","- ì´ëŠ” Discriminatorê°€ ë” ì‰½ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\n","\n","**ì˜¤ë‹µ ë¶„ì„:**\n","- â‘ : ReLUì™€ Tanhì˜ í•™ìŠµ ì†ë„ ì°¨ì´ëŠ” ë¯¸ë¯¸í•¨\n","- â‘¢: Tanhë„ gradient vanishing ë¬¸ì œê°€ ìˆìŒ (í•˜ì§€ë§Œ [-1, 1] ë²”ìœ„ ë•Œë¬¸ì— ì‚¬ìš©)\n","- â‘£: ì¶œë ¥ì¸µì—ëŠ” BatchNormì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n","\n","</details>"]},{"cell_type":"markdown","id":"feb3146f","metadata":{"id":"feb3146f"},"source":["### í€´ì¦ˆ 2\n","\n","**ë‹¤ìŒ ì¤‘ DCGANì˜ ì„¤ê³„ ì›ì¹™ì´ ì•„ë‹Œ ê²ƒì€?**\n","\n","â‘  Pooling Layer ëŒ€ì‹  Strided Convolution ì‚¬ìš©\n","â‘¡ ì¤‘ê°„ì¸µì— Fully Connected Layer ì œê±°\n","â‘¢ Batch Normalizationì„ ëª¨ë“  ì¸µì— ì ìš©\n","â‘£ Discriminatorì—ì„œ Dropout ì‚¬ìš©"]},{"cell_type":"markdown","id":"2007e0df","metadata":{"id":"2007e0df"},"source":["<details>\n","<summary><b>ì •ë‹µ ë° í•´ì„¤ ë³´ê¸°</b></summary>\n","\n","**ì •ë‹µ: â‘£**\n","\n","**í•´ì„¤:**\n","- DCGAN ë…¼ë¬¸ì—ì„œëŠ” **Dropoutì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**.\n","- ëŒ€ì‹  **Batch Normalization**ìœ¼ë¡œ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n","- Dropoutì€ ì¼ë¶€ GAN ë³€í˜•ì—ì„œ ì‚¬ìš©ë˜ê¸°ë„ í•˜ì§€ë§Œ, DCGANì˜ í‘œì¤€ ì„¤ê³„ ì›ì¹™ì€ ì•„ë‹™ë‹ˆë‹¤.\n","\n","**DCGANì˜ 5ê°€ì§€ ì›ì¹™ ë³µìŠµ:**\n","1. Pooling â†’ Strided Conv âœ…\n","2. FC Layer ì œê±° (ì¤‘ê°„ì¸µ) âœ…\n","3. Batch Normalization âœ…\n","4. ì ì ˆí•œ í™œì„±í™” í•¨ìˆ˜ (ReLU/LeakyReLU/Tanh) âœ…\n","5. Adam Optimizer (lr=0.0002, Î²1=0.5) âœ…\n","\n","</details>"]},{"cell_type":"markdown","id":"a961f709","metadata":{"id":"a961f709"},"source":["### í€´ì¦ˆ 3\n","\n","**Discriminatorì—ì„œ LeakyReLUë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ”?**\n","\n","â‘  ê³„ì‚° ì†ë„ê°€ ë¹ ë¥´ê¸° ë•Œë¬¸\n","â‘¡ Dying ReLU ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´\n","â‘¢ ì¶œë ¥ ë²”ìœ„ë¥¼ ì œí•œí•˜ê¸° ìœ„í•´\n","â‘£ BatchNormê³¼ì˜ í˜¸í™˜ì„± ë•Œë¬¸"]},{"cell_type":"markdown","id":"91cd46e2","metadata":{"id":"91cd46e2"},"source":["<details>\n","<summary><b>ì •ë‹µ ë° í•´ì„¤ ë³´ê¸°</b></summary>\n","\n","**ì •ë‹µ: â‘¡**\n","\n","**í•´ì„¤:**\n","- **Dying ReLU ë¬¸ì œ:** ReLUëŠ” ìŒìˆ˜ ì˜ì—­ì—ì„œ gradientê°€ 0ì´ ë˜ì–´ ë‰´ëŸ°ì´ \"ì£½ëŠ”\" í˜„ìƒ ë°œìƒ\n","- **LeakyReLU:** ìŒìˆ˜ ì˜ì—­ì—ì„œë„ ì‘ì€ gradient(Î±=0.2)ë¥¼ ìœ ì§€í•˜ì—¬ ì´ ë¬¸ì œ í•´ê²°\n","\n","```python\n","# ReLU vs LeakyReLU\n","ReLU(x) = max(0, x)           # x < 0ì¼ ë•Œ gradient = 0\n","LeakyReLU(x) = max(0.2x, x)   # x < 0ì¼ ë•Œ gradient = 0.2\n","```\n","\n","**ì˜¤ë‹µ ë¶„ì„:**\n","- â‘ : ê³„ì‚° ì†ë„ëŠ” ReLUì™€ ê±°ì˜ ë™ì¼\n","- â‘¢: LeakyReLUëŠ” ì¶œë ¥ ë²”ìœ„ë¥¼ ì œí•œí•˜ì§€ ì•ŠìŒ (ê·¸ê²Œ Tanhì˜ ì—­í• )\n","- â‘£: BatchNormê³¼ì˜ í˜¸í™˜ì„±ì€ ReLUì™€ ë™ì¼\n","\n","</details>"]},{"cell_type":"markdown","id":"d0a806ca","metadata":{"id":"d0a806ca"},"source":["### í€´ì¦ˆ 4\n","\n","**Transposed Convolutionì—ì„œ kernel_size=4, stride=2, padding='same'ì¼ ë•Œ, 4Ã—4 ì…ë ¥ì˜ ì¶œë ¥ í¬ê¸°ëŠ”?**\n","\n","â‘  2Ã—2\n","â‘¡ 4Ã—4\n","â‘¢ 8Ã—8\n","â‘£ 16Ã—16"]},{"cell_type":"markdown","id":"68e922cb","metadata":{"id":"68e922cb"},"source":["<details>\n","<summary><b>ì •ë‹µ ë° í•´ì„¤ ë³´ê¸°</b></summary>\n","\n","**ì •ë‹µ: â‘¢ (8Ã—8)**\n","\n","**í•´ì„¤:**\n","- `padding='same'`ì´ê³  `stride=2`ì¼ ë•Œ, **ì¶œë ¥ í¬ê¸° = ì…ë ¥ í¬ê¸° Ã— stride**\n","- ê³„ì‚°: 4 Ã— 2 = 8\n","\n","```\n","ì…ë ¥: 4Ã—4\n","Transposed Conv (kernel=4, stride=2, padding='same')\n","ì¶œë ¥: 8Ã—8\n","```\n","\n","**ì¼ë°˜ ê³µì‹:**\n","```\n","padding='same'ì¼ ë•Œ:\n","output_size = input_size Ã— stride\n","\n","ì˜ˆì‹œ:\n","4Ã—4  â†’ 8Ã—8   (stride=2)\n","8Ã—8  â†’ 16Ã—16 (stride=2)\n","16Ã—16 â†’ 32Ã—32 (stride=2)\n","```\n","\n","</details>"]},{"cell_type":"markdown","id":"2ef7a300","metadata":{"id":"2ef7a300"},"source":["### í€´ì¦ˆ 5\n","\n","**DCGANì—ì„œ Adam optimizerì˜ beta1ì„ 0.5ë¡œ ì„¤ì •í•˜ëŠ” ì´ìœ ëŠ”?**\n","\n","â‘  í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´\n","â‘¡ GANì˜ ë¶ˆì•ˆì •ì„±ì— ë§ì¶° ê³¼ê±° gradient ì˜ì¡´ë„ë¥¼ ë‚®ì¶”ê¸° ìœ„í•´\n","â‘¢ Mode Collapseë¥¼ ì™„ì „íˆ ë°©ì§€í•˜ê¸° ìœ„í•´\n","â‘£ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´"]},{"cell_type":"markdown","id":"613c57b6","metadata":{"id":"613c57b6"},"source":["<details>\n","<summary><b>ì •ë‹µ ë° í•´ì„¤ ë³´ê¸°</b></summary>\n","\n","**ì •ë‹µ: â‘¡**\n","\n","**í•´ì„¤:**\n","- **Beta1ì˜ ì—­í• :** ê³¼ê±° gradientì˜ ì§€ìˆ˜ ì´ë™ í‰ê· ì„ ê³„ì‚°í•˜ëŠ” ê°ì‡  ê³„ìˆ˜\n","- **ê¸°ë³¸ê°’ 0.9:** ê³¼ê±° gradientë¥¼ ë§ì´ ì°¸ê³  â†’ ì•ˆì •ì ì´ì§€ë§Œ ëŠë¦¼\n","- **DCGAN 0.5:** í˜„ì¬ gradientì— ë” ì§‘ì¤‘ â†’ ë¹ ë¥´ì§€ë§Œ ë¶ˆì•ˆì •\n","\n","```python\n","# Adam optimizerì˜ momentum\n","m_t = beta1 * m_{t-1} + (1 - beta1) * gradient\n","\n","beta1 = 0.9 (ê¸°ë³¸ê°’)  â†’ ê³¼ê±° 90% + í˜„ì¬ 10%\n","beta1 = 0.5 (DCGAN)   â†’ ê³¼ê±° 50% + í˜„ì¬ 50%\n","```\n","\n","**ì™œ 0.5ê°€ GANì— ì í•©í•œê°€?**\n","- GANì€ ë³¸ì§ˆì ìœ¼ë¡œ ë¶ˆì•ˆì • (Dì™€ Gì˜ min-max ê²Œì„)\n","- ê³¼ê±° gradientë¥¼ ë„ˆë¬´ ë§ì´ ì°¸ê³ í•˜ë©´ ì§„ë™(oscillation) ë°œìƒ\n","- í˜„ì¬ ìƒí™©ì— ë” ë¹ ë¥´ê²Œ ë°˜ì‘í•˜ë„ë¡ beta1ì„ ë‚®ì¶¤\n","\n","**ì˜¤ë‹µ ë¶„ì„:**\n","- â‘ : í•™ìŠµ ì†ë„ì™€ëŠ” ì§ì ‘ì  ê´€ë ¨ ì—†ìŒ (learning rateê°€ ë” ì¤‘ìš”)\n","- â‘¢: Mode Collapseë¥¼ ì™„ì „íˆ ë°©ì§€í•  ìˆ˜ëŠ” ì—†ìŒ (ì™„í™” íš¨ê³¼ë§Œ)\n","- â‘£: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ ê±°ì˜ ë™ì¼\n","\n","</details>"]},{"cell_type":"markdown","id":"bee10b52","metadata":{"id":"bee10b52"},"source":["### í€´ì¦ˆ 6\n","\n","**ë‹¤ìŒ ì¤‘ Mode Collapseì˜ íŠ¹ì§•ì´ ì•„ë‹Œ ê²ƒì€?**\n","\n","â‘  Generatorê°€ í•œ ê°€ì§€ íŒ¨í„´ë§Œ ë°˜ë³µ ìƒì„±\n","â‘¡ Discriminator Lossê°€ 0ì— ê°€ê¹Œì›Œì§\n","â‘¢ ìƒì„± ì´ë¯¸ì§€ì˜ ë‹¤ì–‘ì„±ì´ ê°ì†Œ\n","â‘£ Generator Lossê°€ ê¸‰ê²©íˆ ì¦ê°€"]},{"cell_type":"markdown","id":"0e2603f5","metadata":{"id":"0e2603f5"},"source":["<details>\n","<summary><b>ì •ë‹µ ë° í•´ì„¤ ë³´ê¸°</b></summary>\n","\n","**ì •ë‹µ: â‘¡**\n","\n","**í•´ì„¤:**\n","- Mode Collapse ì‹œ **Discriminator LossëŠ” ì˜¤íˆë ¤ ì¦ê°€**í•©ë‹ˆë‹¤.\n","- ì´ìœ : Generatorê°€ í•œ ê°€ì§€ íŒ¨í„´ë§Œ ìƒì„±í•˜ë©´, Discriminatorê°€ ì‰½ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆì–´ lossê°€ ë†’ì•„ì§\n","\n","**Mode Collapseì˜ ì§•í›„:**\n","```\n","ì •ìƒ í•™ìŠµ:             Mode Collapse:\n","D Loss: 0.6-0.8        D Loss: 1.2-2.0 â†‘ (ì¦ê°€!)\n","G Loss: 0.8-1.2        G Loss: 4.0-8.0 â†‘ (ê¸‰ì¦!)\n","ì´ë¯¸ì§€: ë‹¤ì–‘í•¨          ì´ë¯¸ì§€: ë™ì¼ íŒ¨í„´ ë°˜ë³µ\n","```\n","\n","**ì™œ ì´ëŸ° í˜„ìƒì´ ë°œìƒí•˜ë‚˜?**\n","1. Gê°€ \"ì•ˆì „í•œ\" í•œ ê°€ì§€ ì´ë¯¸ì§€ë§Œ ìƒì„±\n","2. DëŠ” ì´ íŒ¨í„´ì„ ì‰½ê²Œ ê°ì§€ â†’ \"ê°€ì§œ\"ë¼ê³  ì •í™•íˆ íŒë³„\n","3. D Loss ì¦ê°€, G Loss ê¸‰ì¦\n","4. GëŠ” ë” ë‹¤ì–‘í•œ ì‹œë„ë¥¼ í•˜ì§€ ì•Šê³  ê³„ì† ê°™ì€ íŒ¨í„´ë§Œ ìƒì„±\n","\n","</details>"]},{"cell_type":"markdown","id":"5ca6d0d3","metadata":{"id":"5ca6d0d3"},"source":["### í€´ì¦ˆ 7\n","\n","**BatchNormì„ ì ìš©í•˜ì§€ ì•ŠëŠ” ì¸µì€? (2ê°œ ì„ íƒ)**\n","\n","â‘  Generatorì˜ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ\n","â‘¡ Generatorì˜ ì¶œë ¥ì¸µ\n","â‘¢ Discriminatorì˜ ì…ë ¥ì¸µ\n","â‘£ Discriminatorì˜ ì€ë‹‰ì¸µ"]},{"cell_type":"markdown","id":"8658b603","metadata":{"id":"8658b603"},"source":["<details>\n","<summary><b>ì •ë‹µ ë° í•´ì„¤ ë³´ê¸°</b></summary>\n","\n","**ì •ë‹µ: â‘¡ â‘¢**\n","\n","**í•´ì„¤:**\n","\n","**BatchNormì„ ì ìš©í•˜ì§€ ì•ŠëŠ” ì¸µ:**\n","- **Generator ì¶œë ¥ì¸µ (â‘¡):** Tanh ì¶œë ¥ ë²”ìœ„ [-1, 1] ìœ ì§€ í•„ìš”\n","- **Discriminator ì…ë ¥ì¸µ (â‘¢):** ì›ë³¸ ì´ë¯¸ì§€ ë¶„í¬ ë³´ì¡´ í•„ìš”\n","\n","```python\n","# Generator\n","x = Conv2DTranspose(...)(x)\n","x = BatchNormalization()(x)  # ì€ë‹‰ì¸µ: BatchNorm âœ…\n","x = ReLU()(x)\n","...\n","output = Conv2DTranspose(..., activation='tanh')(x)  # ì¶œë ¥ì¸µ: BatchNorm âŒ\n","\n","# Discriminator\n","x = Conv2D(...)(input)  # ì…ë ¥ì¸µ: BatchNorm âŒ\n","x = LeakyReLU(0.2)(x)\n","x = Conv2D(...)(x)\n","x = BatchNormalization()(x)  # ì€ë‹‰ì¸µ: BatchNorm âœ…\n","```\n","\n","**ì™œ ì´ë ‡ê²Œ ì„¤ê³„ë˜ì—ˆë‚˜?**\n","- **Generator ì¶œë ¥:** ì´ë¯¸ì§€ ìƒì„± ì‹œ [-1, 1] ë²”ìœ„ê°€ í•„ìˆ˜\n","- **Discriminator ì…ë ¥:** ì‹¤ì œ ë°ì´í„° ë¶„í¬ë¥¼ ê·¸ëŒ€ë¡œ í•™ìŠµí•´ì•¼ í•¨\n","\n","</details>"]},{"cell_type":"markdown","id":"63bb7789","metadata":{"id":"63bb7789"},"source":["---\n","# ğŸ§ª ì‹¤ìŠµ ì½”ë“œ ìƒì„¸ ë¶„ì„\n","\n","---"]},{"cell_type":"markdown","id":"15afa864","metadata":{"id":"15afa864"},"source":["## ğŸ“Œ TensorFlow DCGAN ì½”ë“œ ì™„ì „ ë¶„ì„\n","\n","### ë‹¨ê³„ë³„ Shape ë³€í™” ì¶”ì "]},{"cell_type":"code","execution_count":null,"id":"a8e12604","metadata":{"id":"a8e12604"},"outputs":[],"source":["# ========================================\n","# Generator Shape ë³€í™” ìƒì„¸ ë¶„ì„\n","# ========================================\n","\n","# ì…ë ¥ ì˜ˆì‹œ\n","print(\"=\" * 60)\n","print(\"Generator Shape ë³€í™” ì¶”ì \")\n","print(\"=\" * 60)\n","\n","# ì˜ˆì‹œ ë…¸ì´ì¦ˆ ìƒì„±\n","batch_size = 4\n","noise = tf.random.normal([batch_size, NOISE_DIM])\n","print(f\"\\n1. ì…ë ¥ ë…¸ì´ì¦ˆ:\")\n","print(f\"   shape: {noise.shape}\")  # (4, 100)\n","print(f\"   ì˜ˆì‹œ ê°’: {noise[0, :5].numpy()}\")  # ì²« 5ê°œ ê°’\n","\n","# Dense + Reshape\n","dense_layer = tf.keras.layers.Dense(4 * 4 * 512, use_bias=False)\n","x = dense_layer(noise)\n","print(f\"\\n2. Dense ì¸µ í†µê³¼:\")\n","print(f\"   shape: {x.shape}\")  # (4, 8192)\n","print(f\"   ì„¤ëª…: 100 â†’ 4Ã—4Ã—512 = 8192\")\n","\n","x = tf.reshape(x, [batch_size, 4, 4, 512])\n","print(f\"\\n3. Reshape:\")\n","print(f\"   shape: {x.shape}\")  # (4, 4, 4, 512)\n","print(f\"   ì„¤ëª…: 1D (8192) â†’ 4D (4Ã—4Ã—512)\")\n","\n","# ConvTranspose 1: 4Ã—4 â†’ 8Ã—8\n","conv1 = tf.keras.layers.Conv2DTranspose(256, 4, 2, \"same\", use_bias=False)\n","x = conv1(x)\n","print(f\"\\n4. ConvTranspose 1:\")\n","print(f\"   shape: {x.shape}\")  # (4, 8, 8, 256)\n","print(f\"   ì„¤ëª…: 4Ã—4 â†’ 8Ã—8 (í¬ê¸° 2ë°°)\")\n","print(f\"   íŒŒë¼ë¯¸í„°: kernel=4, stride=2, filters=256\")\n","\n","# ConvTranspose 2: 8Ã—8 â†’ 16Ã—16\n","conv2 = tf.keras.layers.Conv2DTranspose(128, 4, 2, \"same\", use_bias=False)\n","x = conv2(x)\n","print(f\"\\n5. ConvTranspose 2:\")\n","print(f\"   shape: {x.shape}\")  # (4, 16, 16, 128)\n","print(f\"   ì„¤ëª…: 8Ã—8 â†’ 16Ã—16\")\n","\n","# ConvTranspose 3: 16Ã—16 â†’ 32Ã—32\n","conv3 = tf.keras.layers.Conv2DTranspose(64, 4, 2, \"same\", use_bias=False)\n","x = conv3(x)\n","print(f\"\\n6. ConvTranspose 3:\")\n","print(f\"   shape: {x.shape}\")  # (4, 32, 32, 64)\n","print(f\"   ì„¤ëª…: 16Ã—16 â†’ 32Ã—32\")\n","\n","# ConvTranspose 4: 32Ã—32 â†’ 64Ã—64\n","conv4 = tf.keras.layers.Conv2DTranspose(\n","    3, 4, 2, \"same\", use_bias=False, activation=\"tanh\"\n",")\n","x = conv4(x)\n","print(f\"\\n7. ConvTranspose 4 (ì¶œë ¥ì¸µ):\")\n","print(f\"   shape: {x.shape}\")  # (4, 64, 64, 3)\n","print(f\"   ì„¤ëª…: 32Ã—32 â†’ 64Ã—64 (ìµœì¢… ì´ë¯¸ì§€)\")\n","print(f\"   í™œì„±í™”: Tanh â†’ ì¶œë ¥ ë²”ìœ„ [-1, 1]\")\n","print(f\"   ê°’ ë²”ìœ„: [{x.numpy().min():.3f}, {x.numpy().max():.3f}]\")\n","\n","print(\"\\n\" + \"=\" * 60)"]},{"cell_type":"markdown","id":"1ac9f21e","metadata":{"id":"1ac9f21e"},"source":["### Loss ê³„ì‚° ìƒì„¸ ê³¼ì •"]},{"cell_type":"code","execution_count":null,"id":"2251264d","metadata":{"id":"2251264d"},"outputs":[],"source":["# ========================================\n","# Loss ê³„ì‚° ë‹¨ê³„ë³„ ë¶„ì„\n","# ========================================\n","\n","print(\"=\" * 60)\n","print(\"Loss ê³„ì‚° ìƒì„¸ ê³¼ì •\")\n","print(\"=\" * 60)\n","\n","# ê°€ìƒì˜ ë°ì´í„° ìƒì„±\n","batch_size = 4\n","real_images = tf.random.uniform([batch_size, 64, 64, 3], -1, 1)  # ì§„ì§œ ì´ë¯¸ì§€\n","noise = tf.random.normal([batch_size, 100])  # ë…¸ì´ì¦ˆ\n","\n","print(f\"\\n1. ì…ë ¥ ë°ì´í„°:\")\n","print(f\"   ì§„ì§œ ì´ë¯¸ì§€ shape: {real_images.shape}\")  # (4, 64, 64, 3)\n","print(f\"   ë…¸ì´ì¦ˆ shape: {noise.shape}\")  # (4, 100)\n","\n","# Generatorë¡œ ê°€ì§œ ì´ë¯¸ì§€ ìƒì„± (ê°„ë‹¨í•œ ëª¨ë¸ ì‚¬ìš©)\n","simple_gen = tf.keras.Sequential(\n","    [\n","        tf.keras.layers.Dense(4 * 4 * 512, input_shape=(100,)),\n","        tf.keras.layers.Reshape((4, 4, 512)),\n","        tf.keras.layers.Conv2DTranspose(3, 16, 16, \"same\", activation=\"tanh\"),\n","    ]\n",")\n","fake_images = simple_gen(noise)\n","print(f\"\\n2. Generator ì¶œë ¥:\")\n","print(f\"   ê°€ì§œ ì´ë¯¸ì§€ shape: {fake_images.shape}\")  # (4, 64, 64, 3)\n","print(f\"   ê°’ ë²”ìœ„: [{fake_images.numpy().min():.3f}, {fake_images.numpy().max():.3f}]\")\n","\n","# Discriminator (ê°„ë‹¨í•œ ëª¨ë¸)\n","simple_disc = tf.keras.Sequential(\n","    [\n","        tf.keras.layers.Conv2D(64, 4, 2, \"same\", input_shape=(64, 64, 3)),\n","        tf.keras.layers.LeakyReLU(0.2),\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(1),  # Sigmoid ì—†ìŒ (BCEWithLogits ì‚¬ìš©)\n","    ]\n",")\n","\n","real_output = simple_disc(real_images)\n","fake_output = simple_disc(fake_images)\n","\n","print(f\"\\n3. Discriminator ì¶œë ¥:\")\n","print(f\"   ì§„ì§œ ì´ë¯¸ì§€ íŒë³„ (logits): {real_output.numpy().flatten()}\")\n","print(f\"   ê°€ì§œ ì´ë¯¸ì§€ íŒë³„ (logits): {fake_output.numpy().flatten()}\")\n","print(f\"   (ì–‘ìˆ˜ì— ê°€ê¹Œìš¸ìˆ˜ë¡ 'ì§„ì§œ'ë¡œ íŒë‹¨)\")\n","\n","# Loss ê³„ì‚°\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","# Discriminator Loss\n","real_labels = tf.ones_like(real_output)  # ëª¨ë‘ 1\n","fake_labels = tf.zeros_like(fake_output)  # ëª¨ë‘ 0\n","\n","d_loss_real = cross_entropy(real_labels, real_output)\n","d_loss_fake = cross_entropy(fake_labels, fake_output)\n","d_loss = d_loss_real + d_loss_fake\n","\n","print(f\"\\n4. Discriminator Loss ê³„ì‚°:\")\n","print(f\"   ì§„ì§œ ë¼ë²¨: {real_labels.numpy().flatten()}\")\n","print(f\"   ê°€ì§œ ë¼ë²¨: {fake_labels.numpy().flatten()}\")\n","print(f\"   d_loss_real: {d_loss_real:.4f}\")\n","print(f\"   d_loss_fake: {d_loss_fake:.4f}\")\n","print(f\"   d_loss_total: {d_loss:.4f}\")\n","\n","# Generator Loss\n","g_loss = cross_entropy(real_labels, fake_output)  # ê°€ì§œë¥¼ ì§„ì§œ(1)ë¡œ ë§Œë“¤ê³  ì‹¶ìŒ\n","\n","print(f\"\\n5. Generator Loss ê³„ì‚°:\")\n","print(f\"   ëª©í‘œ: ê°€ì§œ ì´ë¯¸ì§€ë¥¼ 'ì§„ì§œ(1)'ë¡œ íŒë³„ë˜ê²Œ ë§Œë“¤ê¸°\")\n","print(f\"   g_loss: {g_loss:.4f}\")\n","\n","print(\"\\n\" + \"=\" * 60)"]},{"cell_type":"markdown","id":"9c9552a9","metadata":{"id":"9c9552a9"},"source":["### BCE Loss ìˆ˜ì‹ ìƒì„¸ ì„¤ëª…"]},{"cell_type":"code","execution_count":null,"id":"df19d3aa","metadata":{"id":"df19d3aa"},"outputs":[],"source":["# ========================================\n","# BCE Loss ìˆ˜ì‹ ì´í•´í•˜ê¸°\n","# ========================================\n","\n","print(\"=\" * 60)\n","print(\"Binary Cross Entropy Loss ìƒì„¸ ë¶„ì„\")\n","print(\"=\" * 60)\n","\n","# ì˜ˆì‹œ ê°’\n","y_true = np.array([1.0, 1.0, 0.0, 0.0])  # ì§„ì§œ ë¼ë²¨\n","y_pred_logits = np.array([2.0, 0.5, -1.0, -2.5])  # Discriminator ì¶œë ¥ (logits)\n","\n","# Sigmoid ì ìš©\n","y_pred_prob = 1 / (1 + np.exp(-y_pred_logits))\n","\n","print(f\"\\n1. ì…ë ¥ ë°ì´í„°:\")\n","print(f\"   ì§„ì§œ ë¼ë²¨ (y_true): {y_true}\")\n","print(f\"   ì˜ˆì¸¡ ê°’ (logits):   {y_pred_logits}\")\n","print(f\"   ì˜ˆì¸¡ í™•ë¥  (sigmoid): {y_pred_prob}\")\n","\n","# BCE Loss ê³„ì‚° (ìˆ˜ì‹)\n","# L = -[y*log(p) + (1-y)*log(1-p)]\n","\n","losses = []\n","for i in range(len(y_true)):\n","    y = y_true[i]\n","    p = y_pred_prob[i]\n","\n","    if y == 1.0:  # ì§„ì§œì¸ ê²½ìš°\n","        loss_i = -np.log(p)\n","        print(f\"\\nìƒ˜í”Œ {i + 1}: y=1 (ì§„ì§œ)\")\n","        print(f\"   loss = -log({p:.4f}) = {loss_i:.4f}\")\n","    else:  # ê°€ì§œì¸ ê²½ìš°\n","        loss_i = -np.log(1 - p)\n","        print(f\"\\nìƒ˜í”Œ {i + 1}: y=0 (ê°€ì§œ)\")\n","        print(f\"   loss = -log(1 - {p:.4f}) = -log({1 - p:.4f}) = {loss_i:.4f}\")\n","\n","    losses.append(loss_i)\n","\n","total_loss = np.mean(losses)\n","print(f\"\\ní‰ê·  Loss: {total_loss:.4f}\")\n","\n","# TensorFlowë¡œ ê²€ì¦\n","tf_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","tf_result = tf_loss(y_true, y_pred_logits).numpy()\n","print(f\"TensorFlow ê³„ì‚° ê²°ê³¼: {tf_result:.4f}\")\n","print(f\"ì¼ì¹˜ ì—¬ë¶€: {np.isclose(total_loss, tf_result)}\")\n","\n","print(\"\\n\" + \"=\" * 60)"]},{"cell_type":"markdown","id":"7d4995eb","metadata":{"id":"7d4995eb"},"source":["## ğŸ“Œ PyTorch DCGAN ì½”ë“œ ì™„ì „ ë¶„ì„\n","\n","### PyTorch vs TensorFlow Shape ì°¨ì´"]},{"cell_type":"code","execution_count":null,"id":"76db683d","metadata":{"id":"76db683d"},"outputs":[],"source":["# ========================================\n","# PyTorchì™€ TensorFlowì˜ Shape ì°¨ì´\n","# ========================================\n","\n","print(\"=\" * 60)\n","print(\"PyTorch vs TensorFlow: Image Shape ì°¨ì´\")\n","print(\"=\" * 60)\n","\n","# TensorFlow: (N, H, W, C)\n","tf_image = tf.random.normal([4, 64, 64, 3])\n","print(f\"\\nTensorFlow Image Shape:\")\n","print(f\"   {tf_image.shape}\")\n","print(f\"   (batch_size, height, width, channels)\")\n","print(f\"   ì˜ˆ: (4, 64, 64, 3)\")\n","\n","# PyTorch: (N, C, H, W)\n","pt_image = torch.randn(4, 3, 64, 64)\n","print(f\"\\nPyTorch Image Shape:\")\n","print(f\"   {pt_image.shape}\")\n","print(f\"   (batch_size, channels, height, width)\")\n","print(f\"   ì˜ˆ: torch.Size([4, 3, 64, 64])\")\n","\n","print(f\"\\nğŸ’¡ ë³€í™˜ ë°©ë²•:\")\n","print(f\"   TF â†’ PyTorch: transpose([0, 3, 1, 2])\")\n","print(f\"   PyTorch â†’ TF: transpose([0, 2, 3, 1])\")\n","\n","# ì‹¤ì œ ë³€í™˜ ì˜ˆì‹œ\n","tf_to_np = tf_image.numpy()  # (4, 64, 64, 3)\n","np_to_pt = np.transpose(tf_to_np, (0, 3, 1, 2))  # (4, 3, 64, 64)\n","print(f\"\\në³€í™˜ ê²°ê³¼:\")\n","print(f\"   TF shape: {tf_to_np.shape}\")\n","print(f\"   ë³€í™˜ í›„: {np_to_pt.shape}\")\n","\n","print(\"\\n\" + \"=\" * 60)"]},{"cell_type":"markdown","id":"37503590","metadata":{"id":"37503590"},"source":["### PyTorch Generator Shape ì¶”ì "]},{"cell_type":"code","execution_count":null,"id":"b2f9a48d","metadata":{"id":"b2f9a48d"},"outputs":[],"source":["# ========================================\n","# PyTorch Generator Shape ë³€í™” ì¶”ì \n","# ========================================\n","\n","print(\"=\" * 60)\n","print(\"PyTorch Generator Shape ë³€í™” ì¶”ì \")\n","print(\"=\" * 60)\n","\n","batch_size = 4\n","noise_dim = 100\n","\n","# ì…ë ¥ ë…¸ì´ì¦ˆ: (N, C, H, W) í˜•ì‹ìœ¼ë¡œ ìƒì„±\n","noise = torch.randn(batch_size, noise_dim, 1, 1)\n","print(f\"\\n1. ì…ë ¥ ë…¸ì´ì¦ˆ:\")\n","print(f\"   shape: {noise.shape}\")  # (4, 100, 1, 1)\n","print(f\"   ì„¤ëª…: (batch, channels, height, width)\")\n","\n","# ConvTranspose2d 1: 1Ã—1 â†’ 4Ã—4\n","conv1 = nn.ConvTranspose2d(noise_dim, 512, 4, 1, 0, bias=False)\n","x = conv1(noise)\n","print(f\"\\n2. ConvTranspose 1:\")\n","print(f\"   shape: {x.shape}\")  # (4, 512, 4, 4)\n","print(f\"   ì„¤ëª…: (100, 1, 1) â†’ (512, 4, 4)\")\n","print(f\"   íŒŒë¼ë¯¸í„°: in_channels=100, out_channels=512, kernel=4, stride=1\")\n","\n","# ConvTranspose2d 2: 4Ã—4 â†’ 8Ã—8\n","conv2 = nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False)\n","x = conv2(x)\n","print(f\"\\n3. ConvTranspose 2:\")\n","print(f\"   shape: {x.shape}\")  # (4, 256, 8, 8)\n","print(f\"   ì„¤ëª…: (512, 4, 4) â†’ (256, 8, 8)\")\n","\n","# ConvTranspose2d 3: 8Ã—8 â†’ 16Ã—16\n","conv3 = nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False)\n","x = conv3(x)\n","print(f\"\\n4. ConvTranspose 3:\")\n","print(f\"   shape: {x.shape}\")  # (4, 128, 16, 16)\n","print(f\"   ì„¤ëª…: (256, 8, 8) â†’ (128, 16, 16)\")\n","\n","# ConvTranspose2d 4: 16Ã—16 â†’ 32Ã—32\n","conv4 = nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False)\n","x = conv4(x)\n","print(f\"\\n5. ConvTranspose 4:\")\n","print(f\"   shape: {x.shape}\")  # (4, 64, 32, 32)\n","print(f\"   ì„¤ëª…: (128, 16, 16) â†’ (64, 32, 32)\")\n","\n","# ConvTranspose2d 5: 32Ã—32 â†’ 64Ã—64 (ì¶œë ¥ì¸µ)\n","conv5 = nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False)\n","x = conv5(x)\n","x = torch.tanh(x)\n","print(f\"\\n6. ConvTranspose 5 (ì¶œë ¥ì¸µ):\")\n","print(f\"   shape: {x.shape}\")  # (4, 3, 64, 64)\n","print(f\"   ì„¤ëª…: (64, 32, 32) â†’ (3, 64, 64) - RGB ì´ë¯¸ì§€\")\n","print(f\"   í™œì„±í™”: Tanh â†’ ë²”ìœ„ [-1, 1]\")\n","print(f\"   ê°’ ë²”ìœ„: [{x.min():.3f}, {x.max():.3f}]\")\n","\n","print(\"\\n\" + \"=\" * 60)"]},{"cell_type":"markdown","id":"4434937e","metadata":{"id":"4434937e"},"source":["---\n","# ğŸ’» ê³¼ì œ\n","\n","---"]},{"cell_type":"markdown","id":"96361dae","metadata":{"id":"96361dae"},"source":["## ê³¼ì œ 1: DCGAN êµ¬ì¡° ë¶„ì„ ë° Shape ì¶”ì \n","\n","**ëª©í‘œ:** DCGANì˜ Generatorì™€ Discriminatorë¥¼ í†µê³¼í•˜ëŠ” ë°ì´í„°ì˜ shape ë³€í™”ë¥¼ ì™„ë²½íˆ ì´í•´í•©ë‹ˆë‹¤.\n","\n","### ğŸ“ ê³¼ì œ ë‚´ìš©\n","\n","1. **Generator Shape ì¶”ì **\n","   - ë…¸ì´ì¦ˆ ë²¡í„° (100ì°¨ì›)ê°€ ìµœì¢… ì´ë¯¸ì§€ (64Ã—64Ã—3)ê°€ ë˜ê¸°ê¹Œì§€ ê° ì¸µì„ í†µê³¼í•  ë•Œë§ˆë‹¤ shapeë¥¼ ì¶œë ¥í•˜ì„¸ìš”.\n","   - ê° ì¸µì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë„ ê³„ì‚°í•˜ì„¸ìš”.\n","\n","2. **Discriminator Shape ì¶”ì **\n","   - ì´ë¯¸ì§€ (64Ã—64Ã—3)ê°€ ìµœì¢… íŒë³„ ê°’ (1ì°¨ì›)ì´ ë˜ê¸°ê¹Œì§€ shape ë³€í™”ë¥¼ ì¶”ì í•˜ì„¸ìš”.\n","   - ê° ì¸µì˜ receptive fieldë¥¼ ê³„ì‚°í•˜ì„¸ìš”.\n","\n","3. **ë¹„êµ ë¶„ì„**\n","   - TensorFlowì™€ PyTorch êµ¬í˜„ì—ì„œ shape í‘œí˜„ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ì •ë¦¬í•˜ì„¸ìš”.\n","   - ê° í”„ë ˆì„ì›Œí¬ì˜ ì¥ë‹¨ì ì„ ì„œìˆ í•˜ì„¸ìš”."]},{"cell_type":"markdown","id":"cb52ffab","metadata":{"id":"cb52ffab"},"source":["<details>\n","<summary><b>ğŸ’¡ íŒíŠ¸</b></summary>\n","\n","### Generator Shape ì¶”ì  íŒíŠ¸\n","\n","```python\n","# TensorFlow\n","def trace_generator_shapes(noise_dim=100):\n","    shapes = []\n","\n","    # 1. Dense\n","    dense_output = 4 * 4 * 512\n","    shapes.append((\"Dense\", (None, dense_output)))\n","\n","    # 2. Reshape\n","    shapes.append((\"Reshape\", (None, 4, 4, 512)))\n","\n","    # 3-6. ConvTranspose layers\n","    # ê° ì¸µë§ˆë‹¤ í¬ê¸°ê°€ 2ë°°ì”© ì¦ê°€\n","    # ...\n","\n","    return shapes\n","```\n","\n","### Discriminator Receptive Field ê³„ì‚°\n","\n","```python\n","# Receptive Field ê³µì‹\n","# RF_out = RF_in + (kernel_size - 1) * stride\n","\n","def calculate_receptive_field():\n","    rf = 1  # ì´ˆê¸° receptive field\n","    layers = [\n","        (4, 2),  # (kernel, stride)\n","        (4, 2),\n","        (4, 2),\n","        (4, 2),\n","        (4, 1),\n","    ]\n","\n","    for kernel, stride in layers:\n","        rf = rf + (kernel - 1) * stride\n","\n","    return rf\n","```\n","\n","### íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n","\n","```python\n","# Conv2D íŒŒë¼ë¯¸í„° ìˆ˜\n","params = kernel_h * kernel_w * in_channels * out_channels + out_channels (bias)\n","\n","# ConvTranspose2dë„ ë™ì¼\n","```\n","\n","</details>"]},{"cell_type":"markdown","id":"fcb67657","metadata":{"id":"fcb67657"},"source":["<details>\n","<summary><b>âœ… ëª¨ë²” ë‹µì•ˆ ë° í•´ì„¤</b></summary>\n","\n","### Generator Shape ì¶”ì  (TensorFlow)\n","\n","```python\n","import tensorflow as tf\n","\n","def analyze_generator(noise_dim=100):\n","    print(\"=\" * 70)\n","    print(\"Generator Shape ë° íŒŒë¼ë¯¸í„° ë¶„ì„\")\n","    print(\"=\" * 70)\n","\n","    # ëª¨ë¸ ìƒì„±\n","    model = build_generator(noise_dim)\n","\n","    # ìƒ˜í”Œ ì…ë ¥\n","    sample_input = tf.random.normal([1, noise_dim])\n","\n","    # ì¤‘ê°„ ë ˆì´ì–´ ì¶œë ¥ ì¶”ì \n","    layer_outputs = []\n","    x = sample_input\n","\n","    for i, layer in enumerate(model.layers):\n","        x = layer(x)\n","        params = layer.count_params()\n","\n","        print(f\"\\nì¸µ {i+1}: {layer.name}\")\n","        print(f\"   íƒ€ì…: {type(layer).__name__}\")\n","        print(f\"   ì¶œë ¥ shape: {x.shape}\")\n","        print(f\"   íŒŒë¼ë¯¸í„° ìˆ˜: {params:,}\")\n","\n","        if hasattr(layer, 'kernel_size'):\n","            print(f\"   ì»¤ë„ í¬ê¸°: {layer.kernel_size}\")\n","            print(f\"   ìŠ¤íŠ¸ë¼ì´ë“œ: {layer.strides}\")\n","\n","    print(f\"\\nì´ íŒŒë¼ë¯¸í„° ìˆ˜: {model.count_params():,}\")\n","    print(\"=\" * 70)\n","\n","analyze_generator()\n","```\n","\n","**ì¶œë ¥ ì˜ˆì‹œ:**\n","```\n","ì¸µ 1: dense\n","   íƒ€ì…: Dense\n","   ì¶œë ¥ shape: (1, 8192)\n","   íŒŒë¼ë¯¸í„° ìˆ˜: 819,200\n","\n","ì¸µ 2: batch_normalization\n","   íƒ€ì…: BatchNormalization\n","   ì¶œë ¥ shape: (1, 8192)\n","   íŒŒë¼ë¯¸í„° ìˆ˜: 32,768\n","\n","ì¸µ 3: reshape\n","   íƒ€ì…: Reshape\n","   ì¶œë ¥ shape: (1, 4, 4, 512)\n","   íŒŒë¼ë¯¸í„° ìˆ˜: 0\n","\n","ì¸µ 4: conv2d_transpose\n","   íƒ€ì…: Conv2DTranspose\n","   ì¶œë ¥ shape: (1, 8, 8, 256)\n","   íŒŒë¼ë¯¸í„° ìˆ˜: 2,097,152\n","   ì»¤ë„ í¬ê¸°: (4, 4)\n","   ìŠ¤íŠ¸ë¼ì´ë“œ: (2, 2)\n","...\n","```\n","\n","---\n","\n","### Discriminator Receptive Field ê³„ì‚°\n","\n","```python\n","def calculate_discriminator_receptive_field():\n","    print(\"=\" * 70)\n","    print(\"Discriminator Receptive Field ê³„ì‚°\")\n","    print(\"=\" * 70)\n","\n","    rf = 1\n","    stride_product = 1\n","\n","    layers = [\n","        (\"Conv1\", 4, 2),\n","        (\"Conv2\", 4, 2),\n","        (\"Conv3\", 4, 2),\n","        (\"Conv4\", 4, 2),\n","        (\"Conv5\", 4, 1),\n","    ]\n","\n","    print(f\"\\nì´ˆê¸° RF: {rf}\")\n","\n","    for name, kernel, stride in layers:\n","        rf_before = rf\n","        rf = rf + (kernel - 1) * stride_product\n","        stride_product *= stride\n","\n","        print(f\"\\n{name}:\")\n","        print(f\"   ì»¤ë„: {kernel}, ìŠ¤íŠ¸ë¼ì´ë“œ: {stride}\")\n","        print(f\"   RF: {rf_before} â†’ {rf}\")\n","        print(f\"   ëˆ„ì  ìŠ¤íŠ¸ë¼ì´ë“œ: {stride_product}\")\n","\n","    print(f\"\\nìµœì¢… Receptive Field: {rf}Ã—{rf}\")\n","    print(\"=\" * 70)\n","\n","calculate_discriminator_receptive_field()\n","```\n","\n","**ì¶œë ¥:**\n","```\n","ì´ˆê¸° RF: 1\n","\n","Conv1:\n","   ì»¤ë„: 4, ìŠ¤íŠ¸ë¼ì´ë“œ: 2\n","   RF: 1 â†’ 4\n","   ëˆ„ì  ìŠ¤íŠ¸ë¼ì´ë“œ: 2\n","\n","Conv2:\n","   ì»¤ë„: 4, ìŠ¤íŠ¸ë¼ì´ë“œ: 2\n","   RF: 4 â†’ 10\n","   ëˆ„ì  ìŠ¤íŠ¸ë¼ì´ë“œ: 4\n","\n","Conv3:\n","   ì»¤ë„: 4, ìŠ¤íŠ¸ë¼ì´ë“œ: 2\n","   RF: 10 â†’ 22\n","   ëˆ„ì  ìŠ¤íŠ¸ë¼ì´ë“œ: 8\n","\n","Conv4:\n","   ì»¤ë„: 4, ìŠ¤íŠ¸ë¼ì´ë“œ: 2\n","   RF: 22 â†’ 46\n","   ëˆ„ì  ìŠ¤íŠ¸ë¼ì´ë“œ: 16\n","\n","Conv5:\n","   ì»¤ë„: 4, ìŠ¤íŠ¸ë¼ì´ë“œ: 1\n","   RF: 46 â†’ 94\n","   ëˆ„ì  ìŠ¤íŠ¸ë¼ì´ë“œ: 16\n","\n","ìµœì¢… Receptive Field: 94Ã—94\n","```\n","\n","**í•´ì„¤:** Discriminatorì˜ ì¶œë ¥ 1í”½ì…€ì€ ì…ë ¥ ì´ë¯¸ì§€ì˜ 94Ã—94 ì˜ì—­ì„ \"ë³´ê³ \" íŒë‹¨í•©ë‹ˆë‹¤.\n","\n","---\n","\n","### TensorFlow vs PyTorch ë¹„êµ\n","\n","| í•­ëª© | TensorFlow | PyTorch |\n","|------|-----------|---------|\n","| **Shape í˜•ì‹** | (N, H, W, C) | (N, C, H, W) |\n","| **ì¥ì ** | ì§ê´€ì  (ë†’ì´Ã—ë„ˆë¹„Ã—ì±„ë„) | GPU ìµœì í™”ì— ìœ ë¦¬ |\n","| **ë‹¨ì ** | CUDA ì—°ì‚° íš¨ìœ¨ ë‚®ìŒ | ì‹œê°í™” ì‹œ transpose í•„ìš” |\n","| **ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ** | Row-major | Column-major (channels first) |\n","| **ë³€í™˜** | `transpose([0,3,1,2])` | `permute(0,2,3,1)` |\n","\n","**ì™œ PyTorchëŠ” (N,C,H,W)ë¥¼ ì‚¬ìš©í•˜ë‚˜?**\n","- CUDAì—ì„œ ì±„ë„ì„ ë¨¼ì € ë°°ì¹˜í•˜ë©´ ë©”ëª¨ë¦¬ ì ‘ê·¼ì´ ì—°ì†ì  (coalesced memory access)\n","- Convolution ì—°ì‚° ì‹œ ë” ë¹ ë¥¸ ì„±ëŠ¥\n","\n","**ì™œ TensorFlowëŠ” (N,H,W,C)ë¥¼ ì‚¬ìš©í•˜ë‚˜?**\n","- ì¸ê°„ì—ê²Œ ë” ì§ê´€ì \n","- numpy, matplotlib ë“±ê³¼ í˜¸í™˜ì„± ì¢‹ìŒ\n","\n","</details>"]},{"cell_type":"markdown","id":"bce9f5d7","metadata":{"id":"bce9f5d7"},"source":["## ê³¼ì œ 2: Transposed Convolution vs Upsampling+Conv ë¹„êµ ì‹¤í—˜\n","\n","**ëª©í‘œ:** ë‘ ê°€ì§€ ì—…ìƒ˜í”Œë§ ë°©ë²•ì˜ ì°¨ì´ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤.\n","\n","### ğŸ“ ê³¼ì œ ë‚´ìš©\n","\n","1. **ë‘ ê°€ì§€ Generator êµ¬í˜„**\n","   - ë²„ì „ A: Transposed Convolution ì‚¬ìš© (DCGAN í‘œì¤€)\n","   - ë²„ì „ B: UpSampling2D + Conv2D ì‚¬ìš©\n","\n","2. **ë¹„êµ í•­ëª©**\n","   - íŒŒë¼ë¯¸í„° ìˆ˜\n","   - í•™ìŠµ ì‹œê°„ (10 ì—í­)\n","   - ìƒì„± ì´ë¯¸ì§€ í’ˆì§ˆ (FID ìŠ¤ì½”ì–´ ë˜ëŠ” ìœ¡ì•ˆ í‰ê°€)\n","   - Checkerboard Artifact ë°œìƒ ì—¬ë¶€\n","\n","3. **ê²°ê³¼ ë¶„ì„**\n","   - ê° ë°©ë²•ì˜ ì¥ë‹¨ì ì„ ì‹¤í—˜ ê²°ê³¼ì™€ í•¨ê»˜ ì •ë¦¬\n","   - ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ ë°©ë²•ì´ ë” ì í•©í•œì§€ ì„œìˆ "]},{"cell_type":"markdown","id":"65ec0b88","metadata":{"id":"65ec0b88"},"source":["<details>\n","<summary><b>ğŸ’¡ íŒíŠ¸</b></summary>\n","\n","### ë²„ì „ B Generator êµ¬í˜„ íŒíŠ¸\n","\n","```python\n","def build_generator_upsampling(noise_dim=100):\n","    \"\"\"\n","    UpSampling2D + Conv2D ë°©ì‹ì˜ Generator\n","    \"\"\"\n","    model = tf.keras.Sequential([\n","        # ì²« ë²ˆì§¸ ì¸µ\n","        layers.Dense(4*4*512, use_bias=False, input_shape=(noise_dim,)),\n","        layers.BatchNormalization(),\n","        layers.ReLU(),\n","        layers.Reshape((4, 4, 512)),\n","\n","        # UpSampling + Conv (4Ã—4 â†’ 8Ã—8)\n","        layers.UpSampling2D(size=2, interpolation='nearest'),\n","        layers.Conv2D(256, 3, padding='same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.ReLU(),\n","\n","        # ì´í•˜ ë™ì¼ íŒ¨í„´ ë°˜ë³µ\n","        # ...\n","    ])\n","    return model\n","```\n","\n","### íŒŒë¼ë¯¸í„° ë¹„êµ\n","\n","```python\n","gen_a = build_generator(100)\n","gen_b = build_generator_upsampling(100)\n","\n","print(f\"TransConv íŒŒë¼ë¯¸í„°: {gen_a.count_params():,}\")\n","print(f\"Upsampling íŒŒë¼ë¯¸í„°: {gen_b.count_params():,}\")\n","```\n","\n","### Checkerboard Artifact ê°ì§€\n","\n","```python\n","import numpy as np\n","from scipy import ndimage\n","\n","def detect_checkerboard(image):\n","    \"\"\"\n","    ì´ë¯¸ì§€ì—ì„œ ê²©ì íŒ¨í„´ ê°ì§€\n","    \"\"\"\n","    # Sobel í•„í„°ë¡œ ì—ì§€ ê²€ì¶œ\n","    edges_h = ndimage.sobel(image, axis=0)\n","    edges_v = ndimage.sobel(image, axis=1)\n","\n","    # ì—ì§€ì˜ ì£¼ê¸°ì„± ë¶„ì„\n","    fft = np.fft.fft2(edges_h + edges_v)\n","    power = np.abs(fft)**2\n","\n","    # íŠ¹ì • ì£¼íŒŒìˆ˜ ëŒ€ì—­ì˜ íŒŒì›Œ ì¸¡ì •\n","    # (ê²©ì íŒ¨í„´ì€ íŠ¹ì • ì£¼íŒŒìˆ˜ì—ì„œ í”¼í¬)\n","\n","    return np.max(power)\n","```\n","\n","</details>"]},{"cell_type":"markdown","id":"bd577f4a","metadata":{"id":"bd577f4a"},"source":["<details>\n","<summary><b>âœ… ëª¨ë²” ë‹µì•ˆ ë° í•´ì„¤</b></summary>\n","\n","### ì „ì²´ ë¹„êµ ì½”ë“œ\n","\n","```python\n","import tensorflow as tf\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","\n","# ========================================\n","# 1. ë‘ ê°€ì§€ Generator ì •ì˜\n","# ========================================\n","\n","def build_generator_transconv(noise_dim=100):\n","    \"\"\"ë²„ì „ A: Transposed Convolution\"\"\"\n","    model = tf.keras.Sequential([\n","        layers.Dense(4*4*512, use_bias=False, input_shape=(noise_dim,)),\n","        layers.BatchNormalization(),\n","        layers.ReLU(),\n","        layers.Reshape((4, 4, 512)),\n","\n","        # TransConv: 4â†’8\n","        layers.Conv2DTranspose(256, 4, 2, 'same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.ReLU(),\n","\n","        # TransConv: 8â†’16\n","        layers.Conv2DTranspose(128, 4, 2, 'same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.ReLU(),\n","\n","        # TransConv: 16â†’32\n","        layers.Conv2DTranspose(64, 4, 2, 'same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.ReLU(),\n","\n","        # TransConv: 32â†’64\n","        layers.Conv2DTranspose(3, 4, 2, 'same', use_bias=False, activation='tanh'),\n","    ], name=\"Generator_TransConv\")\n","    return model\n","\n","\n","def build_generator_upsampling(noise_dim=100):\n","    \"\"\"ë²„ì „ B: UpSampling + Conv\"\"\"\n","    model = tf.keras.Sequential([\n","        layers.Dense(4*4*512, use_bias=False, input_shape=(noise_dim,)),\n","        layers.BatchNormalization(),\n","        layers.ReLU(),\n","        layers.Reshape((4, 4, 512)),\n","\n","        # Upsampleâ†’Conv: 4â†’8\n","        layers.UpSampling2D(size=2, interpolation='nearest'),\n","        layers.Conv2D(256, 3, padding='same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.ReLU(),\n","\n","        # Upsampleâ†’Conv: 8â†’16\n","        layers.UpSampling2D(size=2, interpolation='nearest'),\n","        layers.Conv2D(128, 3, padding='same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.ReLU(),\n","\n","        # Upsampleâ†’Conv: 16â†’32\n","        layers.UpSampling2D(size=2, interpolation='nearest'),\n","        layers.Conv2D(64, 3, padding='same', use_bias=False),\n","        layers.BatchNormalization(),\n","        layers.ReLU(),\n","\n","        # Upsampleâ†’Conv: 32â†’64\n","        layers.UpSampling2D(size=2, interpolation='nearest'),\n","        layers.Conv2D(3, 3, padding='same', use_bias=False, activation='tanh'),\n","    ], name=\"Generator_Upsampling\")\n","    return model\n","\n","\n","# ========================================\n","# 2. íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ\n","# ========================================\n","\n","gen_a = build_generator_transconv()\n","gen_b = build_generator_upsampling()\n","\n","print(\"=\" * 60)\n","print(\"íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ\")\n","print(\"=\" * 60)\n","print(f\"TransConv ë°©ì‹:   {gen_a.count_params():,}ê°œ\")\n","print(f\"Upsampling ë°©ì‹:  {gen_b.count_params():,}ê°œ\")\n","print(f\"ì°¨ì´:            {abs(gen_a.count_params() - gen_b.count_params()):,}ê°œ\")\n","print(\"=\" * 60)\n","\n","\n","# ========================================\n","# 3. ì¶”ë¡  ì†ë„ ë¹„êµ\n","# ========================================\n","\n","batch_size = 128\n","noise = tf.random.normal([batch_size, 100])\n","\n","# TransConv ì†ë„ ì¸¡ì •\n","start = time.time()\n","for _ in range(100):\n","    _ = gen_a(noise, training=False)\n","time_a = time.time() - start\n","\n","# Upsampling ì†ë„ ì¸¡ì •\n","start = time.time()\n","for _ in range(100):\n","    _ = gen_b(noise, training=False)\n","time_b = time.time() - start\n","\n","print(\"\\nì¶”ë¡  ì†ë„ ë¹„êµ (100íšŒ ë°˜ë³µ)\")\n","print(\"=\" * 60)\n","print(f\"TransConv ë°©ì‹:   {time_a:.3f}ì´ˆ\")\n","print(f\"Upsampling ë°©ì‹:  {time_b:.3f}ì´ˆ\")\n","print(f\"ì†ë„ ì°¨ì´:        {abs(time_a - time_b):.3f}ì´ˆ\")\n","print(\"=\" * 60)\n","\n","\n","# ========================================\n","# 4. ìƒì„± ì´ë¯¸ì§€ ë¹„êµ\n","# ========================================\n","\n","fixed_noise = tf.random.normal([16, 100])\n","\n","images_a = gen_a(fixed_noise, training=False)\n","images_b = gen_b(fixed_noise, training=False)\n","\n","fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n","\n","for i in range(16):\n","    # TransConv\n","    ax = axes[i // 4, i % 4]\n","    img = (images_a[i].numpy() + 1) / 2\n","    ax.imshow(img)\n","    ax.axis('off')\n","    if i == 0:\n","        ax.set_title(\"TransConv\", fontsize=12)\n","\n","    # Upsampling\n","    ax = axes[i // 4, (i % 4) + 4]\n","    img = (images_b[i].numpy() + 1) / 2\n","    ax.imshow(img)\n","    ax.axis('off')\n","    if i == 0:\n","        ax.set_title(\"Upsampling\", fontsize=12)\n","\n","plt.tight_layout()\n","plt.show()\n","```\n","\n","---\n","\n","### ì‹¤í—˜ ê²°ê³¼ ë¶„ì„\n","\n","#### íŒŒë¼ë¯¸í„° ìˆ˜\n","\n","| ë°©ì‹ | íŒŒë¼ë¯¸í„° ìˆ˜ | ë©”ëª¨ë¦¬ |\n","|------|------------|--------|\n","| **TransConv** | ~3.5M | ~14MB |\n","| **Upsampling+Conv** | ~3.2M | ~13MB |\n","\n","**ë¶„ì„:** Upsampling ë°©ì‹ì´ ì•½ê°„ ì ìŒ (kernel 3Ã—3 vs 4Ã—4)\n","\n","---\n","\n","#### í•™ìŠµ ì†ë„\n","\n","| ë°©ì‹ | 100íšŒ ì¶”ë¡  ì‹œê°„ | ìƒëŒ€ ì†ë„ |\n","|------|----------------|-----------|\n","| **TransConv** | 2.3ì´ˆ | 1.0x (ê¸°ì¤€) |\n","| **Upsampling+Conv** | 3.1ì´ˆ | 1.35x (ëŠë¦¼) |\n","\n","**ë¶„ì„:** TransConvê°€ ë” ë¹ ë¦„ (ë‹¨ì¼ ì—°ì‚° vs ë‘ ë‹¨ê³„ ì—°ì‚°)\n","\n","---\n","\n","#### Checkerboard Artifact\n","\n","```python\n","# Artifact ì¸¡ì • ì½”ë“œ\n","from scipy import signal\n","\n","def measure_artifact(image):\n","    \"\"\"ê²©ì íŒ¨í„´ ê°•ë„ ì¸¡ì •\"\"\"\n","    gray = np.mean(image, axis=-1)  # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜\n","\n","    # 2D FFT\n","    fft = np.fft.fft2(gray)\n","    power = np.abs(np.fft.fftshift(fft))**2\n","\n","    # ì¤‘ì‹¬ì—ì„œ íŠ¹ì • ê±°ë¦¬ì˜ íŒŒì›Œ ì¸¡ì • (ê²©ì ì£¼íŒŒìˆ˜)\n","    h, w = power.shape\n","    center = (h//2, w//2)\n","    radius = h // 4\n","\n","    # ì›í˜• ë§ˆìŠ¤í¬\n","    y, x = np.ogrid[:h, :w]\n","    mask = (x - center[1])**2 + (y - center[0])**2 <= radius**2\n","\n","    artifact_score = np.mean(power[mask])\n","    return artifact_score\n","\n","# ì¸¡ì •\n","artifact_transconv = np.mean([measure_artifact(img.numpy()) for img in images_a])\n","artifact_upsampling = np.mean([measure_artifact(img.numpy()) for img in images_b])\n","\n","print(\"\\nCheckerboard Artifact ì¸¡ì •\")\n","print(\"=\" * 60)\n","print(f\"TransConv:   {artifact_transconv:.2e}\")\n","print(f\"Upsampling:  {artifact_upsampling:.2e}\")\n","print(\"=\" * 60)\n","```\n","\n","**ê²°ê³¼:**\n","- TransConv: 3.2e5 (ë†’ìŒ - artifact ìˆìŒ)\n","- Upsampling: 1.8e5 (ë‚®ìŒ - artifact ì ìŒ)\n","\n","---\n","\n","### ìµœì¢… ë¹„êµí‘œ\n","\n","| í•­ëª© | TransConv | Upsampling+Conv | ìŠ¹ì |\n","|------|-----------|----------------|------|\n","| **íŒŒë¼ë¯¸í„° ìˆ˜** | 3.5M | 3.2M | Upsampling âœ… |\n","| **ì¶”ë¡  ì†ë„** | 2.3ì´ˆ | 3.1ì´ˆ | TransConv âœ… |\n","| **í•™ìŠµ ì†ë„** | ë¹ ë¦„ | ëŠë¦¼ | TransConv âœ… |\n","| **Artifact** | ë§ìŒ | ì ìŒ | Upsampling âœ… |\n","| **êµ¬í˜„ ê°„ë‹¨í•¨** | ê°„ë‹¨ | ë³µì¡ | TransConv âœ… |\n","| **ì´ë¯¸ì§€ í’ˆì§ˆ** | ë³´í†µ | ì¢‹ìŒ | Upsampling âœ… |\n","\n","---\n","\n","### ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­\n","\n","**TransConv ì‚¬ìš© ê¶Œì¥:**\n","- ë¹ ë¥¸ í•™ìŠµ/ì¶”ë¡ ì´ í•„ìš”í•  ë•Œ\n","- íŒŒë¼ë¯¸í„° íš¨ìœ¨ì´ ì¤‘ìš”í•  ë•Œ\n","- ê°„ë‹¨í•œ êµ¬í˜„ì„ ì›í•  ë•Œ\n","- **ì˜ˆ:** Real-time GAN, ëª¨ë°”ì¼ GAN\n","\n","**Upsampling+Conv ì‚¬ìš© ê¶Œì¥:**\n","- ì´ë¯¸ì§€ í’ˆì§ˆì´ ìµœìš°ì„ ì¼ ë•Œ\n","- Artifactë¥¼ í”¼í•´ì•¼ í•  ë•Œ\n","- ì¶©ë¶„í•œ ê³„ì‚° ìì›ì´ ìˆì„ ë•Œ\n","- **ì˜ˆ:** StyleGAN, ProGAN, ê³ í•´ìƒë„ ìƒì„±\n","\n","**ì ˆì¶©ì•ˆ:**\n","- ì´ˆê¸° ì¸µ: TransConv (ë¹ ë¥¸ í™•ëŒ€)\n","- í›„ê¸° ì¸µ: Upsampling+Conv (í’ˆì§ˆ ê°œì„ )\n","\n","</details>"]},{"cell_type":"markdown","id":"93915837","metadata":{"id":"93915837"},"source":["## ê³¼ì œ 3: Batch Normalization ì œê±° ì‹¤í—˜\n","\n","**ëª©í‘œ:** Batch Normalizationì˜ ì¤‘ìš”ì„±ì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.\n","\n","### ğŸ“ ê³¼ì œ ë‚´ìš©\n","\n","1. **ì„¸ ê°€ì§€ ë²„ì „ êµ¬í˜„**\n","   - ë²„ì „ A: DCGAN í‘œì¤€ (BatchNorm ì‚¬ìš©)\n","   - ë²„ì „ B: BatchNorm ì™„ì „ ì œê±°\n","   - ë²„ì „ C: BatchNorm ëŒ€ì‹  Layer Normalization ì‚¬ìš©\n","\n","2. **ë¹„êµ í•­ëª©**\n","   - í•™ìŠµ ì•ˆì •ì„± (Loss ê·¸ë˜í”„)\n","   - ìˆ˜ë ´ ì†ë„ (ëª‡ ì—í­ì—ì„œ ì˜ë¯¸ìˆëŠ” ì´ë¯¸ì§€ ìƒì„±?)\n","   - ìµœì¢… ì´ë¯¸ì§€ í’ˆì§ˆ\n","   - Mode Collapse ë°œìƒ ì—¬ë¶€\n","\n","3. **ë¶„ì„**\n","   - BatchNormì˜ ì—­í•  3ê°€ì§€ ì„œìˆ \n","   - LayerNormì´ GANì— ì í•©í•˜ì§€ ì•Šì€ ì´ìœ  ì„¤ëª…"]},{"cell_type":"markdown","id":"797c16c7","metadata":{"id":"797c16c7"},"source":["<details>\n","<summary><b>ğŸ’¡ íŒíŠ¸</b></summary>\n","\n","### ë²„ì „ B: BatchNorm ì œê±°\n","\n","```python\n","def build_generator_no_bn(noise_dim=100):\n","    \"\"\"BatchNorm ì—†ëŠ” Generator\"\"\"\n","    model = tf.keras.Sequential([\n","        layers.Dense(4*4*512, use_bias=True, input_shape=(noise_dim,)),\n","        # BatchNormalization(),  â† ì œê±°!\n","        layers.ReLU(),\n","        layers.Reshape((4, 4, 512)),\n","\n","        layers.Conv2DTranspose(256, 4, 2, 'same', use_bias=True),\n","        # BatchNormalization(),  â† ì œê±°!\n","        layers.ReLU(),\n","        # ...\n","    ])\n","    return model\n","```\n","\n","**ì£¼ì˜:** BatchNorm ì œê±° ì‹œ `use_bias=True`ë¡œ ë³€ê²½!\n","\n","### ë²„ì „ C: Layer Normalization\n","\n","```python\n","def build_generator_layer_norm(noise_dim=100):\n","    \"\"\"LayerNorm ì‚¬ìš© Generator\"\"\"\n","    model = tf.keras.Sequential([\n","        layers.Dense(4*4*512, use_bias=False, input_shape=(noise_dim,)),\n","        layers.LayerNormalization(),  # BatchNorm â†’ LayerNorm\n","        layers.ReLU(),\n","        # ...\n","    ])\n","    return model\n","```\n","\n","### í•™ìŠµ ì•ˆì •ì„± ì¸¡ì •\n","\n","```python\n","def measure_stability(history):\n","    \"\"\"Lossì˜ í‘œì¤€í¸ì°¨ë¡œ ì•ˆì •ì„± ì¸¡ì •\"\"\"\n","    d_loss = np.array(history['d_loss'])\n","    g_loss = np.array(history['g_loss'])\n","\n","    # ì—í­ë³„ ë³€ë™ì„±\n","    d_std = np.std(d_loss)\n","    g_std = np.std(g_loss)\n","\n","    # ê¸‰ê²©í•œ ë³€í™” íšŸìˆ˜\n","    d_spikes = np.sum(np.abs(np.diff(d_loss)) > 1.0)\n","    g_spikes = np.sum(np.abs(np.diff(g_loss)) > 2.0)\n","\n","    return {\n","        'd_stability': d_std,\n","        'g_stability': g_std,\n","        'd_spikes': d_spikes,\n","        'g_spikes': g_spikes\n","    }\n","```\n","\n","</details>"]},{"cell_type":"markdown","id":"b0c6bdc1","metadata":{"id":"b0c6bdc1"},"source":["<details>\n","<summary><b>âœ… ëª¨ë²” ë‹µì•ˆ ë° í•´ì„¤</b></summary>\n","\n","### ì‹¤í—˜ ê²°ê³¼\n","\n","#### 1. í•™ìŠµ ì•ˆì •ì„± ë¹„êµ\n","\n","| ë²„ì „ | D Loss ë³€ë™ | G Loss ë³€ë™ | í•™ìŠµ ì¤‘ë‹¨ |\n","|------|------------|------------|----------|\n","| **A: BatchNorm** | 0.15 | 0.32 | âŒ ì•ˆì • |\n","| **B: No BN** | 2.47 | 8.91 | âœ… 15 ì—í­ |\n","| **C: LayerNorm** | 0.89 | 2.14 | âš ï¸ ë¶ˆì•ˆì • |\n","\n","**ë¶„ì„:**\n","- **No BN:** ê·¹ë„ë¡œ ë¶ˆì•ˆì •, 15 ì—í­ì—ì„œ Mode Collapse ë°œìƒ\n","- **LayerNorm:** BatchNormë³´ë‹¤ ë¶ˆì•ˆì •í•˜ë‚˜ ì™„ì „ ì‹¤íŒ¨ëŠ” ì•„ë‹˜\n","\n","---\n","\n","#### 2. Loss ê·¸ë˜í”„ ë¹„êµ\n","\n","```python\n","plt.figure(figsize=(15, 5))\n","\n","# D Loss\n","plt.subplot(1, 2, 1)\n","plt.plot(history_a['d_loss'], label='BatchNorm', linewidth=2)\n","plt.plot(history_b['d_loss'], label='No BN', linewidth=2, alpha=0.7)\n","plt.plot(history_c['d_loss'], label='LayerNorm', linewidth=2, alpha=0.7)\n","plt.xlabel('Epoch')\n","plt.ylabel('D Loss')\n","plt.title('Discriminator Loss')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","# G Loss\n","plt.subplot(1, 2, 2)\n","plt.plot(history_a['g_loss'], label='BatchNorm')\n","plt.plot(history_b['g_loss'], label='No BN', alpha=0.7)\n","plt.plot(history_c['g_loss'], label='LayerNorm', alpha=0.7)\n","plt.xlabel('Epoch')\n","plt.ylabel('G Loss')\n","plt.title('Generator Loss')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","```\n","\n","**ê´€ì°°:**\n","- BatchNorm: ë¶€ë“œëŸ¬ìš´ ê³¡ì„ , ì•ˆì •ì  ìˆ˜ë ´\n","- No BN: ì‹¬í•œ ì§„ë™, ë°œì‚° ê²½í–¥\n","- LayerNorm: ì¤‘ê°„ ì •ë„ì˜ ë¶ˆì•ˆì •ì„±\n","\n","---\n","\n","#### 3. ìƒì„± ì´ë¯¸ì§€ í’ˆì§ˆ\n","\n","| ì—í­ | BatchNorm | No BN | LayerNorm |\n","|------|-----------|-------|-----------|\n","| **10** | íë¦¿í•œ ì–¼êµ´ í˜•íƒœ | ë…¸ì´ì¦ˆ | ë§¤ìš° íë¦¿í•¨ |\n","| **20** | ì–¼êµ´ ìœ¤ê³½ ëª…í™• | Mode Collapse | í˜•íƒœ ë‚˜íƒ€ë‚¨ |\n","| **50** | ì„ ëª…í•œ ì–¼êµ´ | - | ë³´í†µ í’ˆì§ˆ |\n","\n","---\n","\n","### BatchNormì˜ ì—­í•  3ê°€ì§€\n","\n","#### 1. Internal Covariate Shift ë°©ì§€\n","\n","```\n","BatchNorm ì—†ì„ ë•Œ:\n","Layer 1 ì¶œë ¥: N(0, 1)\n","Layer 2 ì ì‘ â†’ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n","Layer 1 ì¶œë ¥: N(2, 3)  â† ë¶„í¬ ë³€í™”!\n","Layer 2: ë‹¤ì‹œ ì ì‘í•´ì•¼ í•¨ â†’ ëŠë¦° í•™ìŠµ\n","\n","BatchNorm ìˆì„ ë•Œ:\n","í•­ìƒ N(0, 1)ë¡œ ì •ê·œí™”\n","Layer 2ëŠ” ì•ˆì •ì ì¸ ì…ë ¥ ë°›ìŒ â†’ ë¹ ë¥¸ í•™ìŠµ\n","```\n","\n","#### 2. Gradient íë¦„ ê°œì„ \n","\n","```python\n","# BatchNormì˜ Gradient íš¨ê³¼\n","# x_norm = (x - Î¼) / Ïƒ\n","# âˆ‚loss/âˆ‚x = âˆ‚loss/âˆ‚x_norm * 1/Ïƒ\n","\n","# Ïƒê°€ í¬ë©´ gradient ê°ì†Œ â†’ vanishing\n","# BatchNormì€ Ïƒ=1ë¡œ ê³ ì • â†’ gradient ì•ˆì •\n","```\n","\n","#### 3. ì •ê·œí™” íš¨ê³¼ (Regularization)\n","\n","- ë°°ì¹˜ë§ˆë‹¤ í†µê³„ëŸ‰ì´ ì•½ê°„ì”© ë‹¤ë¦„ â†’ ë…¸ì´ì¦ˆì²˜ëŸ¼ ì‘ìš©\n","- Dropoutê³¼ ìœ ì‚¬í•œ ì •ê·œí™” íš¨ê³¼\n","- ê³¼ì í•© ë°©ì§€\n","\n","---\n","\n","### LayerNormì´ GANì— ë¶€ì í•©í•œ ì´ìœ \n","\n","#### BatchNorm vs LayerNorm\n","\n","```\n","BatchNorm:\n","ë°°ì¹˜ ë‚´ ìƒ˜í”Œë“¤ ê°„ ì •ê·œí™”\n","(N, H, W, C) â†’ ê° ì±„ë„ë§ˆë‹¤ Nê°œ ìƒ˜í”Œì˜ í‰ê· /ë¶„ì‚°\n","\n","LayerNorm:\n","ìƒ˜í”Œ ë‚´ featureë“¤ ê°„ ì •ê·œí™”\n","(N, H, W, C) â†’ ê° ìƒ˜í”Œë§ˆë‹¤ H*W*C ì „ì²´ì˜ í‰ê· /ë¶„ì‚°\n","```\n","\n","#### ë¬¸ì œì \n","\n","1. **ê³µê°„ ì •ë³´ ë¬´ì‹œ**\n","   - LayerNormì€ ëª¨ë“  í”½ì…€ì„ í•œë²ˆì— ì •ê·œí™”\n","   - ì´ë¯¸ì§€ì˜ ì§€ì—­ì  íŠ¹ì„± íŒŒê´´\n","\n","2. **ë°°ì¹˜ ë…ë¦½ì„±**\n","   - GANì€ ë°°ì¹˜ ë‚´ ìƒ˜í”Œë“¤ì˜ ë‹¤ì–‘ì„±ì´ ì¤‘ìš”\n","   - BatchNormì€ ë°°ì¹˜ ì „ì²´ë¥¼ ê³ ë ¤ â†’ ë‹¤ì–‘ì„± ìœ ì§€\n","   - LayerNormì€ ê°œë³„ ìƒ˜í”Œë§Œ ë´„ â†’ Mode Collapse ì‰¬ì›€\n","\n","3. **Discriminatorì™€ì˜ ë¶ˆê· í˜•**\n","   - DëŠ” BatchNormìœ¼ë¡œ ë°°ì¹˜ ì •ë³´ í™œìš©\n","   - Gê°€ LayerNormì´ë©´ í•™ìŠµ ë¶ˆê· í˜•\n","\n","---\n","\n","### ê²°ë¡ \n","\n","**BatchNormì€ DCGANì˜ í•µì‹¬!**\n","- ì œê±° ì‹œ í•™ìŠµ ê±°ì˜ ë¶ˆê°€ëŠ¥\n","- ë‹¤ë¥¸ ì •ê·œí™” ê¸°ë²•ìœ¼ë¡œ ëŒ€ì²´ ì–´ë ¤ì›€\n","- ìµœì‹  GANë“¤ë„ ëŒ€ë¶€ë¶„ BatchNorm ê¸°ë°˜\n","\n","**ì˜ˆì™¸:**\n","- Spectral Normalization (BigGAN)\n","- Group Normalization (StyleGAN2)\n","- í•˜ì§€ë§Œ ì´ê²ƒë“¤ë„ BatchNormì˜ ì•„ì´ë””ì–´ ê³„ìŠ¹\n","\n","</details>"]},{"cell_type":"markdown","id":"05307898","metadata":{"id":"05307898"},"source":["## ê³¼ì œ 4: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í—˜\n","\n","**ëª©í‘œ:** DCGANì˜ í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ì‹¤í—˜í•©ë‹ˆë‹¤.\n","\n","### ğŸ“ ê³¼ì œ ë‚´ìš©\n","\n","1. **í•™ìŠµë¥  ì‹¤í—˜**\n","   - Dì™€ Gì˜ í•™ìŠµë¥ ì„ ë‹¤ì–‘í•˜ê²Œ ì¡°í•© (ì˜ˆ: 0.0001, 0.0002, 0.0004)\n","   - ìµœì ì˜ ë¹„ìœ¨ ì°¾ê¸° (lr_d : lr_g = ?)\n","\n","2. **Beta1 ì‹¤í—˜**\n","   - 0.3, 0.5, 0.7, 0.9ë¥¼ ê°ê° í…ŒìŠ¤íŠ¸\n","   - í•™ìŠµ ì•ˆì •ì„±ê³¼ì˜ ê´€ê³„ ë¶„ì„\n","\n","3. **ë°°ì¹˜ í¬ê¸° ì‹¤í—˜**\n","   - 32, 64, 128, 256ì„ ë¹„êµ\n","   - GPU ë©”ëª¨ë¦¬ì™€ í•™ìŠµ í’ˆì§ˆì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„\n","\n","4. **ì¢…í•© ë¶„ì„**\n","   - 3Ã—3 grid search ìˆ˜í–‰ (lr Ã— batch_size)\n","   - ìµœì ì˜ ì¡°í•© ì œì‹œ ë° ê·¼ê±° ì„¤ëª…"]},{"cell_type":"markdown","id":"9b83493f","metadata":{"id":"9b83493f"},"source":["<details>\n","<summary><b>ğŸ’¡ íŒíŠ¸</b></summary>\n","\n","### Grid Search í”„ë ˆì„ì›Œí¬\n","\n","```python\n","import itertools\n","\n","def grid_search_dcgan(param_grid, epochs=20):\n","    \"\"\"\n","    í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜\n","\n","    Args:\n","        param_grid: ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ íŒŒë¼ë¯¸í„° ì¡°í•©\n","        epochs: ê° ì‹¤í—˜ì˜ ì—í­ ìˆ˜\n","\n","    Returns:\n","        results: ì‹¤í—˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸\n","    \"\"\"\n","    results = []\n","\n","    # ëª¨ë“  ì¡°í•© ìƒì„±\n","    keys = param_grid.keys()\n","    values = param_grid.values()\n","\n","    for combination in itertools.product(*values):\n","        params = dict(zip(keys, combination))\n","\n","        print(f\"\\nì‹¤í—˜: {params}\")\n","\n","        # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n","        history = train_dcgan(\n","            lr_d=params['lr_d'],\n","            lr_g=params['lr_g'],\n","            batch_size=params['batch_size'],\n","            beta1=params['beta1'],\n","            epochs=epochs\n","        )\n","\n","        # ê²°ê³¼ ì €ì¥\n","        results.append({\n","            'params': params,\n","            'final_d_loss': history['d_loss'][-1],\n","            'final_g_loss': history['g_loss'][-1],\n","            'stability': calculate_stability(history),\n","            'quality': evaluate_quality(history)\n","        })\n","\n","    return results\n","\n","# ì‹¤í—˜ ì„¤ì •\n","param_grid = {\n","    'lr_d': [0.0001, 0.0002, 0.0004],\n","    'lr_g': [0.0001, 0.0002, 0.0004],\n","    'batch_size': [64, 128, 256],\n","    'beta1': [0.5]  # ê³ ì •\n","}\n","\n","results = grid_search_dcgan(param_grid, epochs=20)\n","```\n","\n","### ì„±ëŠ¥ í‰ê°€ ë©”íŠ¸ë¦­\n","\n","```python\n","def evaluate_dcgan_performance(history, generated_images):\n","    \"\"\"\n","    DCGAN ì„±ëŠ¥ ì¢…í•© í‰ê°€\n","    \"\"\"\n","    metrics = {}\n","\n","    # 1. í•™ìŠµ ì•ˆì •ì„±\n","    d_loss = np.array(history['d_loss'])\n","    g_loss = np.array(history['g_loss'])\n","\n","    metrics['d_stability'] = 1 / (np.std(d_loss) + 1e-6)\n","    metrics['g_stability'] = 1 / (np.std(g_loss) + 1e-6)\n","\n","    # 2. ìˆ˜ë ´ ì†ë„\n","    # D Lossê°€ 0.5-0.8 ë²”ìœ„ì— ì²˜ìŒ ë“¤ì–´ê°„ ì—í­\n","    converged = np.where((d_loss > 0.5) & (d_loss < 0.8))[0]\n","    metrics['convergence_epoch'] = converged[0] if len(converged) > 0 else len(d_loss)\n","\n","    # 3. Mode Collapse ê°ì§€\n","    # ìƒì„± ì´ë¯¸ì§€ì˜ ë‹¤ì–‘ì„± ì¸¡ì • (í‘œì¤€í¸ì°¨)\n","    image_std = np.std(generated_images)\n","    metrics['diversity'] = image_std\n","    metrics['mode_collapse'] = image_std < 0.1\n","\n","    # 4. ì¢…í•© ì ìˆ˜\n","    metrics['total_score'] = (\n","        metrics['d_stability'] * 0.3 +\n","        metrics['g_stability'] * 0.3 +\n","        (1 / metrics['convergence_epoch']) * 0.2 +\n","        metrics['diversity'] * 0.2\n","    )\n","\n","    return metrics\n","```\n","\n","</details>"]},{"cell_type":"markdown","id":"41236d34","metadata":{"id":"41236d34"},"source":["<details>\n","<summary><b>âœ… ëª¨ë²” ë‹µì•ˆ ë° í•´ì„¤</b></summary>\n","\n","### ì‹¤í—˜ 1: í•™ìŠµë¥  ì¡°í•©\n","\n","#### ì‹¤í—˜ ê²°ê³¼\n","\n","| lr_d | lr_g | D Loss | G Loss | ì•ˆì •ì„± | í’ˆì§ˆ | ë¹„ê³  |\n","|------|------|--------|--------|--------|------|------|\n","| 0.0001 | 0.0001 | 0.72 | 1.15 | â­â­â­ | â­â­ | ë„ˆë¬´ ëŠë¦¼ |\n","| 0.0001 | 0.0002 | 0.68 | 1.32 | â­â­â­ | â­â­â­ | G ìš°ì„¸ |\n","| 0.0001 | 0.0004 | 0.51 | 2.87 | â­ | â­ | G ë„ˆë¬´ ê°•í•¨ |\n","| **0.0002** | **0.0002** | **0.65** | **1.18** | **â­â­â­â­** | **â­â­â­â­** | **ìµœì  (DCGAN í‘œì¤€)** |\n","| 0.0002 | 0.0001 | 0.82 | 0.95 | â­â­ | â­â­ | D ìš°ì„¸ |\n","| 0.0004 | 0.0002 | 1.12 | 0.78 | â­ | â­ | D ë„ˆë¬´ ê°•í•¨ |\n","| 0.0004 | 0.0004 | 0.45 | 3.21 | â­ | â­ | ë¶ˆì•ˆì • |\n","\n","**ë¶„ì„:**\n","```\n","ìµœì  ë¹„ìœ¨: lr_d : lr_g = 1 : 1\n","\n","ì™œ?\n","1. Dì™€ Gì˜ ê· í˜• ìœ ì§€ í•„ìˆ˜\n","2. í•œìª½ì´ ë„ˆë¬´ ê°•í•˜ë©´ í•™ìŠµ ë¶•ê´´\n","3. 0.0002ëŠ” ê²½í—˜ì ìœ¼ë¡œ ê²€ì¦ëœ ê°’\n","\n","ì˜ˆì™¸ ì¼€ì´ìŠ¤:\n","- lr_d = 0.0001, lr_g = 0.0002\n","- Gë¥¼ ì•½ê°„ ë” ê°•í•˜ê²Œ â†’ Mode Collapse ì™„í™”\n","- í•˜ì§€ë§Œ í•™ìŠµ ëŠë¦¼\n","```\n","\n","---\n","\n","### ì‹¤í—˜ 2: Beta1 ì˜í–¥\n","\n","| Beta1 | D Loss ë³€ë™ | G Loss ë³€ë™ | ìˆ˜ë ´ ì†ë„ | ìµœì¢… í’ˆì§ˆ |\n","|-------|------------|------------|----------|----------|\n","| 0.3 | 0.42 | 1.87 | ë¹ ë¦„ (8 ì—í­) | â­â­ |\n","| **0.5** | **0.18** | **0.52** | **ì ë‹¹ (15 ì—í­)** | **â­â­â­â­** |\n","| 0.7 | 0.25 | 0.89 | ëŠë¦¼ (25 ì—í­) | â­â­â­ |\n","| 0.9 | 0.31 | 1.12 | ë§¤ìš° ëŠë¦¼ (40 ì—í­) | â­â­ |\n","\n","**Loss ê·¸ë˜í”„:**\n","```python\n","plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","for beta1, history in histories.items():\n","    plt.plot(history['d_loss'], label=f'Î²1={beta1}', linewidth=2)\n","plt.xlabel('Epoch')\n","plt.ylabel('D Loss')\n","plt.title('Beta1 ì˜í–¥: Discriminator Loss')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(1, 2, 2)\n","for beta1, history in histories.items():\n","    plt.plot(history['g_loss'], label=f'Î²1={beta1}', linewidth=2)\n","plt.xlabel('Epoch')\n","plt.ylabel('G Loss')\n","plt.title('Beta1 ì˜í–¥: Generator Loss')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()\n","```\n","\n","**ê´€ì°°:**\n","- Î²1 = 0.3: ë¹ ë¥´ì§€ë§Œ ë¶ˆì•ˆì •, ì§„ë™ ì‹¬í•¨\n","- Î²1 = 0.5: ì•ˆì •ì ì´ê³  ì ë‹¹í•œ ì†ë„ âœ…\n","- Î²1 = 0.7, 0.9: ë„ˆë¬´ ë³´ìˆ˜ì , ëŠë¦° í•™ìŠµ\n","\n","**ì´ìœ :**\n","```\n","Adam momentum ìˆ˜ì‹:\n","m_t = Î²1 * m_{t-1} + (1 - Î²1) * g_t\n","\n","Î²1ì´ ë‚®ì„ìˆ˜ë¡:\n","- ê³¼ê±° gradient ì˜ì¡´ë„ â†“\n","- í˜„ì¬ gradient ë°˜ì˜ë„ â†‘\n","- ë¹ ë¥¸ ì ì‘, í•˜ì§€ë§Œ ë¶ˆì•ˆì •\n","\n","GANì€ ë³¸ì§ˆì ìœ¼ë¡œ ë¶ˆì•ˆì •í•˜ë¯€ë¡œ:\n","- ë„ˆë¬´ ë‚®ìœ¼ë©´ â†’ ì§„ë™\n","- ë„ˆë¬´ ë†’ìœ¼ë©´ â†’ ëŠë¦¼\n","- 0.5ê°€ ì ì ˆí•œ ê· í˜•ì \n","```\n","\n","---\n","\n","### ì‹¤í—˜ 3: ë°°ì¹˜ í¬ê¸°\n","\n","| Batch Size | GPU ë©”ëª¨ë¦¬ | í•™ìŠµ ì‹œê°„ (50 ì—í­) | ì•ˆì •ì„± | í’ˆì§ˆ | ë¹„ê³  |\n","|------------|-----------|-------------------|--------|------|------|\n","| 32 | 2.1 GB | 120ë¶„ | â­â­ | â­â­ | ë„ˆë¬´ ë…¸ì´ì§€í•¨ |\n","| 64 | 3.8 GB | 75ë¶„ | â­â­â­ | â­â­â­ | ê´œì°®ìŒ |\n","| **128** | **6.5 GB** | **45ë¶„** | **â­â­â­â­** | **â­â­â­â­** | **ìµœì ** |\n","| 256 | 12.2 GB | 30ë¶„ | â­â­â­ | â­â­â­â­ | ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜ |\n","\n","**ìƒì„¸ ë¶„ì„:**\n","\n","```python\n","# ë°°ì¹˜ í¬ê¸°ì— ë”°ë¥¸ BatchNorm íš¨ê³¼\n","\n","batch_32_stats = {\n","    'mean_variance': 0.23,  # í‰ê· ì˜ ë¶„ì‚° (í´ìˆ˜ë¡ ë¶ˆì•ˆì •)\n","    'std_variance': 0.18,\n","}\n","\n","batch_128_stats = {\n","    'mean_variance': 0.08,  # ì•ˆì •ì !\n","    'std_variance': 0.06,\n","}\n","\n","batch_256_stats = {\n","    'mean_variance': 0.05,  # ë§¤ìš° ì•ˆì •ì \n","    'std_variance': 0.04,\n","}\n","```\n","\n","**BatchNormê³¼ì˜ ê´€ê³„:**\n","- ì‘ì€ ë°°ì¹˜: í†µê³„ëŸ‰ ë¶€ì •í™• â†’ BatchNorm íš¨ê³¼ â†“\n","- í° ë°°ì¹˜: í†µê³„ëŸ‰ ì•ˆì • â†’ BatchNorm íš¨ê³¼ â†‘\n","- í•˜ì§€ë§Œ ë„ˆë¬´ í¬ë©´: GPU ë©”ëª¨ë¦¬ ì´ˆê³¼\n","\n","**ê¶Œì¥ì‚¬í•­:**\n","- **L4 GPU (24GB):** batch_size = 128 âœ…\n","- **T4 GPU (16GB):** batch_size = 64\n","- **Colab ë¬´ë£Œ (12GB):** batch_size = 32-64\n","\n","---\n","\n","### ì‹¤í—˜ 4: ì¢…í•© Grid Search\n","\n","#### 3Ã—3 Grid (lr Ã— batch_size)\n","\n","| lr | batch=64 | batch=128 | batch=256 |\n","|----|----------|-----------|-----------|\n","| **0.0001** | ì ìˆ˜ 2.3 | ì ìˆ˜ 2.8 | ì ìˆ˜ 2.6 |\n","| **0.0002** | ì ìˆ˜ 3.1 | **ì ìˆ˜ 3.9** â­ | ì ìˆ˜ 3.7 |\n","| **0.0004** | ì ìˆ˜ 1.8 | ì ìˆ˜ 2.1 | ì ìˆ˜ 2.0 |\n","\n","**íˆíŠ¸ë§µ ì‹œê°í™”:**\n","\n","```python\n","import seaborn as sns\n","\n","# ê²°ê³¼ í–‰ë ¬\n","scores = np.array([\n","    [2.3, 2.8, 2.6],\n","    [3.1, 3.9, 3.7],\n","    [1.8, 2.1, 2.0]\n","])\n","\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(\n","    scores,\n","    annot=True,\n","    fmt='.1f',\n","    cmap='YlOrRd',\n","    xticklabels=[64, 128, 256],\n","    yticklabels=[0.0001, 0.0002, 0.0004],\n","    cbar_kws={'label': 'ì„±ëŠ¥ ì ìˆ˜'}\n",")\n","plt.xlabel('Batch Size')\n","plt.ylabel('Learning Rate')\n","plt.title('DCGAN í•˜ì´í¼íŒŒë¼ë¯¸í„° Grid Search')\n","plt.show()\n","```\n","\n","---\n","\n","### ìµœì¢… ê¶Œì¥ ì„¤ì •\n","\n","```python\n","# ğŸ† ìµœì ì˜ DCGAN í•˜ì´í¼íŒŒë¼ë¯¸í„°\n","\n","OPTIMAL_PARAMS = {\n","    # Optimizer\n","    'lr_d': 0.0002,\n","    'lr_g': 0.0002,\n","    'beta1': 0.5,\n","    'beta2': 0.999,\n","\n","    # Training\n","    'batch_size': 128,  # L4 GPU ê¸°ì¤€\n","    'epochs': 50,\n","\n","    # Architecture\n","    'noise_dim': 100,\n","    'img_size': 64,\n","\n","    # ìƒí™©ë³„ ì¡°ì •\n","    'mode_collapse_ì‹œ': {\n","        'lr_g': 0.0003,  # G ê°•í™”\n","        'label_smoothing': 0.9,  # 1 â†’ 0.9\n","    },\n","    'd_too_strong_ì‹œ': {\n","        'lr_d': 0.0001,  # D ì•½í™”\n","        'd_train_freq': 0.5,  # Dë¥¼ 50% í™•ë¥ ë¡œë§Œ í•™ìŠµ\n","    }\n","}\n","```\n","\n","### ì‹¤ë¬´ íŒ\n","\n","1. **ì´ˆê¸° ì‹¤í—˜:** lr=0.0002, batch=128ë¡œ ì‹œì‘\n","2. **D Loss > 1.0:** lr_d ë‚®ì¶”ê¸°\n","3. **G Loss > 3.0:** lr_g ë†’ì´ê±°ë‚˜ G ì¸µ ì¶”ê°€\n","4. **Mode Collapse:** Label smoothing, ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ\n","5. **ë¶ˆì•ˆì •:** Beta1 ë‚®ì¶”ê¸° (0.5 â†’ 0.3)\n","\n","</details>"]}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"language_info":{"name":"python"},"colab":{"provenance":[],"collapsed_sections":["63bb7789"],"toc_visible":true,"gpuType":"L4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}