{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda729d0",
   "metadata": {},
   "source": [
    "# 딥러닝 1일차 복습\n",
    "\n",
    "이번 복습에서는 1일차에 학습한 핵심 개념들을 다시 정리하고, 실습 문제를 통해 이해도를 점검합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 손실함수, 최적화함수, 활성화함수의 개념 정리\n",
    "- 정규화와 표준화의 차이점 이해\n",
    "- CNN의 출력 크기 계산 능력 향상\n",
    "- MLP와 CNN 모델 직접 구현\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c6186",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae60953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rc(\"font\", family=\"NanumGothic\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# 시드 고정\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# GPU 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b143e",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: 객관식 퀴즈\n",
    "\n",
    "먼저 1일차에 학습한 내용을 객관식 문제로 점검해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960f52d",
   "metadata": {},
   "source": [
    "## 퀴즈 1: 손실함수의 역할\n",
    "\n",
    "**문제:** 딥러닝에서 손실함수(Loss Function)의 주요 역할은 무엇인가요?\n",
    "\n",
    "1. 모델의 가중치를 초기화하는 역할\n",
    "2. 모델의 예측값과 실제값의 차이를 수치화하는 역할\n",
    "3. 활성화 함수를 선택하는 역할\n",
    "4. 학습률을 자동으로 조정하는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd6ac68",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>정답 및 풀이 보기</b></summary>\n",
    "\n",
    "**정답: 2번**\n",
    "\n",
    "**풀이:**\n",
    "- 손실함수는 모델의 **예측값(prediction)**과 **실제값(ground truth)** 간의 차이를 수치화합니다.\n",
    "- 이 손실값(loss value)을 최소화하는 방향으로 모델을 학습시킵니다.\n",
    "- 대표적인 손실함수:\n",
    "  - 회귀: MSE (Mean Squared Error), MAE (Mean Absolute Error)\n",
    "  - 분류: CrossEntropyLoss, BCELoss\n",
    "- 손실함수의 값이 작을수록 모델의 성능이 좋다고 판단할 수 있습니다.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a82f00",
   "metadata": {},
   "source": [
    "## 퀴즈 2: 경사하강법\n",
    "\n",
    "**문제:** 경사하강법(Gradient Descent)에서 학습률(Learning Rate)이 너무 클 때 발생할 수 있는 문제는?\n",
    "\n",
    "1. 학습 속도가 너무 느려진다\n",
    "2. 최적값을 지나쳐서 발산할 수 있다\n",
    "3. 가중치가 0으로 수렴한다\n",
    "4. 손실함수가 항상 증가한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1559a5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>정답 및 풀이 보기</b></summary>\n",
    "\n",
    "**정답: 2번**\n",
    "\n",
    "**풀이:**\n",
    "- 학습률이 너무 크면 경사를 따라 내려가는 **보폭이 너무 커져서** 최적값(minimum)을 지나칠 수 있습니다.\n",
    "- 이 경우 손실값이 진동하거나 발산(diverge)할 수 있습니다.\n",
    "- 반대로 학습률이 너무 작으면:\n",
    "  - 학습 속도가 매우 느려집니다\n",
    "  - Local minimum에 갇힐 가능성이 높아집니다\n",
    "- 적절한 학습률 선택이 중요합니다 (보통 0.001 ~ 0.01 범위에서 시작)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ab8ac",
   "metadata": {},
   "source": [
    "## 퀴즈 3: 활성화 함수\n",
    "\n",
    "**문제:** ReLU 활성화 함수의 수식은 무엇인가요?\n",
    "\n",
    "1. $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "2. $f(x) = \\max(0, x)$\n",
    "3. $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "4. $f(x) = x \\cdot \\sigma(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425c2d3b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>정답 및 풀이 보기</b></summary>\n",
    "\n",
    "**정답: 2번**\n",
    "\n",
    "**풀이:**\n",
    "- ReLU(Rectified Linear Unit)는 $f(x) = \\max(0, x)$로 정의됩니다.\n",
    "- 특징:\n",
    "  - x > 0일 때: f(x) = x (선형)\n",
    "  - x ≤ 0일 때: f(x) = 0\n",
    "- 장점:\n",
    "  - 계산이 간단하고 빠름\n",
    "  - Gradient vanishing 문제 완화\n",
    "- 단점:\n",
    "  - Dying ReLU 문제 (음수 영역에서 gradient가 0)\n",
    "- 보기의 다른 함수들:\n",
    "  - 1번: Sigmoid\n",
    "  - 3번: Tanh\n",
    "  - 4번: Swish\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c754d77",
   "metadata": {},
   "source": [
    "## 퀴즈 4: CNN의 특징\n",
    "\n",
    "**문제:** CNN(Convolutional Neural Network)에서 Pooling 레이어의 주요 목적은?\n",
    "\n",
    "1. 파라미터 수를 증가시켜 표현력을 높인다\n",
    "2. 특징 맵의 크기를 줄이고 중요한 특징을 추출한다\n",
    "3. 가중치를 학습 가능하게 만든다\n",
    "4. 과적합을 발생시켜 모델을 복잡하게 만든다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b8826",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>정답 및 풀이 보기</b></summary>\n",
    "\n",
    "**정답: 2번**\n",
    "\n",
    "**풀이:**\n",
    "- Pooling 레이어는 특징 맵(feature map)의 **공간적 크기를 줄이는** 역할을 합니다.\n",
    "- 주요 목적:\n",
    "  - **다운샘플링**: 특징 맵 크기 축소 → 계산량 감소\n",
    "  - **중요 특징 추출**: 가장 두드러진 특징을 보존\n",
    "  - **Translation Invariance**: 약간의 위치 변화에 강건함\n",
    "  - **과적합 방지**: 파라미터 수 감소\n",
    "- 종류:\n",
    "  - Max Pooling: 최댓값 선택 (가장 많이 사용)\n",
    "  - Average Pooling: 평균값 계산\n",
    "- 중요: Pooling 레이어는 **학습 가능한 파라미터가 없습니다**\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c27e2c",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: 서술형 문제 (용어 설명)\n",
    "\n",
    "핵심 개념들을 자신의 언어로 설명해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6238e",
   "metadata": {},
   "source": [
    "## 문제 1: 손실함수 (Loss Function)\n",
    "\n",
    "**문제:** 손실함수(Loss Function)가 무엇인지 설명하고, 회귀와 분류 문제에서 각각 어떤 손실함수를 주로 사용하는지 서술하세요.\n",
    "\n",
    "---\n",
    "\n",
    "### 여러분의 답안을 작성해보세요\n",
    "\n",
    "```\n",
    "[여기에 답안을 작성하세요]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4894e4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>힌트 보기</b></summary>\n",
    "\n",
    "- 손실함수의 **목적**은 무엇인가요?\n",
    "- 예측값과 실제값의 관계를 어떻게 표현하나요?\n",
    "- 회귀는 연속적인 값을, 분류는 범주를 예측합니다\n",
    "- MSE, MAE, CrossEntropy 등을 떠올려보세요\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f98afc4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>모범 답안 보기</b></summary>\n",
    "\n",
    "### 모범 답안\n",
    "\n",
    "**손실함수(Loss Function)**는 모델의 예측값과 실제 정답 간의 차이를 수치화하여 모델의 성능을 평가하는 함수입니다. 손실함수의 값이 작을수록 모델이 더 정확한 예측을 하고 있다는 의미이며, 학습 과정에서 이 손실값을 최소화하는 방향으로 모델의 가중치를 업데이트합니다.\n",
    "\n",
    "**회귀 문제**에서는 주로 다음의 손실함수를 사용합니다:\n",
    "- **MSE (Mean Squared Error)**: 예측값과 실제값의 차이를 제곱하여 평균을 구합니다. 큰 오차에 더 큰 패널티를 부여합니다.\n",
    "- **MAE (Mean Absolute Error)**: 예측값과 실제값의 절댓값 차이의 평균입니다. 이상치에 MSE보다 덜 민감합니다.\n",
    "\n",
    "**분류 문제**에서는 주로 다음의 손실함수를 사용합니다:\n",
    "- **CrossEntropyLoss**: 다중 클래스 분류에 사용하며, 예측 확률 분포와 실제 레이블 간의 교차 엔트로피를 계산합니다.\n",
    "- **BCELoss (Binary Cross Entropy)**: 이진 분류에 사용합니다.\n",
    "\n",
    "**핵심 키워드:**\n",
    "- 예측값과 실제값의 차이 수치화\n",
    "- 모델 성능 평가\n",
    "- 회귀: MSE, MAE\n",
    "- 분류: CrossEntropyLoss, BCELoss\n",
    "- 손실값 최소화 → 학습\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb758d6",
   "metadata": {},
   "source": [
    "## 문제 2: 최적화함수 (Optimizer)\n",
    "\n",
    "**문제:** 최적화함수(Optimizer)의 역할을 설명하고, SGD와 Adam의 차이점을 서술하세요.\n",
    "\n",
    "---\n",
    "\n",
    "### 여러분의 답안을 작성해보세요\n",
    "\n",
    "```\n",
    "[여기에 답안을 작성하세요]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631d04e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>힌트 보기</b></summary>\n",
    "\n",
    "- 최적화함수는 **가중치 업데이트**와 관련이 있습니다\n",
    "- 경사하강법(Gradient Descent)의 변형들입니다\n",
    "- SGD는 가장 기본적인 방법입니다\n",
    "- Adam은 **적응형 학습률**을 사용합니다\n",
    "- Momentum의 개념을 생각해보세요\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf902e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>모범 답안 보기</b></summary>\n",
    "\n",
    "### 모범 답안\n",
    "\n",
    "**최적화함수(Optimizer)**는 손실함수의 값을 최소화하기 위해 모델의 가중치를 업데이트하는 알고리즘입니다. 역전파를 통해 계산된 그래디언트(gradient)를 사용하여 가중치를 조정하며, 이 과정을 반복하여 모델이 최적의 성능을 내도록 학습시킵니다.\n",
    "\n",
    "**SGD (Stochastic Gradient Descent)**:\n",
    "- 가장 기본적인 최적화 알고리즘입니다.\n",
    "- 각 미니배치마다 그래디언트를 계산하여 가중치를 업데이트합니다.\n",
    "- 수식: $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta L$\n",
    "- 장점: 단순하고 메모리 효율적\n",
    "- 단점: 학습률이 고정되어 있고, 진동이 심할 수 있으며, 수렴 속도가 느릴 수 있습니다.\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)**:\n",
    "- 적응형 학습률을 사용하는 고급 최적화 알고리즘입니다.\n",
    "- Momentum과 RMSProp의 장점을 결합했습니다.\n",
    "- 각 파라미터마다 **다른 학습률**을 적용합니다.\n",
    "- 1차 모멘트(평균)와 2차 모멘트(분산)를 모두 추정합니다.\n",
    "- 장점: 빠른 수렴, 안정적인 학습, 하이퍼파라미터 튜닝이 덜 필요\n",
    "- 단점: 메모리 사용량이 SGD보다 많음\n",
    "\n",
    "**주요 차이점**:\n",
    "- SGD는 고정 학습률, Adam은 적응형 학습률\n",
    "- Adam이 일반적으로 더 빠르고 안정적으로 수렴\n",
    "- Adam은 각 파라미터별로 학습률을 자동 조정\n",
    "\n",
    "**핵심 키워드:**\n",
    "- 가중치 업데이트\n",
    "- 손실함수 최소화\n",
    "- 경사하강법\n",
    "- SGD: 고정 학습률, 단순\n",
    "- Adam: 적응형 학습률, Momentum + RMSProp\n",
    "- 그래디언트 기반 업데이트\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe87fc0",
   "metadata": {},
   "source": [
    "## 문제 3: 활성화함수 (Activation Function)\n",
    "\n",
    "**문제:** 활성화함수(Activation Function)의 필요성과 역할을 설명하고, ReLU, Sigmoid, Tanh 중 하나를 선택하여 그 특징을 서술하세요.\n",
    "\n",
    "---\n",
    "\n",
    "### 여러분의 답안을 작성해보세요\n",
    "\n",
    "```\n",
    "[여기에 답안을 작성하세요]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c682f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>힌트 보기</b></summary>\n",
    "\n",
    "- 활성화함수가 없으면 어떻게 될까요? (선형성)\n",
    "- **비선형성(non-linearity)**이 왜 중요한가요?\n",
    "- 각 활성화함수의 수식과 출력 범위를 생각해보세요\n",
    "- Gradient vanishing 문제를 고려해보세요\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942847ab",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>모범 답안 보기</b></summary>\n",
    "\n",
    "### 모범 답안\n",
    "\n",
    "**활성화함수(Activation Function)**는 신경망에 **비선형성(non-linearity)**을 추가하여 복잡한 패턴을 학습할 수 있게 만드는 함수입니다.\n",
    "\n",
    "**필요성:**\n",
    "- 활성화함수가 없으면 여러 층을 쌓아도 결국 **선형 변환의 조합**이 되어 단일 선형 레이어와 동일합니다.\n",
    "- 비선형 활성화함수를 사용해야 신경망이 XOR 문제와 같은 비선형 문제를 해결할 수 있습니다.\n",
    "- 각 뉴런의 출력을 조절하여 다음 층으로 전달할 정보를 결정합니다.\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**의 특징:\n",
    "- 수식: $f(x) = \\max(0, x)$\n",
    "- 출력 범위: $[0, \\infty)$\n",
    "- **장점:**\n",
    "  - 계산이 매우 간단하고 빠름\n",
    "  - Gradient vanishing 문제를 완화 (양수 영역에서 gradient가 1)\n",
    "  - 희소성(sparsity) 제공 (음수는 0으로)\n",
    "  - 실제로 가장 많이 사용되는 활성화함수\n",
    "- **단점:**\n",
    "  - Dying ReLU 문제: 음수 입력에 대해 gradient가 0이 되어 뉴런이 죽을 수 있음\n",
    "  - 출력이 0 이상으로 제한되어 평균이 0이 아님\n",
    "- **변형:** Leaky ReLU, PReLU, ELU 등이 Dying ReLU 문제를 해결하기 위해 제안됨\n",
    "\n",
    "**핵심 키워드:**\n",
    "- 비선형성 추가\n",
    "- 복잡한 패턴 학습\n",
    "- 선형 변환의 한계 극복\n",
    "- ReLU: max(0, x), 계산 효율적, Gradient vanishing 완화\n",
    "- Dying ReLU 문제\n",
    "- 가장 널리 사용되는 활성화함수\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce808a1c",
   "metadata": {},
   "source": [
    "## 문제 4: 정규화와 표준화\n",
    "\n",
    "**문제:** 정규화(Normalization)와 표준화(Standardization)의 차이점을 설명하고, 각각 어떤 상황에서 사용하는 것이 적합한지 서술하세요.\n",
    "\n",
    "---\n",
    "\n",
    "### 여러분의 답안을 작성해보세요\n",
    "\n",
    "```\n",
    "[여기에 답안을 작성하세요]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93407f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>힌트 보기</b></summary>\n",
    "\n",
    "- 정규화: 데이터를 **특정 범위**로 변환 (보통 0~1)\n",
    "- 표준화: **평균과 표준편차**를 사용하여 변환\n",
    "- Min-Max Scaling vs Z-score Normalization\n",
    "- 이상치(outlier)의 영향을 고려해보세요\n",
    "- 각 방법의 수식을 떠올려보세요\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90dfc2e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>모범 답안 보기</b></summary>\n",
    "\n",
    "### 모범 답안\n",
    "\n",
    "**정규화(Normalization)**와 **표준화(Standardization)**는 모두 데이터의 스케일을 조정하여 학습을 안정화하고 수렴 속도를 높이는 전처리 기법입니다.\n",
    "\n",
    "**정규화 (Normalization, Min-Max Scaling)**:\n",
    "- 데이터를 **특정 범위**(주로 [0, 1] 또는 [-1, 1])로 변환합니다.\n",
    "- 수식: $x' = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "- **특징:**\n",
    "  - 모든 값이 동일한 범위 내에 위치\n",
    "  - 데이터의 분포 형태는 변하지 않음\n",
    "  - 이상치에 민감함 (min, max가 이상치의 영향을 받음)\n",
    "- **사용 시기:**\n",
    "  - 데이터가 특정 범위에 제한되어야 할 때\n",
    "  - 이미지 데이터 (픽셀값 0~255 → 0~1)\n",
    "  - 신경망의 출력층에서 확률값이 필요할 때\n",
    "  - 이상치가 적고 균등 분포에 가까운 데이터\n",
    "\n",
    "**표준화 (Standardization, Z-score Normalization)**:\n",
    "- 데이터를 **평균 0, 표준편차 1**인 분포로 변환합니다.\n",
    "- 수식: $x' = \\frac{x - \\mu}{\\sigma}$\n",
    "- **특징:**\n",
    "  - 데이터를 정규분포 형태로 변환\n",
    "  - 이상치의 영향을 상대적으로 덜 받음\n",
    "  - 데이터가 특정 범위로 제한되지 않음\n",
    "- **사용 시기:**\n",
    "  - 데이터가 정규분포를 따르거나 정규분포에 가까울 때\n",
    "  - 이상치가 많은 데이터\n",
    "  - 대부분의 머신러닝 알고리즘 (SVM, 선형회귀 등)\n",
    "  - 딥러닝에서 입력 데이터 전처리 (일반적으로 더 선호됨)\n",
    "\n",
    "**차이점 요약:**\n",
    "| 구분 | 정규화 | 표준화 |\n",
    "|------|--------|--------|\n",
    "| 범위 | [0, 1] 또는 [-1, 1] | 제한 없음 |\n",
    "| 기준 | Min, Max | 평균, 표준편차 |\n",
    "| 이상치 민감도 | 높음 | 낮음 |\n",
    "| 분포 형태 | 유지 | 정규분포로 변환 |\n",
    "\n",
    "**핵심 키워드:**\n",
    "- 정규화: Min-Max Scaling, [0, 1] 범위, 이상치 민감\n",
    "- 표준화: Z-score, 평균 0 표준편차 1, 이상치 강건\n",
    "- 데이터 스케일 조정\n",
    "- 학습 안정화 및 수렴 속도 향상\n",
    "- 이미지는 정규화, 일반 데이터는 표준화\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f335ca",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: CNN 필터 크기 계산 문제\n",
    "\n",
    "CNN에서 출력 크기를 계산하는 능력은 매우 중요합니다. 다양한 상황에서 출력 크기를 계산해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ca0c4",
   "metadata": {},
   "source": [
    "## CNN 출력 크기 계산 공식\n",
    "\n",
    "**출력 크기 = $\\lfloor \\frac{입력크기 - 커널크기 + 2 \\times padding}{stride} \\rfloor + 1$**\n",
    "\n",
    "- **입력 크기** (Input size): 입력 feature map의 가로/세로 크기\n",
    "- **커널 크기** (Kernel size): 필터의 가로/세로 크기\n",
    "- **Padding**: 입력 주변에 추가되는 0의 개수\n",
    "- **Stride**: 필터가 이동하는 간격\n",
    "- $\\lfloor \\cdot \\rfloor$: 내림 (floor) 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d6464",
   "metadata": {},
   "source": [
    "## 문제 1: 기본 계산\n",
    "\n",
    "**입력:** 32×32 이미지\n",
    "**필터:** 5×5, stride=1, padding=0\n",
    "\n",
    "**출력 크기는?**\n",
    "\n",
    "---\n",
    "\n",
    "### 여러분의 답안:\n",
    "```\n",
    "[여기에 계산 과정과 답을 작성하세요]\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce5be5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>정답 보기</b></summary>\n",
    "\n",
    "### 계산 과정:\n",
    "\n",
    "공식: $\\lfloor \\frac{32 - 5 + 2 \\times 0}{1} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor \\frac{32 - 5 + 0}{1} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor \\frac{27}{1} \\rfloor + 1$\n",
    "\n",
    "= $27 + 1$\n",
    "\n",
    "= **28**\n",
    "\n",
    "### 정답: 28×28\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca8cc5",
   "metadata": {},
   "source": [
    "## 문제 2: Same Padding\n",
    "\n",
    "**입력:** 28×28 이미지\n",
    "**필터:** 3×3, stride=1, **same padding** (출력 크기가 입력과 같도록)\n",
    "\n",
    "**필요한 padding 크기와 출력 크기는?**\n",
    "\n",
    "---\n",
    "\n",
    "### 여러분의 답안:\n",
    "```\n",
    "[여기에 계산 과정과 답을 작성하세요]\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd42488",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>힌트 보기</b></summary>\n",
    "\n",
    "- Same padding은 출력 크기 = 입력 크기가 되도록 padding을 설정하는 것입니다.\n",
    "- stride=1일 때, padding = (kernel_size - 1) / 2 공식을 사용할 수 있습니다.\n",
    "- 3×3 커널이면 padding은?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a8d87",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>정답 보기</b></summary>\n",
    "\n",
    "### 계산 과정:\n",
    "\n",
    "**Same padding 공식 (stride=1):**\n",
    "\n",
    "$padding = \\frac{kernel\\_size - 1}{2} = \\frac{3 - 1}{2} = 1$\n",
    "\n",
    "**출력 크기 계산:**\n",
    "\n",
    "$\\lfloor \\frac{28 - 3 + 2 \\times 1}{1} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor \\frac{28 - 3 + 2}{1} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor \\frac{27}{1} \\rfloor + 1$\n",
    "\n",
    "= $28$\n",
    "\n",
    "### 정답: padding=1, 출력 크기 28×28\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c389bd4",
   "metadata": {},
   "source": [
    "## 문제 3: Stride가 있는 경우\n",
    "\n",
    "**입력:** 64×64 이미지\n",
    "**필터:** 3×3, **stride=2**, padding=1\n",
    "\n",
    "**출력 크기는?**\n",
    "\n",
    "---\n",
    "\n",
    "### 여러분의 답안:\n",
    "```\n",
    "[여기에 계산 과정과 답을 작성하세요]\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90605ee5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>정답 보기</b></summary>\n",
    "\n",
    "### 계산 과정:\n",
    "\n",
    "$\\lfloor \\frac{64 - 3 + 2 \\times 1}{2} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor \\frac{64 - 3 + 2}{2} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor \\frac{63}{2} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor 31.5 \\rfloor + 1$\n",
    "\n",
    "= $31 + 1$\n",
    "\n",
    "= **32**\n",
    "\n",
    "### 정답: 32×32\n",
    "\n",
    "**핵심:** Stride가 2이면 출력 크기가 대략 절반으로 줄어듭니다.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e923e",
   "metadata": {},
   "source": [
    "## 문제 4: Pooling과 결합\n",
    "\n",
    "**입력:** 224×224 이미지\n",
    "**연산 순서:**\n",
    "1. Conv: 7×7, stride=2, padding=3\n",
    "2. MaxPool: 3×3, stride=2, padding=1\n",
    "\n",
    "**최종 출력 크기는?**\n",
    "\n",
    "---\n",
    "\n",
    "### 여러분의 답안:\n",
    "```\n",
    "[여기에 계산 과정과 답을 작성하세요]\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbd359",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>정답 보기</b></summary>\n",
    "\n",
    "### 계산 과정:\n",
    "\n",
    "**1단계 - Conv 레이어 후:**\n",
    "\n",
    "$\\lfloor \\frac{224 - 7 + 2 \\times 3}{2} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor \\frac{224 - 7 + 6}{2} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor \\frac{223}{2} \\rfloor + 1$\n",
    "\n",
    "= $111 + 1 = 112$\n",
    "\n",
    "**2단계 - MaxPool 레이어 후:**\n",
    "\n",
    "$\\lfloor \\frac{112 - 3 + 2 \\times 1}{2} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor \\frac{112 - 3 + 2}{2} \\rfloor + 1$\n",
    "\n",
    "= $\\lfloor \\frac{111}{2} \\rfloor + 1$\n",
    "\n",
    "= $55 + 1 = 56$\n",
    "\n",
    "### 정답: 56×56\n",
    "\n",
    "**참고:** 이는 ResNet의 초기 레이어와 유사한 구조입니다.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba526e4c",
   "metadata": {},
   "source": [
    "## 문제 5: 복합 계산 (도전 문제)\n",
    "\n",
    "**입력:** 128×128×3 (RGB 이미지)\n",
    "**연산 순서:**\n",
    "1. Conv1: 3×3, 64 filters, stride=1, padding=1\n",
    "2. Conv2: 3×3, 64 filters, stride=1, padding=1\n",
    "3. MaxPool: 2×2, stride=2, padding=0\n",
    "4. Conv3: 3×3, 128 filters, stride=1, padding=1\n",
    "5. MaxPool: 2×2, stride=2, padding=0\n",
    "\n",
    "**각 단계별 출력 크기 (H×W×C)를 구하세요.**\n",
    "\n",
    "---\n",
    "\n",
    "### 여러분의 답안:\n",
    "```\n",
    "[여기에 계산 과정과 답을 작성하세요]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635b5fb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>힌트 보기</b></summary>\n",
    "\n",
    "- Same padding (padding=1, stride=1)은 크기를 유지합니다\n",
    "- MaxPool (2×2, stride=2)은 크기를 절반으로 줄입니다\n",
    "- 채널 수는 필터의 개수로 결정됩니다\n",
    "- 단계별로 차근차근 계산해보세요\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca51784",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "<details>\n",
    "<summary><b>정답 보기</b></summary>\n",
    "\n",
    "### 계산 과정:\n",
    "\n",
    "**입력:** 128×128×3\n",
    "\n",
    "**1단계 - Conv1 (3×3, 64 filters, stride=1, padding=1):**\n",
    "- H, W: $\\lfloor \\frac{128 - 3 + 2 \\times 1}{1} \\rfloor + 1 = \\lfloor \\frac{127}{1} \\rfloor + 1 = 128$\n",
    "- C: 64 (필터 개수)\n",
    "- **출력: 128×128×64**\n",
    "\n",
    "**2단계 - Conv2 (3×3, 64 filters, stride=1, padding=1):**\n",
    "- H, W: 동일하게 128 유지\n",
    "- C: 64\n",
    "- **출력: 128×128×64**\n",
    "\n",
    "**3단계 - MaxPool (2×2, stride=2, padding=0):**\n",
    "- H, W: $\\lfloor \\frac{128 - 2 + 0}{2} \\rfloor + 1 = \\lfloor \\frac{126}{2} \\rfloor + 1 = 63 + 1 = 64$\n",
    "- C: 64 (변화 없음)\n",
    "- **출력: 64×64×64**\n",
    "\n",
    "**4단계 - Conv3 (3×3, 128 filters, stride=1, padding=1):**\n",
    "- H, W: 64 유지\n",
    "- C: 128 (필터 개수)\n",
    "- **출력: 64×64×128**\n",
    "\n",
    "**5단계 - MaxPool (2×2, stride=2, padding=0):**\n",
    "- H, W: $\\lfloor \\frac{64 - 2 + 0}{2} \\rfloor + 1 = \\lfloor \\frac{62}{2} \\rfloor + 1 = 31 + 1 = 32$\n",
    "- C: 128 (변화 없음)\n",
    "- **최종 출력: 32×32×128**\n",
    "\n",
    "### 정답:\n",
    "- Conv1 출력: 128×128×64\n",
    "- Conv2 출력: 128×128×64\n",
    "- MaxPool1 출력: 64×64×64\n",
    "- Conv3 출력: 64×64×128\n",
    "- MaxPool2 출력: **32×32×128**\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbee3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 출력 크기를 자동으로 계산해주는 함수\n",
    "def calculate_conv_output_size(input_size, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Conv2d 레이어의 출력 크기를 계산합니다.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_size : int\n",
    "        입력 feature map의 크기 (H 또는 W)\n",
    "    kernel_size : int\n",
    "        필터의 크기\n",
    "    stride : int\n",
    "        stride 값\n",
    "    padding : int\n",
    "        padding 값\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int : 출력 크기\n",
    "    \"\"\"\n",
    "    output_size = ((input_size - kernel_size + 2 * padding) // stride) + 1\n",
    "    return output_size\n",
    "\n",
    "\n",
    "def calculate_pool_output_size(input_size, pool_size, stride=None, padding=0):\n",
    "    \"\"\"\n",
    "    Pooling 레이어의 출력 크기를 계산합니다.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_size : int\n",
    "        입력 feature map의 크기 (H 또는 W)\n",
    "    pool_size : int\n",
    "        pooling window 크기\n",
    "    stride : int or None\n",
    "        stride 값 (None이면 pool_size와 동일)\n",
    "    padding : int\n",
    "        padding 값\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int : 출력 크기\n",
    "    \"\"\"\n",
    "    if stride is None:\n",
    "        stride = pool_size\n",
    "    output_size = ((input_size - pool_size + 2 * padding) // stride) + 1\n",
    "    return output_size\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "print(\"=== CNN 출력 크기 계산 도구 ===\\n\")\n",
    "\n",
    "# 예시 1: 기본 계산\n",
    "input_size = 32\n",
    "output = calculate_conv_output_size(input_size, kernel_size=5, stride=1, padding=0)\n",
    "print(f\"문제 1 검증: {input_size}×{input_size} → {output}×{output}\")\n",
    "\n",
    "# 예시 2: Same padding\n",
    "input_size = 28\n",
    "output = calculate_conv_output_size(input_size, kernel_size=3, stride=1, padding=1)\n",
    "print(f\"문제 2 검증: {input_size}×{input_size} → {output}×{output}\")\n",
    "\n",
    "# 예시 3: Stride가 있는 경우\n",
    "input_size = 64\n",
    "output = calculate_conv_output_size(input_size, kernel_size=3, stride=2, padding=1)\n",
    "print(f\"문제 3 검증: {input_size}×{input_size} → {output}×{output}\")\n",
    "\n",
    "# 예시 4: Pooling과 결합\n",
    "input_size = 224\n",
    "output1 = calculate_conv_output_size(input_size, kernel_size=7, stride=2, padding=3)\n",
    "output2 = calculate_pool_output_size(output1, pool_size=3, stride=2, padding=1)\n",
    "print(\n",
    "    f\"문제 4 검증: {input_size}×{input_size} → {output1}×{output1} → {output2}×{output2}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26f656",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: 코드 구현 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3da6c3",
   "metadata": {},
   "source": [
    "## 문제 1: 가중합 → 활성화 함수 → 출력 구현\n",
    "\n",
    "**문제:** 신경망의 기본 동작인 \"가중합 계산 → 활성화 함수 적용\" 과정을 NumPy로 구현하세요.\n",
    "\n",
    "**요구사항:**\n",
    "1. 입력 벡터 x와 가중치 행렬 W, 편향 벡터 b를 받아 가중합(z = W·x + b)을 계산\n",
    "2. ReLU, Sigmoid, Tanh 중 하나의 활성화 함수를 선택하여 적용\n",
    "3. 최종 출력값을 반환\n",
    "\n",
    "**입력 예시:**\n",
    "- x = [1.0, 2.0, 3.0]\n",
    "- W = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n",
    "- b = [0.1, 0.2]\n",
    "- activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 여러분의 코드를 작성하세요\n",
    "\n",
    "\n",
    "def forward_pass(x, W, b, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    가중합 계산 후 활성화 함수를 적용합니다.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        입력 벡터 (shape: (n,))\n",
    "    W : array-like\n",
    "        가중치 행렬 (shape: (m, n))\n",
    "    b : array-like\n",
    "        편향 벡터 (shape: (m,))\n",
    "    activation : str\n",
    "        활성화 함수 ('relu', 'sigmoid', 'tanh')\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    output : ndarray\n",
    "        활성화 함수 적용 후 출력 (shape: (m,))\n",
    "    \"\"\"\n",
    "    # 여기에 코드를 작성하세요\n",
    "    pass\n",
    "\n",
    "\n",
    "# 테스트 코드\n",
    "if __name__ == \"__main__\":\n",
    "    # 예시 입력\n",
    "    x = np.array([1.0, 2.0, 3.0])\n",
    "    W = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
    "    b = np.array([0.1, 0.2])\n",
    "\n",
    "    # 각 활성화 함수로 테스트\n",
    "    for act in [\"relu\", \"sigmoid\", \"tanh\"]:\n",
    "        result = forward_pass(x, W, b, activation=act)\n",
    "        print(f\"{act.upper()} 출력: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e67374d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>힌트 보기</b></summary>\n",
    "\n",
    "### 힌트\n",
    "\n",
    "**1단계: 가중합 계산**\n",
    "```python\n",
    "z = np.dot(W, x) + b  # 또는 W @ x + b\n",
    "```\n",
    "\n",
    "**2단계: 활성화 함수**\n",
    "- ReLU: `np.maximum(0, z)`\n",
    "- Sigmoid: `1 / (1 + np.exp(-z))`\n",
    "- Tanh: `np.tanh(z)`\n",
    "\n",
    "**3단계: 조건문으로 분기**\n",
    "```python\n",
    "if activation == 'relu':\n",
    "    output = ...\n",
    "elif activation == 'sigmoid':\n",
    "    output = ...\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c669a3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>모범 답안 및 해설 보기</b></summary>\n",
    "\n",
    "### 모범 답안\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모범 답안 (접혀있음 - 위의 details 블록에서 확인)\n",
    "\n",
    "\n",
    "def forward_pass_solution(x, W, b, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    가중합 계산 후 활성화 함수를 적용합니다.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : array-like\n",
    "        입력 벡터 (shape: (n,))\n",
    "    W : array-like\n",
    "        가중치 행렬 (shape: (m, n))\n",
    "    b : array-like\n",
    "        편향 벡터 (shape: (m,))\n",
    "    activation : str\n",
    "        활성화 함수 ('relu', 'sigmoid', 'tanh')\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    output : ndarray\n",
    "        활성화 함수 적용 후 출력 (shape: (m,))\n",
    "    \"\"\"\n",
    "    # 1. 가중합 계산: z = W·x + b\n",
    "    z = np.dot(W, x) + b\n",
    "\n",
    "    # 2. 활성화 함수 적용\n",
    "    if activation == \"relu\":\n",
    "        # ReLU: max(0, x)\n",
    "        output = np.maximum(0, z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        # Sigmoid: 1 / (1 + e^(-x))\n",
    "        output = 1 / (1 + np.exp(-z))\n",
    "    elif activation == \"tanh\":\n",
    "        # Tanh: (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "        output = np.tanh(z)\n",
    "    else:\n",
    "        raise ValueError(f\"지원하지 않는 활성화 함수: {activation}\")\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# 상세 테스트\n",
    "print(\"=\" * 60)\n",
    "print(\"모범 답안 테스트\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "W = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
    "b = np.array([0.1, 0.2])\n",
    "\n",
    "print(f\"\\n입력 벡터 x: {x}\")\n",
    "print(f\"가중치 행렬 W:\\n{W}\")\n",
    "print(f\"편향 벡터 b: {b}\")\n",
    "\n",
    "# 가중합 계산 (중간 단계 확인)\n",
    "z = np.dot(W, x) + b\n",
    "print(f\"\\n가중합 z = W·x + b: {z}\")\n",
    "print(f\"  - W의 첫 번째 행: [0.1, 0.2, 0.3]\")\n",
    "print(f\"  - 0.1*1 + 0.2*2 + 0.3*3 + 0.1 = {0.1 * 1 + 0.2 * 2 + 0.3 * 3 + 0.1}\")\n",
    "print(f\"  - W의 두 번째 행: [0.4, 0.5, 0.6]\")\n",
    "print(f\"  - 0.4*1 + 0.5*2 + 0.6*3 + 0.2 = {0.4 * 1 + 0.5 * 2 + 0.6 * 3 + 0.2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"활성화 함수별 출력\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for act in [\"relu\", \"sigmoid\", \"tanh\"]:\n",
    "    result = forward_pass_solution(x, W, b, activation=act)\n",
    "    print(f\"\\n{act.upper()}:\")\n",
    "    print(f\"  출력: {result}\")\n",
    "\n",
    "    if act == \"relu\":\n",
    "        print(\n",
    "            f\"  설명: max(0, {z[0]:.2f}) = {result[0]:.4f}, max(0, {z[1]:.2f}) = {result[1]:.4f}\"\n",
    "        )\n",
    "    elif act == \"sigmoid\":\n",
    "        print(\n",
    "            f\"  설명: 1/(1+e^(-{z[0]:.2f})) = {result[0]:.4f}, 1/(1+e^(-{z[1]:.2f})) = {result[1]:.4f}\"\n",
    "        )\n",
    "    elif act == \"tanh\":\n",
    "        print(\n",
    "            f\"  설명: tanh({z[0]:.2f}) = {result[0]:.4f}, tanh({z[1]:.2f}) = {result[1]:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43ad55",
   "metadata": {},
   "source": [
    "### 해설\n",
    "\n",
    "**핵심 개념:**\n",
    "1. **가중합 계산**: 선형 변환을 수행합니다 (z = W·x + b)\n",
    "2. **활성화 함수**: 비선형성을 추가하여 복잡한 패턴을 학습 가능하게 합니다\n",
    "3. **forward pass**: 입력에서 출력으로 신호가 전달되는 과정\n",
    "\n",
    "**각 활성화 함수의 특징:**\n",
    "- **ReLU**: 계산이 빠르고 gradient vanishing 완화, 음수는 0으로\n",
    "- **Sigmoid**: 0~1 사이 값, 확률 해석 가능, gradient vanishing 문제\n",
    "- **Tanh**: -1~1 사이 값, Sigmoid보다 중심이 0에 가까워 학습에 유리\n",
    "\n",
    "**실무 팁:**\n",
    "- 은닉층: ReLU 계열 (ReLU, Leaky ReLU, ELU)\n",
    "- 이진 분류 출력층: Sigmoid\n",
    "- 다중 클래스 분류 출력층: Softmax\n",
    "- 회귀 출력층: 활성화 함수 없음 (선형)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e799a",
   "metadata": {},
   "source": [
    "---\n",
    "## 문제 2: 간단한 MLP 모델 구현 (MNIST)\n",
    "\n",
    "**문제:** MNIST 손글씨 숫자 분류를 위한 Multi-Layer Perceptron (MLP) 모델을 구현하세요.\n",
    "\n",
    "**모델 구조:**\n",
    "```\n",
    "Input (784) → FC1 (512, ReLU) → Dropout(0.2) → FC2 (256, ReLU) → Dropout(0.2) → Output (10)\n",
    "```\n",
    "\n",
    "**요구사항:**\n",
    "1. PyTorch의 nn.Module을 상속받는 MLP 클래스 작성\n",
    "2. `__init__` 메서드에서 레이어 정의\n",
    "3. `forward` 메서드에서 순전파 구현\n",
    "4. 제공된 베이스라인 코드를 완성하여 학습 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f11c0",
   "metadata": {},
   "source": [
    "### 베이스라인 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터 로드\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(\"./data\", train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"학습 데이터: {len(train_dataset)}개\")\n",
    "print(f\"테스트 데이터: {len(test_dataset)}개\")\n",
    "print(f\"이미지 크기: {train_dataset[0][0].shape}\")\n",
    "\n",
    "# 샘플 이미지 시각화\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = train_dataset[i]\n",
    "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b245c",
   "metadata": {},
   "source": [
    "### TODO: MLP 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f46f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 여러분의 코드를 작성하세요\n",
    "\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    간단한 Multi-Layer Perceptron 모델\n",
    "\n",
    "    Architecture:\n",
    "    - Input: 784 (28x28 flattened)\n",
    "    - FC1: 512 neurons + ReLU\n",
    "    - Dropout: 0.2\n",
    "    - FC2: 256 neurons + ReLU\n",
    "    - Dropout: 0.2\n",
    "    - Output: 10 classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # TODO: 레이어들을 정의하세요\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            입력 이미지 (batch_size, 1, 28, 28)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        output : torch.Tensor\n",
    "            클래스별 로짓 (batch_size, 10)\n",
    "        \"\"\"\n",
    "        # TODO: 순전파 과정을 구현하세요\n",
    "        pass\n",
    "\n",
    "\n",
    "# TODO: 학습 함수 구현\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    모델을 1 에폭 학습시킵니다.\n",
    "    \"\"\"\n",
    "    # TODO: 학습 코드를 작성하세요\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: 평가 함수 구현\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    모델을 평가합니다.\n",
    "    \"\"\"\n",
    "    # TODO: 평가 코드를 작성하세요\n",
    "    pass\n",
    "\n",
    "\n",
    "# 모델 생성 및 학습\n",
    "if __name__ == \"__main__\":\n",
    "    model = SimpleMLP().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # TODO: 학습 루프 작성\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2da771",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>힌트 보기</b></summary>\n",
    "\n",
    "### 힌트\n",
    "\n",
    "**모델 구조:**\n",
    "```python\n",
    "self.fc1 = nn.Linear(784, 512)\n",
    "self.dropout1 = nn.Dropout(0.2)\n",
    "self.fc2 = nn.Linear(512, 256)\n",
    "self.dropout2 = nn.Dropout(0.2)\n",
    "self.fc3 = nn.Linear(256, 10)\n",
    "```\n",
    "\n",
    "**Forward 함수:**\n",
    "```python\n",
    "x = x.view(x.size(0), -1)  # Flatten\n",
    "x = F.relu(self.fc1(x))\n",
    "x = self.dropout1(x)\n",
    "# ... 계속\n",
    "```\n",
    "\n",
    "**학습 루프:**\n",
    "```python\n",
    "model.train()\n",
    "for images, labels in train_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9c1bd",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>모범 답안 및 해설 보기</b></summary>\n",
    "\n",
    "### 모범 답안\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ad5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모범 답안\n",
    "\n",
    "\n",
    "class SimpleMLP_Solution(nn.Module):\n",
    "    \"\"\"\n",
    "    간단한 Multi-Layer Perceptron 모델\n",
    "\n",
    "    Architecture:\n",
    "    - Input: 784 (28x28 flattened)\n",
    "    - FC1: 512 neurons + ReLU\n",
    "    - Dropout: 0.2\n",
    "    - FC2: 256 neurons + ReLU\n",
    "    - Dropout: 0.2\n",
    "    - Output: 10 classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP_Solution, self).__init__()\n",
    "        # 레이어 정의\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)  # 입력층 → 은닉층1\n",
    "        self.dropout1 = nn.Dropout(0.2)  # Dropout 1\n",
    "        self.fc2 = nn.Linear(512, 256)  # 은닉층1 → 은닉층2\n",
    "        self.dropout2 = nn.Dropout(0.2)  # Dropout 2\n",
    "        self.fc3 = nn.Linear(256, 10)  # 은닉층2 → 출력층\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            입력 이미지 (batch_size, 1, 28, 28)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        output : torch.Tensor\n",
    "            클래스별 로짓 (batch_size, 10)\n",
    "        \"\"\"\n",
    "        # 1. 입력을 1차원으로 평탄화 (Flatten)\n",
    "        # (batch_size, 1, 28, 28) → (batch_size, 784)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # 2. 첫 번째 은닉층 + ReLU + Dropout\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # 3. 두 번째 은닉층 + ReLU + Dropout\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # 4. 출력층 (활성화 함수 없음, CrossEntropyLoss가 softmax 포함)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model_solution(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    모델을 1 에폭 학습시킵니다.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    avg_loss : float\n",
    "        평균 손실값\n",
    "    accuracy : float\n",
    "        정확도 (%)\n",
    "    \"\"\"\n",
    "    model.train()  # 학습 모드\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        # 1. 데이터를 디바이스로 이동\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 2. 그래디언트 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 3. 순전파\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 4. 손실 계산\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 5. 역전파\n",
    "        loss.backward()\n",
    "\n",
    "        # 6. 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 7. 통계 업데이트\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate_model_solution(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    모델을 평가합니다.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    avg_loss : float\n",
    "        평균 손실값\n",
    "    accuracy : float\n",
    "        정확도 (%)\n",
    "    \"\"\"\n",
    "    model.eval()  # 평가 모드\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for images, labels in test_loader:\n",
    "            # 1. 데이터를 디바이스로 이동\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # 2. 순전파\n",
    "            outputs = model(images)\n",
    "\n",
    "            # 3. 손실 계산\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 4. 통계 업데이트\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# 모델 학습 실행\n",
    "print(\"=\" * 60)\n",
    "print(\"SimpleMLP 모델 학습\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = SimpleMLP_Solution().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 정보 출력\n",
    "print(f\"\\n모델 구조:\")\n",
    "print(model)\n",
    "print(f\"\\n총 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 학습\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "print(f\"\\n학습 시작 (총 {num_epochs} 에폭)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 학습\n",
    "    train_loss, train_acc = train_model_solution(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # 평가\n",
    "    test_loss, test_acc = evaluate_model_solution(model, test_loader, criterion, device)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# 학습 과정 시각화\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 손실 그래프\n",
    "ax1.plot(range(1, num_epochs + 1), train_losses, \"b-o\", label=\"Train Loss\")\n",
    "ax1.plot(range(1, num_epochs + 1), test_losses, \"r-o\", label=\"Test Loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"학습/테스트 손실\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# 정확도 그래프\n",
    "ax2.plot(range(1, num_epochs + 1), train_accs, \"b-o\", label=\"Train Accuracy\")\n",
    "ax2.plot(range(1, num_epochs + 1), test_accs, \"r-o\", label=\"Test Accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.set_title(\"학습/테스트 정확도\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n최종 테스트 정확도: {test_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2b7a4",
   "metadata": {},
   "source": [
    "### 예측 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f455a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일부 테스트 이미지에 대한 예측 시각화\n",
    "model.eval()\n",
    "\n",
    "# 테스트 데이터에서 무작위로 샘플 추출\n",
    "indices = np.random.choice(len(test_dataset), 10, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "for idx, ax in zip(indices, axes.flat):\n",
    "    image, true_label = test_dataset[idx]\n",
    "\n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image.unsqueeze(0).to(device)\n",
    "        output = model(image_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        pred_label = predicted.item()\n",
    "\n",
    "    # 시각화\n",
    "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "    color = \"green\" if pred_label == true_label else \"red\"\n",
    "    ax.set_title(f\"True: {true_label}, Pred: {pred_label}\", color=color)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"MLP 예측 결과 (초록=정답, 빨강=오답)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc8043",
   "metadata": {},
   "source": [
    "### 해설\n",
    "\n",
    "**핵심 포인트:**\n",
    "\n",
    "1. **모델 구조**\n",
    "   - MLP는 완전 연결층(Fully Connected Layer)만으로 구성\n",
    "   - 이미지를 1차원 벡터로 펼쳐서(flatten) 입력\n",
    "   - 은닉층에는 ReLU 활성화 함수 사용\n",
    "   - Dropout으로 과적합 방지\n",
    "\n",
    "2. **학습 과정**\n",
    "   - `model.train()`: Dropout 활성화\n",
    "   - `optimizer.zero_grad()`: 그래디언트 초기화 (필수!)\n",
    "   - `loss.backward()`: 역전파로 그래디언트 계산\n",
    "   - `optimizer.step()`: 가중치 업데이트\n",
    "\n",
    "3. **평가 과정**\n",
    "   - `model.eval()`: Dropout 비활성화\n",
    "   - `torch.no_grad()`: 그래디언트 계산 안 함 (메모리 절약)\n",
    "\n",
    "4. **성능 분석**\n",
    "   - MNIST는 비교적 쉬운 데이터셋\n",
    "   - 단순 MLP로도 97~98% 정확도 달성 가능\n",
    "   - CNN을 사용하면 99% 이상 가능\n",
    "\n",
    "**주요 하이퍼파라미터:**\n",
    "- 학습률(lr): 0.001 (Adam의 기본값)\n",
    "- 배치 크기: 128\n",
    "- Dropout 비율: 0.2\n",
    "- 은닉층 크기: 512, 256\n",
    "\n",
    "**개선 방법:**\n",
    "- 더 깊은 네트워크 (레이어 추가)\n",
    "- Batch Normalization 추가\n",
    "- Learning rate scheduling\n",
    "- Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec25c5d",
   "metadata": {},
   "source": [
    "---\n",
    "## 문제 3: 간단한 CNN 모델 구현 (MNIST)\n",
    "\n",
    "**문제:** MNIST 손글씨 숫자 분류를 위한 Convolutional Neural Network (CNN) 모델을 구현하세요.\n",
    "\n",
    "**모델 구조:**\n",
    "```\n",
    "Input (1×28×28)\n",
    "→ Conv1 (32 filters, 3×3, padding=1) → ReLU → MaxPool (2×2)\n",
    "→ Conv2 (64 filters, 3×3, padding=1) → ReLU → MaxPool (2×2)\n",
    "→ Flatten\n",
    "→ FC1 (128) → ReLU → Dropout(0.5)\n",
    "→ FC2 (10)\n",
    "```\n",
    "\n",
    "**요구사항:**\n",
    "1. PyTorch의 nn.Module을 상속받는 CNN 클래스 작성\n",
    "2. Convolutional Layer와 Pooling Layer 사용\n",
    "3. MLP와 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e87de8",
   "metadata": {},
   "source": [
    "### TODO: CNN 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edab190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 여러분의 코드를 작성하세요\n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    간단한 Convolutional Neural Network 모델\n",
    "\n",
    "    Architecture:\n",
    "    - Conv1: 32 filters, 3x3, padding=1\n",
    "    - MaxPool: 2x2\n",
    "    - Conv2: 64 filters, 3x3, padding=1\n",
    "    - MaxPool: 2x2\n",
    "    - FC1: 128 neurons\n",
    "    - Dropout: 0.5\n",
    "    - FC2: 10 classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # TODO: 레이어들을 정의하세요\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            입력 이미지 (batch_size, 1, 28, 28)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        output : torch.Tensor\n",
    "            클래스별 로짓 (batch_size, 10)\n",
    "        \"\"\"\n",
    "        # TODO: 순전파 과정을 구현하세요\n",
    "        pass\n",
    "\n",
    "\n",
    "# 모델 생성 및 학습\n",
    "if __name__ == \"__main__\":\n",
    "    model = SimpleCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # TODO: 학습 루프 작성 (MLP와 동일한 방식)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a91c0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>힌트 보기</b></summary>\n",
    "\n",
    "### 힌트\n",
    "\n",
    "**Convolutional 레이어:**\n",
    "```python\n",
    "self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "```\n",
    "\n",
    "**출력 크기 계산:**\n",
    "- 입력: 28×28\n",
    "- Conv1 + Pool: 28×28 → 14×14 (32 channels)\n",
    "- Conv2 + Pool: 14×14 → 7×7 (64 channels)\n",
    "- Flatten: 7×7×64 = 3136\n",
    "\n",
    "**Forward:**\n",
    "```python\n",
    "x = self.pool(F.relu(self.conv1(x)))\n",
    "x = self.pool(F.relu(self.conv2(x)))\n",
    "x = x.view(x.size(0), -1)  # Flatten\n",
    "x = F.relu(self.fc1(x))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7604ba",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>모범 답안 및 해설 보기</b></summary>\n",
    "\n",
    "### 모범 답안\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed616010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모범 답안\n",
    "\n",
    "\n",
    "class SimpleCNN_Solution(nn.Module):\n",
    "    \"\"\"\n",
    "    간단한 Convolutional Neural Network 모델\n",
    "\n",
    "    Architecture:\n",
    "    - Conv1: 32 filters, 3x3, padding=1 → ReLU → MaxPool 2x2\n",
    "    - Conv2: 64 filters, 3x3, padding=1 → ReLU → MaxPool 2x2\n",
    "    - Flatten\n",
    "    - FC1: 128 neurons → ReLU → Dropout 0.5\n",
    "    - FC2: 10 classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN_Solution, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # 입력 크기 계산: 28×28 → 14×14 → 7×7, 채널 64\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            입력 이미지 (batch_size, 1, 28, 28)\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        output : torch.Tensor\n",
    "            클래스별 로짓 (batch_size, 10)\n",
    "        \"\"\"\n",
    "        # Conv1 + ReLU + Pool\n",
    "        # (batch, 1, 28, 28) → (batch, 32, 28, 28) → (batch, 32, 14, 14)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Conv2 + ReLU + Pool\n",
    "        # (batch, 32, 14, 14) → (batch, 64, 14, 14) → (batch, 64, 7, 7)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten\n",
    "        # (batch, 64, 7, 7) → (batch, 64*7*7=3136)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # FC1 + ReLU + Dropout\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # FC2 (Output)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# CNN 모델 학습\n",
    "print(\"=\" * 60)\n",
    "print(\"SimpleCNN 모델 학습\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cnn_model = SimpleCNN_Solution().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 정보 출력\n",
    "print(f\"\\n모델 구조:\")\n",
    "print(cnn_model)\n",
    "print(f\"\\n총 파라미터 수: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n",
    "\n",
    "# 학습\n",
    "num_epochs = 5\n",
    "cnn_train_losses = []\n",
    "cnn_train_accs = []\n",
    "cnn_test_losses = []\n",
    "cnn_test_accs = []\n",
    "\n",
    "print(f\"\\n학습 시작 (총 {num_epochs} 에폭)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 학습 (MLP와 동일한 함수 재사용)\n",
    "    train_loss, train_acc = train_model_solution(\n",
    "        cnn_model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    cnn_train_losses.append(train_loss)\n",
    "    cnn_train_accs.append(train_acc)\n",
    "\n",
    "    # 평가\n",
    "    test_loss, test_acc = evaluate_model_solution(\n",
    "        cnn_model, test_loader, criterion, device\n",
    "    )\n",
    "    cnn_test_losses.append(test_loss)\n",
    "    cnn_test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\n최종 테스트 정확도: {cnn_test_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfecd8df",
   "metadata": {},
   "source": [
    "### MLP vs CNN 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP vs CNN 비교 그래프\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 손실 비교\n",
    "axes[0].plot(range(1, num_epochs + 1), train_losses, \"b--\", label=\"MLP Train\")\n",
    "axes[0].plot(range(1, num_epochs + 1), test_losses, \"b-\", label=\"MLP Test\")\n",
    "axes[0].plot(range(1, num_epochs + 1), cnn_train_losses, \"r--\", label=\"CNN Train\")\n",
    "axes[0].plot(range(1, num_epochs + 1), cnn_test_losses, \"r-\", label=\"CNN Test\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"MLP vs CNN - 손실 비교\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# 정확도 비교\n",
    "axes[1].plot(range(1, num_epochs + 1), train_accs, \"b--\", label=\"MLP Train\")\n",
    "axes[1].plot(range(1, num_epochs + 1), test_accs, \"b-\", label=\"MLP Test\")\n",
    "axes[1].plot(range(1, num_epochs + 1), cnn_train_accs, \"r--\", label=\"CNN Train\")\n",
    "axes[1].plot(range(1, num_epochs + 1), cnn_test_accs, \"r-\", label=\"CNN Test\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "axes[1].set_title(\"MLP vs CNN - 정확도 비교\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 성능 비교 요약\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MLP vs CNN 성능 비교 요약\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"MLP 최종 테스트 정확도: {test_accs[-1]:.2f}%\")\n",
    "print(f\"CNN 최종 테스트 정확도: {cnn_test_accs[-1]:.2f}%\")\n",
    "print(f\"정확도 향상: {cnn_test_accs[-1] - test_accs[-1]:.2f}%p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef36bb",
   "metadata": {},
   "source": [
    "### CNN 예측 결과 시각화 및 Feature Map 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c95ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 예측 결과 시각화\n",
    "cnn_model.eval()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "for idx, ax in zip(indices, axes.flat):\n",
    "    image, true_label = test_dataset[idx]\n",
    "\n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image.unsqueeze(0).to(device)\n",
    "        output = cnn_model(image_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        pred_label = predicted.item()\n",
    "\n",
    "    # 시각화\n",
    "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "    color = \"green\" if pred_label == true_label else \"red\"\n",
    "    ax.set_title(f\"True: {true_label}, Pred: {pred_label}\", color=color)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"CNN 예측 결과 (초록=정답, 빨강=오답)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a518e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Map 시각화\n",
    "def visualize_feature_maps(model, image, layer_name=\"conv1\"):\n",
    "    \"\"\"\n",
    "    CNN의 중간 feature map을 시각화합니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Hook을 사용하여 중간 레이어의 출력 캡처\n",
    "    activation = {}\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "\n",
    "        return hook\n",
    "\n",
    "    # Hook 등록\n",
    "    if layer_name == \"conv1\":\n",
    "        model.conv1.register_forward_hook(get_activation(\"conv1\"))\n",
    "    elif layer_name == \"conv2\":\n",
    "        model.conv2.register_forward_hook(get_activation(\"conv2\"))\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(image.unsqueeze(0).to(device))\n",
    "\n",
    "    # Feature map 추출\n",
    "    feature_map = activation[layer_name].squeeze(0).cpu()\n",
    "\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "# 샘플 이미지로 feature map 시각화\n",
    "sample_image, sample_label = test_dataset[0]\n",
    "\n",
    "# Conv1 feature maps\n",
    "conv1_features = visualize_feature_maps(cnn_model, sample_image, \"conv1\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "fig.suptitle(f\"Conv1 Feature Maps (총 32개 필터)\", fontsize=14)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < conv1_features.shape[0]:\n",
    "        ax.imshow(conv1_features[i], cmap=\"viridis\")\n",
    "        ax.set_title(f\"Filter {i + 1}\")\n",
    "        ax.axis(\"off\")\n",
    "    else:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a56cac",
   "metadata": {},
   "source": [
    "### 해설\n",
    "\n",
    "**CNN vs MLP 핵심 차이점:**\n",
    "\n",
    "1. **공간적 구조 보존**\n",
    "   - MLP: 이미지를 1차원으로 펼침 → 공간 정보 손실\n",
    "   - CNN: 2차원 구조 유지 → 지역적 패턴 학습\n",
    "\n",
    "2. **파라미터 수**\n",
    "   - MLP: 784×512 = 401,408개 (첫 레이어만)\n",
    "   - CNN: 3×3×32 = 288개 (첫 레이어)\n",
    "   - CNN이 훨씬 적은 파라미터로 더 좋은 성능\n",
    "\n",
    "3. **특징 추출**\n",
    "   - MLP: 픽셀별 가중치 학습\n",
    "   - CNN: 엣지, 텍스처 등 계층적 특징 학습\n",
    "\n",
    "4. **Translation Invariance**\n",
    "   - CNN은 필터를 공유하므로 위치 변화에 강건\n",
    "\n",
    "**CNN 구조 이해:**\n",
    "\n",
    "```\n",
    "입력: 1×28×28\n",
    "   ↓\n",
    "Conv1(32 filters, 3×3, pad=1): 32×28×28\n",
    "   ↓ ReLU + MaxPool(2×2)\n",
    "32×14×14\n",
    "   ↓\n",
    "Conv2(64 filters, 3×3, pad=1): 64×14×14\n",
    "   ↓ ReLU + MaxPool(2×2)\n",
    "64×7×7 = 3136\n",
    "   ↓ Flatten\n",
    "FC1(128)\n",
    "   ↓ ReLU + Dropout\n",
    "FC2(10)\n",
    "```\n",
    "\n",
    "**Feature Map 해석:**\n",
    "- 첫 번째 층: 엣지, 방향성 등 저수준 특징 감지\n",
    "- 두 번째 층: 코너, 곡선 등 조합된 특징\n",
    "- 깊은 층으로 갈수록 추상적인 특징 학습\n",
    "\n",
    "**성능 향상 팁:**\n",
    "- Batch Normalization 추가\n",
    "- 더 깊은 네트워크 (VGG, ResNet 스타일)\n",
    "- Data Augmentation (회전, 이동, 확대/축소)\n",
    "- Learning rate scheduling\n",
    "\n",
    "**MNIST 데이터셋에서:**\n",
    "- MLP: 약 97-98% 정확도\n",
    "- 간단한 CNN: 약 99% 정확도\n",
    "- 깊은 CNN (ResNet 등): 99.5% 이상"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
